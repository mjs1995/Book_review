# 서평
- 2005년 더크 커팅이 오픈 소스로 공개한 하둡은 분산 저장 파일시스템인 HDFS와 분산 병렬 처리를 담당하는 맵리듀스로 구성되어 있음, 하둡이 나온 후 이를 기반으로 피그, 하이브, HBase, 스쿱, 플룸 등의 오픈 소스 기술이 등장했고, 이들을 모두 결합한 빅데이터 기술은 통칭하여 하둡 에코시스템이라고 부르게 되었음, 스파크의 등장으로 대용량 데이터를 저장하고 일괄 처리하는 수준에서 벗어나 머신러닝과 실시간 분석까지 아우르게 되었음 
- 이 책은 하둡의 핵심, 하둡 에코시스템 관련 프로젝트, 하둡 사례 연구를 다루고 있으며 이 책을 통해 각 기술의 핵심을 설명했고, 다른 기술과의 관계도 친절하게 설명하고 있다.

# 하둡 기초
- 전체 데이터에 질의하기
  - 맵리듀스의 접근법은 무차별대입(brute-force) 방식 처럼 보임 , 맵리듀스 전체가 한 번의 쿼리로 전체나 상당한 규모의 데이터셋을 처리하는 것이기 때문 
  - 맵리듀스는 일괄 질의 처리기고, 전체 데이터셋을 대상으로 비정형(ad hoc) 쿼리를 수행하고 합리적인 시간 내에 그 결과를 보여주는 능력을 지니고 있음 
- 일괄 처리를 넘어서
  - 맵리듀스의 강점은 기본적으로 일괄 처리 시스템, 대화형 분석에는 적합하지 않음, 질의를 실행한 후 수 초 이내에 결과를 받는 것을 불가능함 
  - 온라인 접근을 지원하는 첫 번째 구성요소인 HBase는 HDFS를 기본 저장소로 하는 키-값 저장소, HBase는 개별 행에 대한 온라인 읽기/쓰기와 산적한 데이터를 읽고 쓰는 일괄 처리를 둘다 지원하기 때문에 애플리케이션을 구축하는 데 좋은 솔루션이 될 수 있음
  - YARN은 클러스터 자원 관리 시스템으로, 맵리듀스뿐만 아니라 어떤 분산 프로그램도 하둡 클러스터에 저장된 데이터를 처리할 수 있게 해줌 
  - 다양한 처리 패턴
    - 대화형 SQL
      - 대화형 sQL은 맵리듀스 대신 장기 실행 전용 데몬(임팔라)이나 컨테이너를 재사용하는(테즈 기반의 하이브) 분산 쿼리 엔진을 사용함 
    - 반복 처리
      - 머신러닝과 같은 다수의 알고리즘은 근본적으로 반복 연산을 함 
    - 스트림 처리
      - 스톰, 스파크 스트리밍, 삼자와 같은 스트리밍 시스템은 실시간으로 실행되고 경계가 없는 스트림 데이터를 분산 계산하여 그 결과를 하둡 저장소나 외부 시스템에 보낼 수 있음
    - 검색
      - 솔라(Solr) 검색 플랫폼은 하둡 클러스터에서 실행될 수 있음, 솔라는 문서를 색인하여 HDFS에 저장하고 HDFS에 저장된 색인을 기반으로 검색 쿼리를 제공함 

# 맵리듀스 
- 분산형으로 확장하기
  - 데이터 흐름
    - 맵리듀스 잡은 클라이언트가 수행하는 작업의 기본 단위, 잡은 입력 데이터, 맵리듀스 프로그램, 설정 정보로 구성됨
    - 하둡은 잡을 맵 태스크와 리듀스 태스크로 나누어 실행함, 각 태스크는 YARN을 이용하여 스케줄링되고 클러스터의 여러 노드에서 실행됨, 특정 노드의 태스크 하나가 실패하면 자동으로 다른 노드를 재할당하여 다시 실행됨 
    - 하둡은 맵리듀스 잡의 입력을 입력 스플릿(input split) 또는 단순히 스플릿이라고 부르는 고정 크기 조각으로 분리함, 하둡은 각 스플릿마다 하나의 맵 태스크를 생성하고 스플릿의 각 레코드를 사용자 정의 맵 함수로 처리함 
    - 데이터 지역성 최적화(data locality optimiztation) : 하둡은 HDFS 내의 입력 데이터가 있는 노드에서 맵 태스크를 실행할 때 가장 빠르게 작동함, 클ㄹ러스터의 중요한 공유 자원인 네트워크 대역폭을 사용하지 않는 방법 
  - 컴바이너 함수
    - 하둡은 맵의 결과를 처리하는 컴바이너 함수(컴바이너 함수의 출력이 결국 리듀스 함수의 입력이 됨)를 허용, 컴바이너 함수는 최적화와 관련이 있음 
    - 하둡은 컴바이너 함수의 호출 빈도와 상관없이 리듀스의 결과가 언제나 같도록 보장함 
    - 컴바이너를 사용하면 매퍼와 리듀서 사이의 셔플 단계에서 전송되는 데이터양을 줄이는 데 큰 도움이됨 
    - 컴바이너 함수는 Reducer 클래스를 사용해서 정의함, MaxTemperatureReducer에 있는 리듀서 함수와 동일한 구현체를 사용함 
  - 분산 맵리듀스 잡 실행하기
    - 맵리듀스는 데이터 크기와 하드웨어 성능에 따라 확장할 수 있는데 이것이 바로 맵리듀스의 핵심임
- 하둡 스트리밍
  - 자바 외에 다른 언어로 맵과 리듀스 함수를 작성할 수 있는 맵리듀스 API를 제공함
  - 하둡과 사용자 프로그램 사이의 인터페이스로 유닉스 표준 스트림을 사용함, 사용자는 표준 입력을 읽고 표준 출력으로 쓸 수 있는 다양한 언어를 이용하여 맵리듀스 프로그램을 작성할 수 있음 
  - 스트리밍은 그 특성상 텍스트 처리에 매우 적합함, 맵의 입력 데이터는 표준 입력으로 맵 함수에 전달되고, 행 단위로 처리되어 표준 출력으로 쓰여짐, 맵의 출력 키-값 쌍은 탭으로 구분된 하나의 행으로 출력됨

# 하둡 분산 파일시스템
- 네트워크로 연결된 여러 머신의 스토리지를 관리하는 파일시스템
- HDFS(Hadoop Distributed FileSystem)라는 분산 파일시스템을 제공함
- HDFS 설계
  - 범용 하드웨어로 구성된 클러스터에서 실행되고 스트리밍 방식의 데이터 접근 패턴으로 대용량 파일을 다룰 수 있도록 설계된 파일시스템
  - 설계 특성
    - 매우 큰 파일
    - 스트리밍 방식의 데이터 접근
    - 범용 하드웨어
      - 하둡은 노드 장애가 발생할 확률이 높은 범용 하드웨어(여러 업체에서 제공하는 쉽게 구할 수 있는 하드웨어)로 구성된 대형 클러스터에서 문제없이 실행되도록 설계됨 
  - HDFS가 잘 맞지 않는 응용 분야
    - 빠른 데이터 응답시간
      - HDFS는 높은 데이터 처리량을 제공하기 위해 최적화되어 있고 이를 위해 응답 시간을 희생함, HBase가 하나의 대안이 될 수 있음
    - 수많은 작은 파일
    - 다중 라이터와 파일의 임의 수정
      - HDFS는 단일 라이터로 파일을 씀, 한 번 쓰고 끝나거나 파일의 끝에 덧붙이는 것은 가능하지만 파일에서 임의 위치에 있는 내용을 수정하는 것은 허용하지 않으며 다중 라이터도 지원하지 않음
- HDFS 개념
    - 블록 
        - 블록 크기는 한 번에 읽고 쓸 수 있는 데이터의 최대량 
        - HDFS 블록이 큰 이유
            - HDFS 블록은 디스크 블록에 비해 상당히 큼, 탐색 비용을 최소화하기 위해서, 블록이 매우 크면 블록의 시작점을 탐색하는데 걸리는 시간을 줄일 수 있고 데이터를 전송하는 데 더 많은 시간을 할애할 수 있음, 여러 개의 블록으로 구성된 대용량 파일을 전송하는 시간은 디스크 전송 속도에 크게 영향을 받음
        - 블록 추상화 개념을 도입하면서 얻은 이점
            - 파일 하나의 크기가 단일 디스크의 용량보다 더 커질 수 있다는 것
            - 파일 단위보다는 블록 단위로 추상화를 하면 스토리지의 서브시스템을 단순하게 만들 수 있다는 것 
            - 블록은 내고장성(fault tolerance)과 가용성(availability)을 제공하는 데 필요한 복제(replication)를 구현할 때 매우 적합함,블록의 손상과 디스크 및 머신의 장애에 대처하기 위해 각 블록은 물리적으로 분리된 다수의 머신(보통 3개)에 복제됨
            - 만일 하나의 블록을 이용할 수 없는 상황이 되면 다른 머신에 있는 복사본을 읽도록 클라이언트에 알려주면 됨 
    - 네임노드와 데이터노드
        - HDFS 클러스터는 마스터-워커 패턴으로 동작하는 두 종류의 노드, 마스터인 하나의 네임노드와 워커인 여러 개의 데이터노드로 구성되어 있음, 네임노드는 파일시스템의 네임스페이스를 관리함
        - 네임노드는 파일시스템 트리와 그 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터를 유지함 , 네임스페이스 이미지와 에디트 로그라는 두 종류의 파일로 로컬 디스크에 영속적으로 저장됨, 파일에 속한 모든 블록이 어느 데이터노드에 있는지 파악하고 있음 
        - HDFS 클라이언트는 사용자를 대신해서 네임노드와 데이터노드 사이에서 통신하고 파일시스템에 접근함 
        - 네임노드의 장애복구 기능
            - 파일시스템의 메타데이터를 지속적인 상태로 보존하기 위해 파일로 백업하는 것, 네임노드가 다수의 파일시스템에 영구적인 상태를 저장하도록 하둡을 구성할 수 있음, 백업 작업은 동기화되고 원자적으로 실행됨, 주로 권장하는 방법은 로컬 디스크와 원격의 NFS 마운트를 두 곳에 동시에 백업하는 것 
            - 보조 네임노드(secondary namenode)를 운영하는 것, 에디트 로그가 너무 커지지 않도록 주기적으로 네임스페이스 이미지를 에디트 로그와 병합하여 새로운 네임스페이스 이미지를 만드는 것, 주 네임노드에 장애가 발생할 것을 대비해서 네임스페이스 이미지의 복제본을 보관하는 역할을 함 
    - 블록 캐싱
        - 오프힙(off-heap, 자바 힙 외부에서 관리되는) 블록 캐시라는 데이터노드의 메모리에 명시적으로 캐싱할 수 있음
        - 조인을 할 때 작은 룩업 테이블을 캐싱하는 것은 좋은 활용사례임
        - 사용자나 애플리케이션은 캐시 풀(cache pool)에 캐시 지시자(cache directive)를 추가하여 특정 파일을 캐싱하도록 명령할 수 있음, 캐시 풀은 캐시 권한이나 자원의 용도를 관리하는 관리 그룹의 역할을 맡음 
- HDFS 패더레이션
    - HDFS 페더레이션을 적용하면 각 네임노드는 네임스페이스의 메타데이터를 구성하는 네임스페이스 볼륨과 네임스페이스에 포함된 파일의 전체 블록을 보관하는 블록 풀을 관리함 
- HDFS 고가용성
    - 네임노드는 여전히 단일 고장점(single point of failure - SPOF), 네임노드에 장애가 발생하면 맵리듀스 잡을 포함하여 모든 클라이언트가 파일을 읽거나 쓰거나 조회할 수 없게 됨, 네임노드는 메타데이터와 파일 블록의 매핑 정보를 보관하는 유일한 저장소이기 때문
    - 새로운 네임노드는 네임스페이스 이미지를 메모리에 로드하고 -> 에디트 로그를 갱신하고 -> 전체 데이터노드에서 충분한 블록 리포트를 받아 안전 모드를 벗어날 때까지 그 어떤 요청도 처리하지 못함 
    - HDFS 고가용성(high availability -HA)을 지원함, 고가용성은 활성대기상태로 설정된 한 쌍의 네임노드로 구현됨, 활성네임노드에 장애가 발생하면 대기 네임노드가 그 역할을 이어받아 큰 중단없이 클라이언트의 요청을 처리함         
- 장애복구와 펜싱
  - 대기 네임노드를 활성화시키는 전환 작업은 장애복구 컨트롤러라는 새로운 객체로 관리됨
  - 장애복구는 정기적인 유지관리를 위해 관리자가 수동으로 초기화할 수 있음
  - 우아한 장애복구(graceful failover) - 자애복구 컨트롤러는 두 개의 네임노드가 서로 역할을 바꾸게 하는 방법으로 전환 순서를 제어할 수 있음 
- 인터페이스
  - HTTP
    - HTTP로 HDFS에 접근하는 두 가지 방식
      - 클라이언트의 HTTP 요청을 HDFS 데몬이 직접 처리하는 방식 - 네임노드와 데이터노드에 내장된 웹 서버가 WebHDFS의 말단으로 적용함
      - 클라이언트 대신 DistributedFileSystem API로 HDFS에 접근하는 프록시를 경유하는 방식 - 하나 또는 그 이상의 독립(standalone) 프록시 서버를 통하는 것, 프록시 서버는 상태를 저장할 필요가 없으므로 표준 로드 밸런서를 사용해도 괜찮음, 클러스터의 모든 트래픽은 프록시를 경유하므로 클라이언트는 네임노드와 데이터노드에 직접 접근할 필요가 없음, 프록시를 통하면 엄격한 방화벽이나 대역폭 제한 정책을 적용하기 쉬움, 프록시를 통한 방식은 서로 다른 데이터 센터에 있는 하둡 클러스터 사이의 데이터 전송이나 외부 네트워크에 있는 클라우드에서 운영되는 하둡 클러스터에 접근할 때 일반적으로 이용되는 방법 
  - C 
    - 자바 FileSystem 인터페이스를 모방한 libhdfs라는 C 라이브러리를 제공함
    - libhdfs는 자바 파일시스템 클라이언트를 호출하기 위해 자바 네이티브 인터페이스(JNI)를 사용함 
  - NFS
    - NFSv3 게이트웨이를 이용하면 로컬 클라이언트 파일시스템에 HDFS를 마운트할 수 있음
    - 파일시스템을 다루는 ls나 cat 같은 Unix 유틸리티를 이용할 수 있으며, 파일 업로드 및 일반적인 프로그래밍 언어에서 파일시스템을 다루는 POSIX 라이브러리도 사용할 수 있음
  - FUSE
    - Filesystem in Userspace (사용자 공간에서의 파일시스템)로, 사용자 공간과 유닉스 파일시스템을 통합한 파일시스템을 지원함 
# YARN
- 하둡의 클러스터 자원 관리 시스템, 클러스터의 자원을 요청하고 사용하기 위해 API를 제공함 
- 맵리듀스, 스파크 등과 같은 분산 컴퓨팅 프레임워크는 클러스터 계산 계층(YARN)과 클러스터 저장 계층(HDFS와 HBase) 위에서 YARN 애플리케이션을 실행함
- 리소스 매니저와 노드매니저 등 두가지 유형의 장기 실행 데몬을 통해 핵심 서비스를 제공함
  - 클러스터에서 유일한 리소스 매니저는 클러스터 전체 자원의 사용량을 관리함
  - 모든 머신에서 실행되는 노드 매니저는 컨테이너를 구동하고 모니터링하는 역할을 맡음 
- 자원 요청
  - 분산 데이터 처리 알고리즘에서 클러스터의 네트워크 대역폭을 효율적으로 활용하기 위해서는 지역성을 보장하는 것이 가장 중요함
  - YARN은 특정 애플리케이션이 호출한 컨테이너에 대해 지역성 제약을 규정하는 것을 허용함, 지역성 제약은 특정 노드나 랙 또는 클러스터의 다른 곳(외부 랙)에서 컨테이너를 요청할 때 사용됨 
- 애플리케이션 수명
  - 실행 시간 보다는 사용자가 실행하는 잡의 방식에 따라 애플리케이션을 분리하는 것이 좋음
    - 사용자의 잡 당 하나의 애플리케이션이 실행되는 방식으로, 맵리듀스 잡이 여기에 속함
    - 워크플로나 사용자의 잡 세션(잡은 서로 관련이 없을 수도 있음)당 하나의 애플리케이션이 실행되는 방식, 첫 번째 유형보다 훨씬 더 효율적, 순차적으로 실행되는 잡이 동일한 컨테이너를 재사용할 수 있기 때문, 잡 사이에 공유 데이터를 캐싱할 수 있는 큰 장점도 있음, 사례로는 스파크
    - 서로 다른 사용자들이 공유할 수 있는 장기 실행 애플리케이션, 일종의 코디네이션 역할을 수행함, 아파치 슬라이더는 클러스터에서 다양한 애플리케이션을 구동시키는 장기 실행 애플리케이션 마스터를 가지고 있응, 임팔라는 여러 임팔라 데몬이 클러스터 자원을 요청할 수 있도록 프록시 애플리케이션을 제공함 
- YARN 애플리케이션 만들기
  - 잡의 방향성 비순환 그래프(DAG)를 실행하고 싶으면 스파크나 테즈가 더 작합함, 스트리밍 처리는 스파크, 쌈자 또는 스톰을 사용하는 것이 좋음 
  - 아파치 슬라이더는 기존의 분산 애플리케이션을 YARN 위에서 실행하도록 해줌 
  - 아파치 트윌(Apache Twill) 
    - 슬라이더와 비슷하지만 YARN에서 실행되는 분산 애플리케이션을 개발할 수 있는 간단한 프로그래밍 모델을추가로 제공함
    - 자바 Runnable 객체를 확장한 클러스터 프로세스를 정의한 후 클러스터의 YARN 컨테이너에서 이를 실행하는 기능을 제공함
    - 실시간 로깅(runnalbes의 로그 이벤트를 클라이언트에 스트리밍으로 돌려줌)과 명령 메시지(클라이언트에서 runnables 전송) 기능 등을 제공함 
  - 분산 쉘(distributed shell) - 복잡한 스케줄링 요구사항이 있는 애플리케이션
    - 클라이언트 또는 애플리케이션 마스터가 YARN 데몬과 통신하기 위해 YARN의 클라이언트 API를 어떻게 사용하는지 잘 보여주고 있음 
- YARN과 맵리듀스1의 차이점
  - 맵리듀스 1에는 잡의 실행과정을 제어하는 하나의 잡트래커와 하나 이상의 태스크트래커등 두 종류의 데몬이 있음
  - 잡트래커
    - 여러 태스크트래커에서 실행되는 태스크를 스케줄링함으로써 시스템에서 실행되는 모든 잡을 조율함
    - 맵리듀스 1에서 잡 스케줄링(태스크와 태스크트래커를 연결)과 태스크 진행 모니터링(태스크를 추적하고, 실패하거나 느린 태스크를 다시 시작하고, 전체 카운터를 유지하는 방법으로 태스크 장부(bookeeping)을 맡고 있음, 반면 YARN은 이러한 역할을 분리된 객체인 리소스 매니저와 애플리케이션 마스터(맵리듀스 잡당 하나)를 통해 처리함
    - 완료된 잡에 대한 잡 이력을 저장하는 역할을 맡음, 잡트래커의 부하를 줄이기 위해 별도의 데몬인 히스토리 서버를 통해 수행될 수도 있음, YARN에서 이와 동일한 역할은 애플리케이션의 이력을 저장하는 타임라인 서버가 맡고 있음 
  - 태스크트래커
    - 태스크를 실행하고 진행 상황을 잡트래커에 전송하기 때문에 잡트래커는 각 잡의 전체적인 진행 상황을 파악할 수 있음 
  - 맵리듀스1과 YARN 컴포넌트의 비교
    - 잡트래커 - 리소스 매니저, 애플리케이션 마스터, 타임라인 서버
    - 태스크트래커 - 노드 매니저
    - 슬롯 - 컨테이너
  - YARN을 사용하여 얻을 수 있는 이익
    - 확장성
      - 맵 리듀스 1보다 큰 클러스터에서 실행될 수 있음
    - 가용성
      - 고가용성(high availability - HA)은 서비스 데몬에 문제가 발생했을 때 서비스에 필요한 작업을 다른 데몬이 이어받을 수 있도록 상태 정보를 항상 복사해두는 방법으로 구현함 
    - 효율성
    - 멀티테넌시(다중 사용자)
      - 하둡이 맵리듀스를 뛰어넘어 다양한 분산 애플리케이션을 수용할 수 있다는 것 
- YARN 스케줄링
  - 스케줄러 옵션
    - FIFO, 캐퍼시티(가용량), 페어(균등) 스케줄러를 제공함 
    - FIFO
      - 애플리케이션을 큐에 하나씩 넣고 제출된 순서에 따라 순차적으로 실행함(선입선출 방식), 큐에 처음으로 들어온 애플리케이션 요청을 먼저 할당하고, 이 요청을 처리한 후 큐에 있는 다음 애플리케이션 요청을 처리하는 방식으로 순차적으로 실행함 
      - 공유 클러스터 환경에서는 적합하지 않음, 대형 애플리케이션이 수행될 때는클러스터의 모든 자원을 점유해버릴 수 있기 때문에 다른 애플리케이션은 자기 차례가 올때 까지 계속 대기해야함, 다른 두 스케줄러는 장시간 수행되는 잡을 계속 처리하는 동시에 작은 비정형 질의도 중간에 실행하여 적당한 시간 내에 사용자가 결과를 얻을 수 있도록 허용함 
    - 캐퍼시티(Capacity)
      - 작은 잡을 제출되는 즉시 분리된 전용 큐에서 처리함
      - 물론 해당 큐는 잡을 위한 자원을 미리 예약해두기 때문에 전체 클러스터의 효율성은 떨어짐, 또한 대형 잡은 FIFO 스케줄러보다 늦게 끝나게 됨 
      - 회사의 조직 체계에 맞게 하둡 클러스터를 공유할 수 있음, 각 조직은 전체 클러스터의 지정된 가용량을 미리 할당받음, 각 조직은 분리된 전용 큐를 가지며 클러스터 가용량의 지정된 부분을 사용하도록 설정할 수 있음 
    - 페어(Fair)
      - 실행 중인 모든 잡의 자원을 동적으로 분배하기 때문에 미리 자원의 가용량을 예약할 필요가 없음, 대형 잡이 먼저 시작되면 이때는 실행 중인 잡이 하나밖에 없기 때문에 클러스터의 모든 자원을 얻을 수 있음
      - 대형 잡이 실행되는 도중에 작은 잡이 추가로 시작되면 페어 스케줄러는 클러스터 자원의 절반을 이 잡에 할당함, 각 잡은 클러스터의 자원을 공평하게 사용할 수 있게 됨 
      - 실행 중인 모든 애플리케이션에 동일하게 자원을 할당함 

# 하둡 I/O
- 압축
  - 파일 압축은 파일 저장 공간을 줄이고, 네트워크 또는 디스크로부터 데이터 전송을 고속화할 수 있는 두 가지 커다란 이점이 있음 
  - 압축 포맷의 요약
    - 압축 포맷 - 도구 - 알고리즘 - 파일 확장명 - 분할 가능
    - DEFLATE - N/A - DEFLATE - .deflate - No
    - gzip - gzip - DEFLATE - .gz - No
    - bzip2 - bzip2 - bzip2 - .bz2 - Yes
    - LZO - lzop - LZO - .lzo - No
    - LZ4 - N/A - LZ4 - .lz4 - No
    - Snappy - N/A - Snappy - .snappy - No 
  - gzip은 일반적인 목적의 압축 도구고, 공간/시간 트레이드오프의 중앙에 위치함
  - bzip2는 gzip보다 더 효율적으로 압축하지만 대신 더 느림, 압축 해제 속도는 압축 속도보다 더 빠르지만 여전히 다른 포맷에 비해 더 느림
  - LZO, LZ4, Snappy 모두 속도에 최적화되었고 gzip보다 어느 정도 빠르지만 압축 효율 은 떨어짐 
  - 압축 포맷 사용
    - 압축과 분할 모두를 지원하는 시퀀스 파일, 에이브로, ORCFile, 파케이 같은 컨테이너 파일 포맷을 사용하라, 보통 LZO, LZ4, Snappy와 같은 빠른 압축 형식이 적당함
    - 상당히 느리긴 하지만 bzip2 같은 분할을 지원하는 압축 포맷을 사용하라, 또는 분할을 지원하기 위해 색인 될 수 있는 LZO 같은 포맷을 사용하라 
    - 애플리케이션에 파일을 청크로 분할하고, 지원되는 모든 압축 포맷(분할에 관계없이)을 사용하여 각 청크를 개별적으로 압축하라. 이 경우 압축된 청크가 거의 HDFS 블록 하나의 크기가 되도록 처크 크기를 선택해야 함 
    - 파일을 압축하지 말고 그냥 저장하라 
- 직렬화(serialization)
  - 네트워크 전송을 위해 구조화된 객체를 바이트 스트림으로 전환하는 과정
  - 역직렬화(deserializaition) - 바이트 스트림을 일련의 구조화된 객체로 역전환하는 과정
  - 직렬화는 프로세스 간 통신과 영속적인 저장소와 같은 분산 데이터 처리의 독특한 두 영역에서 나타남 
  - 하둡 시스템에서 노드 사이의 프로세스 간 통신은 원격 프로시저 호출(Remote Procedure Call(RPC))을 사용하여 구현됨
  - RPC 직렬화 포맷이 유익한 이유
    - 간결성
    - 고속화
    - 확장성
    - 상호운용성 
  - 저장소 포맷은 간결하고(저장 공간의 효율적 사용을 위해), 빠르고(테라바이트 데이터를 읽고 쓰는 오버헤드를 최소화하기 위해), 확장 가능하고(예전 포맷으로 쓰인 데이터도 문제없이 읽기 위해), 상호운용(다양한 언어를 사용하여 영속적인 데이터를 읽고 쓰기 위해)할 수 있어야 함 

# 맵리듀스
## 맵리듀스 프로그래밍
- 맵리듀스 웹 UI
  - 클러스터 메트릭스(Cluster Metrics) 부분에서 클러스터의 요약 정보를 볼 수 있음, 클러스터(그리고 다양한 상태)에서 현재 실행 중인 애플리케이션의 개수, 클러스터 가용 자원의 수량(전체 메모리 - Memory Total), 노드 매니저 정보가 포함되어 있음
  - 메인 테이블은 클러스터에서 완료되었거나 현재 실행 중인 모든 애플리케이션을 보여줌, 특정 애플리케이션을 찾을 수 있도록 검색 창을 제공함 
  - 잡 히스토리
    - 완료된 맵리듀스 잡에 대한 이벤트와 설정을 갖고 있음, 잡 히스토리는 잡을 실행한 사용자에게 유용한 정보를 제공하기 위해 잡의 성공 여부와 상관없이 보관함
    - 잡 히스토리 파일은 1주일 동안 보관된 후 시스템에서 삭제됨
    - 히스토리 로그는 잡, 태스크, 태스크 시도 이벤트를 JSON 형태의 파일로 저장함 
- 맵리듀스 잡 페이지
  - Tracking UI를 클릭하면 애플리케이션 마스터 웹 UI(완료된 애플리케이션은 히스토리 페이지)로 이동함
  - Total 컬럼은 각 잡에 대해 맵과 리듀스 태스크의 전체 개수를 한 줄씩 보여줌, 태스크 상태를 Pending(대기 중), Running(실행 중), Complete(실행 성공)로 구분하여 보여줌
  - 맵 또는 리듀스 태스크에 대해 실패했거나 죽은 태스크 시도의 전체 개수를 보여줌, 투기적 실행 중복(speculative execution duplicate), 태스크를 실행하던 노드의 죽음, 사용자의 강제 종료로 태스크 시도가 실패했을 때는 killed로 표시됨 
- 하둡 로그
  - 로그 - 주 사용자 - 설명 
    - 시스템 데몬 로그 - 관리자 - 각 하둡 데몬은 로그파일과 표준 출력과 에러를 결합한 파일을 생성함 
    - HDFS 감사 로그 - 관리자 - HDFS와 관련된 모든 요청에 대한 로그로, 기본적으로 사용하지 않음 
    - 맵리듀스 잡 히스토리 로그 - 사용자 - 잡을 실행하는 과정에 발생하는 이벤트(태스크 완료와 같은)로그 
    - 맵리듀스 태스크 로그 - 사용자 - 각 태스크 자식 프로세스는 log4j를 사용하는 로그 파일(syslog로 불리는), 표준 출력(stdout)으로 보낸 데이터 파일, 표준 에러(stderr)를 위한 파일을 생성함 
  - YARN은 완료된 애플리케이션의 모든 태스크 로그를 가져온 후 병합하여 HDFS에 있는 기록 보관용 컨테이너 파일에 저장하는 로그 통합(log aggregation) 서비스를 제공함 
- 원격디버깅
  - 로컬에서 실패를 재현하기
    - 태스크 실패를 일으키는 파일을 로컬에 내려받은 후 로컬에서 잡을 실행하면 로컬에서 문제를 재현할 수 있음, 가능하면 자바의 VisualVM과 같은 디버거를 사용하라
  - JVM 디버깅 옵션 사용하기
    - 실패의 주된 이유는 태스크 JVM의 자바 메모리 부족 때문, jhat이나 이클립스 메모리 분석기(Eclipse Memory Analyzer)와 같은 도구로 사후 분석이 가능한 힙 덤프를 생성
  - 태스크 프로파일링 사용하기
    - 자바 프로파일러는 JVM에 대한 상당한 통찰력을 제공하며, 하둡은 잡의 일부 태스크에 대한 프로파일링 기법을 제공함 
  - 사후 분석을 위해서는 실패한 태스크 시도의 중간 파일을 보관하는 것이 유용함 
- 잡 튜닝하기
  - 성능 문제와 관련이 있는 하둡 특유의 유력 용의자(usal suspect)가 있음, 태스크 수준에서 프로파일링이나 최적화를 시도하기 전에 점검 목록을 순서대로 실행해 보는것이 좋음
    - 매퍼 수 - 매퍼가 얼마나 오랫동안 수행되고 있는가?
    - 리듀서 수 - 두 개 이상의 리듀서를 사용 중인지 확인해보라
    - 컴바이너 - 셔플을 통해 보내지는 데이터양을 줄이기 위해 컴바이너를 활용할 수 있는지 확인해보라
    - 중간 데이터 압축 - 맵 출력을 압축하면 잡 실행 시간을 거의 대부분 줄일 수 있음
    - 커스텀 직렬화 - 커스텀 Writable 객체나 비교기(comparator)를 사용하고 있다면 RawComparator를 구현했는지 반드시 확인해라
    - 셔플 튜닝 - 맵리듀스 셔플은 메모리 관리를 위해 대략 12개정도의 튜닝 인자를 제공하는데, 이는 성능을 조금이라도 더 향상시키는 데 도움을 줄 수 있음 
- 맵리듀스 작업 흐름
  - 데이터 처리가 더 복잡해지면 복잡한 맵과 리듀스 함수를 만드는 것보다는 맵리듀스 잡을 더 많이 만드는 것이 더 좋은 방법, 잡을 복잡하게 만들기보다는 잡을 더 많이 만드는 것이 좋음
  - 매우 복잡한 문제는 맵리듀스 대신 피그,하이브,캐스케이딩,크런치,스파크와 같은 고수준 언어를 사용하는 것이 좋음 
  - 맵리듀스잡으로 변환하는 작업을 하지 않아도 되므로 분석 작업에만 집중할 수 있다는 것 
- 아파치 오지(Apache OOzie)
  - 종속관계가 있는 여러 잡을 흐름에 따라 실행해주는 시스템
  - 워크플로 엔진은 다른 형태의 하둡잡(맵리듀스, 피그, 하이브 등)을 구성하는 작업 흐름을 저장하고 실행하며, 코디네이터 엔진(coordinator engine)은 미리 정의된 일정과 데이터 가용성을 기반으로 워크플로 잡을 실행함.
  - 오지는 확장 가능하도록 설계되었고 하둡 클러스터 내에 수천 개의 워크플로(수십 개의 연속적인 잡으로 구성된)를 시의 적절하게 실행하도록 관리함 
  - 오지는 실패한워크플로를 다시 실행할 때 성공한 부분에 대해서는 다시 실행하지 않기 때문에 효율적이며 따라서 시간 낭비도 없음 
  - 오지의 워크플로는 액션 노드(action node)와 제어흐름노드(control-flow node)로 이루어진 DAG
    - 액션 노드는 HDFS에 저장된 파일을 옮기거나 맵리듀스, 스트리밍, 피그, 하이브 잡을 실행하거나 스쿱 임포트를 수행하거나 쉘 스크립트나 자바 프로그램을 실행하는 등 워크플로의 태스크를 수행함 
    - 제어흐름 노드는 조건부 로직(분기문)과 같은 구문을 통해 액션 사이의 워크플로 실행을 관장함 
