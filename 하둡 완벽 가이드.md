# 서평
- 2005년 더크 커팅이 오픈 소스로 공개한 하둡은 분산 저장 파일시스템인 HDFS와 분산 병렬 처리를 담당하는 맵리듀스로 구성되어 있음, 하둡이 나온 후 이를 기반으로 피그, 하이브, HBase, 스쿱, 플룸 등의 오픈 소스 기술이 등장했고, 이들을 모두 결합한 빅데이터 기술은 통칭하여 하둡 에코시스템이라고 부르게 되었음, 스파크의 등장으로 대용량 데이터를 저장하고 일괄 처리하는 수준에서 벗어나 머신러닝과 실시간 분석까지 아우르게 되었음 
- 이 책은 하둡의 핵심, 하둡 에코시스템 관련 프로젝트, 하둡 사례 연구를 다루고 있으며 이 책을 통해 각 기술의 핵심을 설명했고, 다른 기술과의 관계도 친절하게 설명하고 있다.

# 하둡 기초
- 전체 데이터에 질의하기
  - 맵리듀스의 접근법은 무차별대입(brute-force) 방식 처럼 보임 , 맵리듀스 전체가 한 번의 쿼리로 전체나 상당한 규모의 데이터셋을 처리하는 것이기 때문 
  - 맵리듀스는 일괄 질의 처리기고, 전체 데이터셋을 대상으로 비정형(ad hoc) 쿼리를 수행하고 합리적인 시간 내에 그 결과를 보여주는 능력을 지니고 있음 
- 일괄 처리를 넘어서
  - 맵리듀스의 강점은 기본적으로 일괄 처리 시스템, 대화형 분석에는 적합하지 않음, 질의를 실행한 후 수 초 이내에 결과를 받는 것을 불가능함 
  - 온라인 접근을 지원하는 첫 번째 구성요소인 HBase는 HDFS를 기본 저장소로 하는 키-값 저장소, HBase는 개별 행에 대한 온라인 읽기/쓰기와 산적한 데이터를 읽고 쓰는 일괄 처리를 둘다 지원하기 때문에 애플리케이션을 구축하는 데 좋은 솔루션이 될 수 있음
  - YARN은 클러스터 자원 관리 시스템으로, 맵리듀스뿐만 아니라 어떤 분산 프로그램도 하둡 클러스터에 저장된 데이터를 처리할 수 있게 해줌 
  - 다양한 처리 패턴
    - 대화형 SQL
      - 대화형 sQL은 맵리듀스 대신 장기 실행 전용 데몬(임팔라)이나 컨테이너를 재사용하는(테즈 기반의 하이브) 분산 쿼리 엔진을 사용함 
    - 반복 처리
      - 머신러닝과 같은 다수의 알고리즘은 근본적으로 반복 연산을 함 
    - 스트림 처리
      - 스톰, 스파크 스트리밍, 삼자와 같은 스트리밍 시스템은 실시간으로 실행되고 경계가 없는 스트림 데이터를 분산 계산하여 그 결과를 하둡 저장소나 외부 시스템에 보낼 수 있음
    - 검색
      - 솔라(Solr) 검색 플랫폼은 하둡 클러스터에서 실행될 수 있음, 솔라는 문서를 색인하여 HDFS에 저장하고 HDFS에 저장된 색인을 기반으로 검색 쿼리를 제공함 

# 맵리듀스 
- 분산형으로 확장하기
  - 데이터 흐름
    - 맵리듀스 잡은 클라이언트가 수행하는 작업의 기본 단위, 잡은 입력 데이터, 맵리듀스 프로그램, 설정 정보로 구성됨
    - 하둡은 잡을 맵 태스크와 리듀스 태스크로 나누어 실행함, 각 태스크는 YARN을 이용하여 스케줄링되고 클러스터의 여러 노드에서 실행됨, 특정 노드의 태스크 하나가 실패하면 자동으로 다른 노드를 재할당하여 다시 실행됨 
    - 하둡은 맵리듀스 잡의 입력을 입력 스플릿(input split) 또는 단순히 스플릿이라고 부르는 고정 크기 조각으로 분리함, 하둡은 각 스플릿마다 하나의 맵 태스크를 생성하고 스플릿의 각 레코드를 사용자 정의 맵 함수로 처리함 
    - 데이터 지역성 최적화(data locality optimiztation) : 하둡은 HDFS 내의 입력 데이터가 있는 노드에서 맵 태스크를 실행할 때 가장 빠르게 작동함, 클ㄹ러스터의 중요한 공유 자원인 네트워크 대역폭을 사용하지 않는 방법 
  - 컴바이너 함수
    - 하둡은 맵의 결과를 처리하는 컴바이너 함수(컴바이너 함수의 출력이 결국 리듀스 함수의 입력이 됨)를 허용, 컴바이너 함수는 최적화와 관련이 있음 
    - 하둡은 컴바이너 함수의 호출 빈도와 상관없이 리듀스의 결과가 언제나 같도록 보장함 
    - 컴바이너를 사용하면 매퍼와 리듀서 사이의 셔플 단계에서 전송되는 데이터양을 줄이는 데 큰 도움이됨 
    - 컴바이너 함수는 Reducer 클래스를 사용해서 정의함, MaxTemperatureReducer에 있는 리듀서 함수와 동일한 구현체를 사용함 
  - 분산 맵리듀스 잡 실행하기
    - 맵리듀스는 데이터 크기와 하드웨어 성능에 따라 확장할 수 있는데 이것이 바로 맵리듀스의 핵심임
- 하둡 스트리밍
  - 자바 외에 다른 언어로 맵과 리듀스 함수를 작성할 수 있는 맵리듀스 API를 제공함
  - 하둡과 사용자 프로그램 사이의 인터페이스로 유닉스 표준 스트림을 사용함, 사용자는 표준 입력을 읽고 표준 출력으로 쓸 수 있는 다양한 언어를 이용하여 맵리듀스 프로그램을 작성할 수 있음 
  - 스트리밍은 그 특성상 텍스트 처리에 매우 적합함, 맵의 입력 데이터는 표준 입력으로 맵 함수에 전달되고, 행 단위로 처리되어 표준 출력으로 쓰여짐, 맵의 출력 키-값 쌍은 탭으로 구분된 하나의 행으로 출력됨

# 하둡 분산 파일시스템
- 네트워크로 연결된 여러 머신의 스토리지를 관리하는 파일시스템
- HDFS(Hadoop Distributed FileSystem)라는 분산 파일시스템을 제공함
- HDFS 설계
  - 범용 하드웨어로 구성된 클러스터에서 실행되고 스트리밍 방식의 데이터 접근 패턴으로 대용량 파일을 다룰 수 있도록 설계된 파일시스템
  - 설계 특성
    - 매우 큰 파일
    - 스트리밍 방식의 데이터 접근
    - 범용 하드웨어
      - 하둡은 노드 장애가 발생할 확률이 높은 범용 하드웨어(여러 업체에서 제공하는 쉽게 구할 수 있는 하드웨어)로 구성된 대형 클러스터에서 문제없이 실행되도록 설계됨 
  - HDFS가 잘 맞지 않는 응용 분야
    - 빠른 데이터 응답시간
      - HDFS는 높은 데이터 처리량을 제공하기 위해 최적화되어 있고 이를 위해 응답 시간을 희생함, HBase가 하나의 대안이 될 수 있음
    - 수많은 작은 파일
    - 다중 라이터와 파일의 임의 수정
      - HDFS는 단일 라이터로 파일을 씀, 한 번 쓰고 끝나거나 파일의 끝에 덧붙이는 것은 가능하지만 파일에서 임의 위치에 있는 내용을 수정하는 것은 허용하지 않으며 다중 라이터도 지원하지 않음
- HDFS 개념
    - 블록 
        - 블록 크기는 한 번에 읽고 쓸 수 있는 데이터의 최대량 
        - HDFS 블록이 큰 이유
            - HDFS 블록은 디스크 블록에 비해 상당히 큼, 탐색 비용을 최소화하기 위해서, 블록이 매우 크면 블록의 시작점을 탐색하는데 걸리는 시간을 줄일 수 있고 데이터를 전송하는 데 더 많은 시간을 할애할 수 있음, 여러 개의 블록으로 구성된 대용량 파일을 전송하는 시간은 디스크 전송 속도에 크게 영향을 받음
        - 블록 추상화 개념을 도입하면서 얻은 이점
            - 파일 하나의 크기가 단일 디스크의 용량보다 더 커질 수 있다는 것
            - 파일 단위보다는 블록 단위로 추상화를 하면 스토리지의 서브시스템을 단순하게 만들 수 있다는 것 
            - 블록은 내고장성(fault tolerance)과 가용성(availability)을 제공하는 데 필요한 복제(replication)를 구현할 때 매우 적합함,블록의 손상과 디스크 및 머신의 장애에 대처하기 위해 각 블록은 물리적으로 분리된 다수의 머신(보통 3개)에 복제됨
            - 만일 하나의 블록을 이용할 수 없는 상황이 되면 다른 머신에 있는 복사본을 읽도록 클라이언트에 알려주면 됨 
    - 네임노드와 데이터노드
        - HDFS 클러스터는 마스터-워커 패턴으로 동작하는 두 종류의 노드, 마스터인 하나의 네임노드와 워커인 여러 개의 데이터노드로 구성되어 있음, 네임노드는 파일시스템의 네임스페이스를 관리함
        - 네임노드는 파일시스템 트리와 그 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터를 유지함 , 네임스페이스 이미지와 에디트 로그라는 두 종류의 파일로 로컬 디스크에 영속적으로 저장됨, 파일에 속한 모든 블록이 어느 데이터노드에 있는지 파악하고 있음 
        - HDFS 클라이언트는 사용자를 대신해서 네임노드와 데이터노드 사이에서 통신하고 파일시스템에 접근함 
        - 네임노드의 장애복구 기능
            - 파일시스템의 메타데이터를 지속적인 상태로 보존하기 위해 파일로 백업하는 것, 네임노드가 다수의 파일시스템에 영구적인 상태를 저장하도록 하둡을 구성할 수 있음, 백업 작업은 동기화되고 원자적으로 실행됨, 주로 권장하는 방법은 로컬 디스크와 원격의 NFS 마운트를 두 곳에 동시에 백업하는 것 
            - 보조 네임노드(secondary namenode)를 운영하는 것, 에디트 로그가 너무 커지지 않도록 주기적으로 네임스페이스 이미지를 에디트 로그와 병합하여 새로운 네임스페이스 이미지를 만드는 것, 주 네임노드에 장애가 발생할 것을 대비해서 네임스페이스 이미지의 복제본을 보관하는 역할을 함 
    - 블록 캐싱
        - 오프힙(off-heap, 자바 힙 외부에서 관리되는) 블록 캐시라는 데이터노드의 메모리에 명시적으로 캐싱할 수 있음
        - 조인을 할 때 작은 룩업 테이블을 캐싱하는 것은 좋은 활용사례임
        - 사용자나 애플리케이션은 캐시 풀(cache pool)에 캐시 지시자(cache directive)를 추가하여 특정 파일을 캐싱하도록 명령할 수 있음, 캐시 풀은 캐시 권한이나 자원의 용도를 관리하는 관리 그룹의 역할을 맡음 
- HDFS 패더레이션
    - HDFS 페더레이션을 적용하면 각 네임노드는 네임스페이스의 메타데이터를 구성하는 네임스페이스 볼륨과 네임스페이스에 포함된 파일의 전체 블록을 보관하는 블록 풀을 관리함 
- HDFS 고가용성
    - 네임노드는 여전히 단일 고장점(single point of failure - SPOF), 네임노드에 장애가 발생하면 맵리듀스 잡을 포함하여 모든 클라이언트가 파일을 읽거나 쓰거나 조회할 수 없게 됨, 네임노드는 메타데이터와 파일 블록의 매핑 정보를 보관하는 유일한 저장소이기 때문
    - 새로운 네임노드는 네임스페이스 이미지를 메모리에 로드하고 -> 에디트 로그를 갱신하고 -> 전체 데이터노드에서 충분한 블록 리포트를 받아 안전 모드를 벗어날 때까지 그 어떤 요청도 처리하지 못함 
    - HDFS 고가용성(high availability -HA)을 지원함, 고가용성은 활성대기상태로 설정된 한 쌍의 네임노드로 구현됨, 활성네임노드에 장애가 발생하면 대기 네임노드가 그 역할을 이어받아 큰 중단없이 클라이언트의 요청을 처리함         
- 장애복구와 펜싱
  - 대기 네임노드를 활성화시키는 전환 작업은 장애복구 컨트롤러라는 새로운 객체로 관리됨
  - 장애복구는 정기적인 유지관리를 위해 관리자가 수동으로 초기화할 수 있음
  - 우아한 장애복구(graceful failover) - 자애복구 컨트롤러는 두 개의 네임노드가 서로 역할을 바꾸게 하는 방법으로 전환 순서를 제어할 수 있음 
- 인터페이스
  - HTTP
    - HTTP로 HDFS에 접근하는 두 가지 방식
      - 클라이언트의 HTTP 요청을 HDFS 데몬이 직접 처리하는 방식 - 네임노드와 데이터노드에 내장된 웹 서버가 WebHDFS의 말단으로 적용함
      - 클라이언트 대신 DistributedFileSystem API로 HDFS에 접근하는 프록시를 경유하는 방식 - 하나 또는 그 이상의 독립(standalone) 프록시 서버를 통하는 것, 프록시 서버는 상태를 저장할 필요가 없으므로 표준 로드 밸런서를 사용해도 괜찮음, 클러스터의 모든 트래픽은 프록시를 경유하므로 클라이언트는 네임노드와 데이터노드에 직접 접근할 필요가 없음, 프록시를 통하면 엄격한 방화벽이나 대역폭 제한 정책을 적용하기 쉬움, 프록시를 통한 방식은 서로 다른 데이터 센터에 있는 하둡 클러스터 사이의 데이터 전송이나 외부 네트워크에 있는 클라우드에서 운영되는 하둡 클러스터에 접근할 때 일반적으로 이용되는 방법 
  - C 
    - 자바 FileSystem 인터페이스를 모방한 libhdfs라는 C 라이브러리를 제공함
    - libhdfs는 자바 파일시스템 클라이언트를 호출하기 위해 자바 네이티브 인터페이스(JNI)를 사용함 
  - NFS
    - NFSv3 게이트웨이를 이용하면 로컬 클라이언트 파일시스템에 HDFS를 마운트할 수 있음
    - 파일시스템을 다루는 ls나 cat 같은 Unix 유틸리티를 이용할 수 있으며, 파일 업로드 및 일반적인 프로그래밍 언어에서 파일시스템을 다루는 POSIX 라이브러리도 사용할 수 있음
  - FUSE
    - Filesystem in Userspace (사용자 공간에서의 파일시스템)로, 사용자 공간과 유닉스 파일시스템을 통합한 파일시스템을 지원함 
# YARN
- 하둡의 클러스터 자원 관리 시스템, 클러스터의 자원을 요청하고 사용하기 위해 API를 제공함 
- 맵리듀스, 스파크 등과 같은 분산 컴퓨팅 프레임워크는 클러스터 계산 계층(YARN)과 클러스터 저장 계층(HDFS와 HBase) 위에서 YARN 애플리케이션을 실행함
- 리소스 매니저와 노드매니저 등 두가지 유형의 장기 실행 데몬을 통해 핵심 서비스를 제공함
  - 클러스터에서 유일한 리소스 매니저는 클러스터 전체 자원의 사용량을 관리함
  - 모든 머신에서 실행되는 노드 매니저는 컨테이너를 구동하고 모니터링하는 역할을 맡음 
- 자원 요청
  - 분산 데이터 처리 알고리즘에서 클러스터의 네트워크 대역폭을 효율적으로 활용하기 위해서는 지역성을 보장하는 것이 가장 중요함
  - YARN은 특정 애플리케이션이 호출한 컨테이너에 대해 지역성 제약을 규정하는 것을 허용함, 지역성 제약은 특정 노드나 랙 또는 클러스터의 다른 곳(외부 랙)에서 컨테이너를 요청할 때 사용됨 
- 애플리케이션 수명
  - 실행 시간 보다는 사용자가 실행하는 잡의 방식에 따라 애플리케이션을 분리하는 것이 좋음
    - 사용자의 잡 당 하나의 애플리케이션이 실행되는 방식으로, 맵리듀스 잡이 여기에 속함
    - 워크플로나 사용자의 잡 세션(잡은 서로 관련이 없을 수도 있음)당 하나의 애플리케이션이 실행되는 방식, 첫 번째 유형보다 훨씬 더 효율적, 순차적으로 실행되는 잡이 동일한 컨테이너를 재사용할 수 있기 때문, 잡 사이에 공유 데이터를 캐싱할 수 있는 큰 장점도 있음, 사례로는 스파크
    - 서로 다른 사용자들이 공유할 수 있는 장기 실행 애플리케이션, 일종의 코디네이션 역할을 수행함, 아파치 슬라이더는 클러스터에서 다양한 애플리케이션을 구동시키는 장기 실행 애플리케이션 마스터를 가지고 있응, 임팔라는 여러 임팔라 데몬이 클러스터 자원을 요청할 수 있도록 프록시 애플리케이션을 제공함 
- YARN 애플리케이션 만들기
  - 잡의 방향성 비순환 그래프(DAG)를 실행하고 싶으면 스파크나 테즈가 더 작합함, 스트리밍 처리는 스파크, 쌈자 또는 스톰을 사용하는 것이 좋음 
  - 아파치 슬라이더는 기존의 분산 애플리케이션을 YARN 위에서 실행하도록 해줌 
  - 아파치 트윌(Apache Twill) 
    - 슬라이더와 비슷하지만 YARN에서 실행되는 분산 애플리케이션을 개발할 수 있는 간단한 프로그래밍 모델을추가로 제공함
    - 자바 Runnable 객체를 확장한 클러스터 프로세스를 정의한 후 클러스터의 YARN 컨테이너에서 이를 실행하는 기능을 제공함
    - 실시간 로깅(runnalbes의 로그 이벤트를 클라이언트에 스트리밍으로 돌려줌)과 명령 메시지(클라이언트에서 runnables 전송) 기능 등을 제공함 
  - 분산 쉘(distributed shell) - 복잡한 스케줄링 요구사항이 있는 애플리케이션
    - 클라이언트 또는 애플리케이션 마스터가 YARN 데몬과 통신하기 위해 YARN의 클라이언트 API를 어떻게 사용하는지 잘 보여주고 있음 
- YARN과 맵리듀스1의 차이점
  - 맵리듀스 1에는 잡의 실행과정을 제어하는 하나의 잡트래커와 하나 이상의 태스크트래커등 두 종류의 데몬이 있음
  - 잡트래커
    - 여러 태스크트래커에서 실행되는 태스크를 스케줄링함으로써 시스템에서 실행되는 모든 잡을 조율함
    - 맵리듀스 1에서 잡 스케줄링(태스크와 태스크트래커를 연결)과 태스크 진행 모니터링(태스크를 추적하고, 실패하거나 느린 태스크를 다시 시작하고, 전체 카운터를 유지하는 방법으로 태스크 장부(bookeeping)을 맡고 있음, 반면 YARN은 이러한 역할을 분리된 객체인 리소스 매니저와 애플리케이션 마스터(맵리듀스 잡당 하나)를 통해 처리함
    - 완료된 잡에 대한 잡 이력을 저장하는 역할을 맡음, 잡트래커의 부하를 줄이기 위해 별도의 데몬인 히스토리 서버를 통해 수행될 수도 있음, YARN에서 이와 동일한 역할은 애플리케이션의 이력을 저장하는 타임라인 서버가 맡고 있음 
  - 태스크트래커
    - 태스크를 실행하고 진행 상황을 잡트래커에 전송하기 때문에 잡트래커는 각 잡의 전체적인 진행 상황을 파악할 수 있음 
  - 맵리듀스1과 YARN 컴포넌트의 비교
    - 잡트래커 - 리소스 매니저, 애플리케이션 마스터, 타임라인 서버
    - 태스크트래커 - 노드 매니저
    - 슬롯 - 컨테이너
  - YARN을 사용하여 얻을 수 있는 이익
    - 확장성
      - 맵 리듀스 1보다 큰 클러스터에서 실행될 수 있음
    - 가용성
      - 고가용성(high availability - HA)은 서비스 데몬에 문제가 발생했을 때 서비스에 필요한 작업을 다른 데몬이 이어받을 수 있도록 상태 정보를 항상 복사해두는 방법으로 구현함 
    - 효율성
    - 멀티테넌시(다중 사용자)
      - 하둡이 맵리듀스를 뛰어넘어 다양한 분산 애플리케이션을 수용할 수 있다는 것 
- YARN 스케줄링
  - 스케줄러 옵션
    - FIFO, 캐퍼시티(가용량), 페어(균등) 스케줄러를 제공함 
    - FIFO
      - 애플리케이션을 큐에 하나씩 넣고 제출된 순서에 따라 순차적으로 실행함(선입선출 방식), 큐에 처음으로 들어온 애플리케이션 요청을 먼저 할당하고, 이 요청을 처리한 후 큐에 있는 다음 애플리케이션 요청을 처리하는 방식으로 순차적으로 실행함 
      - 공유 클러스터 환경에서는 적합하지 않음, 대형 애플리케이션이 수행될 때는클러스터의 모든 자원을 점유해버릴 수 있기 때문에 다른 애플리케이션은 자기 차례가 올때 까지 계속 대기해야함, 다른 두 스케줄러는 장시간 수행되는 잡을 계속 처리하는 동시에 작은 비정형 질의도 중간에 실행하여 적당한 시간 내에 사용자가 결과를 얻을 수 있도록 허용함 
    - 캐퍼시티(Capacity)
      - 작은 잡을 제출되는 즉시 분리된 전용 큐에서 처리함
      - 물론 해당 큐는 잡을 위한 자원을 미리 예약해두기 때문에 전체 클러스터의 효율성은 떨어짐, 또한 대형 잡은 FIFO 스케줄러보다 늦게 끝나게 됨 
      - 회사의 조직 체계에 맞게 하둡 클러스터를 공유할 수 있음, 각 조직은 전체 클러스터의 지정된 가용량을 미리 할당받음, 각 조직은 분리된 전용 큐를 가지며 클러스터 가용량의 지정된 부분을 사용하도록 설정할 수 있음 
    - 페어(Fair)
      - 실행 중인 모든 잡의 자원을 동적으로 분배하기 때문에 미리 자원의 가용량을 예약할 필요가 없음, 대형 잡이 먼저 시작되면 이때는 실행 중인 잡이 하나밖에 없기 때문에 클러스터의 모든 자원을 얻을 수 있음
      - 대형 잡이 실행되는 도중에 작은 잡이 추가로 시작되면 페어 스케줄러는 클러스터 자원의 절반을 이 잡에 할당함, 각 잡은 클러스터의 자원을 공평하게 사용할 수 있게 됨 
      - 실행 중인 모든 애플리케이션에 동일하게 자원을 할당함 

# 하둡 I/O
- 압축
  - 파일 압축은 파일 저장 공간을 줄이고, 네트워크 또는 디스크로부터 데이터 전송을 고속화할 수 있는 두 가지 커다란 이점이 있음 
  - 압축 포맷의 요약
    - 압축 포맷 - 도구 - 알고리즘 - 파일 확장명 - 분할 가능
    - DEFLATE - N/A - DEFLATE - .deflate - No
    - gzip - gzip - DEFLATE - .gz - No
    - bzip2 - bzip2 - bzip2 - .bz2 - Yes
    - LZO - lzop - LZO - .lzo - No
    - LZ4 - N/A - LZ4 - .lz4 - No
    - Snappy - N/A - Snappy - .snappy - No 
  - gzip은 일반적인 목적의 압축 도구고, 공간/시간 트레이드오프의 중앙에 위치함
  - bzip2는 gzip보다 더 효율적으로 압축하지만 대신 더 느림, 압축 해제 속도는 압축 속도보다 더 빠르지만 여전히 다른 포맷에 비해 더 느림
  - LZO, LZ4, Snappy 모두 속도에 최적화되었고 gzip보다 어느 정도 빠르지만 압축 효율 은 떨어짐 
  - 압축 포맷 사용
    - 압축과 분할 모두를 지원하는 시퀀스 파일, 에이브로, ORCFile, 파케이 같은 컨테이너 파일 포맷을 사용하라, 보통 LZO, LZ4, Snappy와 같은 빠른 압축 형식이 적당함
    - 상당히 느리긴 하지만 bzip2 같은 분할을 지원하는 압축 포맷을 사용하라, 또는 분할을 지원하기 위해 색인 될 수 있는 LZO 같은 포맷을 사용하라 
    - 애플리케이션에 파일을 청크로 분할하고, 지원되는 모든 압축 포맷(분할에 관계없이)을 사용하여 각 청크를 개별적으로 압축하라. 이 경우 압축된 청크가 거의 HDFS 블록 하나의 크기가 되도록 처크 크기를 선택해야 함 
    - 파일을 압축하지 말고 그냥 저장하라 
- 직렬화(serialization)
  - 네트워크 전송을 위해 구조화된 객체를 바이트 스트림으로 전환하는 과정
  - 역직렬화(deserializaition) - 바이트 스트림을 일련의 구조화된 객체로 역전환하는 과정
  - 직렬화는 프로세스 간 통신과 영속적인 저장소와 같은 분산 데이터 처리의 독특한 두 영역에서 나타남 
  - 하둡 시스템에서 노드 사이의 프로세스 간 통신은 원격 프로시저 호출(Remote Procedure Call(RPC))을 사용하여 구현됨
  - RPC 직렬화 포맷이 유익한 이유
    - 간결성
    - 고속화
    - 확장성
    - 상호운용성 
  - 저장소 포맷은 간결하고(저장 공간의 효율적 사용을 위해), 빠르고(테라바이트 데이터를 읽고 쓰는 오버헤드를 최소화하기 위해), 확장 가능하고(예전 포맷으로 쓰인 데이터도 문제없이 읽기 위해), 상호운용(다양한 언어를 사용하여 영속적인 데이터를 읽고 쓰기 위해)할 수 있어야 함 

# 맵리듀스
## 맵리듀스 프로그래밍
- 맵리듀스 웹 UI
  - 클러스터 메트릭스(Cluster Metrics) 부분에서 클러스터의 요약 정보를 볼 수 있음, 클러스터(그리고 다양한 상태)에서 현재 실행 중인 애플리케이션의 개수, 클러스터 가용 자원의 수량(전체 메모리 - Memory Total), 노드 매니저 정보가 포함되어 있음
  - 메인 테이블은 클러스터에서 완료되었거나 현재 실행 중인 모든 애플리케이션을 보여줌, 특정 애플리케이션을 찾을 수 있도록 검색 창을 제공함 
  - 잡 히스토리
    - 완료된 맵리듀스 잡에 대한 이벤트와 설정을 갖고 있음, 잡 히스토리는 잡을 실행한 사용자에게 유용한 정보를 제공하기 위해 잡의 성공 여부와 상관없이 보관함
    - 잡 히스토리 파일은 1주일 동안 보관된 후 시스템에서 삭제됨
    - 히스토리 로그는 잡, 태스크, 태스크 시도 이벤트를 JSON 형태의 파일로 저장함 
- 맵리듀스 잡 페이지
  - Tracking UI를 클릭하면 애플리케이션 마스터 웹 UI(완료된 애플리케이션은 히스토리 페이지)로 이동함
  - Total 컬럼은 각 잡에 대해 맵과 리듀스 태스크의 전체 개수를 한 줄씩 보여줌, 태스크 상태를 Pending(대기 중), Running(실행 중), Complete(실행 성공)로 구분하여 보여줌
  - 맵 또는 리듀스 태스크에 대해 실패했거나 죽은 태스크 시도의 전체 개수를 보여줌, 투기적 실행 중복(speculative execution duplicate), 태스크를 실행하던 노드의 죽음, 사용자의 강제 종료로 태스크 시도가 실패했을 때는 killed로 표시됨 
- 하둡 로그
  - 로그 - 주 사용자 - 설명 
    - 시스템 데몬 로그 - 관리자 - 각 하둡 데몬은 로그파일과 표준 출력과 에러를 결합한 파일을 생성함 
    - HDFS 감사 로그 - 관리자 - HDFS와 관련된 모든 요청에 대한 로그로, 기본적으로 사용하지 않음 
    - 맵리듀스 잡 히스토리 로그 - 사용자 - 잡을 실행하는 과정에 발생하는 이벤트(태스크 완료와 같은)로그 
    - 맵리듀스 태스크 로그 - 사용자 - 각 태스크 자식 프로세스는 log4j를 사용하는 로그 파일(syslog로 불리는), 표준 출력(stdout)으로 보낸 데이터 파일, 표준 에러(stderr)를 위한 파일을 생성함 
  - YARN은 완료된 애플리케이션의 모든 태스크 로그를 가져온 후 병합하여 HDFS에 있는 기록 보관용 컨테이너 파일에 저장하는 로그 통합(log aggregation) 서비스를 제공함 
- 원격디버깅
  - 로컬에서 실패를 재현하기
    - 태스크 실패를 일으키는 파일을 로컬에 내려받은 후 로컬에서 잡을 실행하면 로컬에서 문제를 재현할 수 있음, 가능하면 자바의 VisualVM과 같은 디버거를 사용하라
  - JVM 디버깅 옵션 사용하기
    - 실패의 주된 이유는 태스크 JVM의 자바 메모리 부족 때문, jhat이나 이클립스 메모리 분석기(Eclipse Memory Analyzer)와 같은 도구로 사후 분석이 가능한 힙 덤프를 생성
  - 태스크 프로파일링 사용하기
    - 자바 프로파일러는 JVM에 대한 상당한 통찰력을 제공하며, 하둡은 잡의 일부 태스크에 대한 프로파일링 기법을 제공함 
  - 사후 분석을 위해서는 실패한 태스크 시도의 중간 파일을 보관하는 것이 유용함 
- 잡 튜닝하기
  - 성능 문제와 관련이 있는 하둡 특유의 유력 용의자(usal suspect)가 있음, 태스크 수준에서 프로파일링이나 최적화를 시도하기 전에 점검 목록을 순서대로 실행해 보는것이 좋음
    - 매퍼 수 - 매퍼가 얼마나 오랫동안 수행되고 있는가?
    - 리듀서 수 - 두 개 이상의 리듀서를 사용 중인지 확인해보라
    - 컴바이너 - 셔플을 통해 보내지는 데이터양을 줄이기 위해 컴바이너를 활용할 수 있는지 확인해보라
    - 중간 데이터 압축 - 맵 출력을 압축하면 잡 실행 시간을 거의 대부분 줄일 수 있음
    - 커스텀 직렬화 - 커스텀 Writable 객체나 비교기(comparator)를 사용하고 있다면 RawComparator를 구현했는지 반드시 확인해라
    - 셔플 튜닝 - 맵리듀스 셔플은 메모리 관리를 위해 대략 12개정도의 튜닝 인자를 제공하는데, 이는 성능을 조금이라도 더 향상시키는 데 도움을 줄 수 있음 
- 맵리듀스 작업 흐름
  - 데이터 처리가 더 복잡해지면 복잡한 맵과 리듀스 함수를 만드는 것보다는 맵리듀스 잡을 더 많이 만드는 것이 더 좋은 방법, 잡을 복잡하게 만들기보다는 잡을 더 많이 만드는 것이 좋음
  - 매우 복잡한 문제는 맵리듀스 대신 피그,하이브,캐스케이딩,크런치,스파크와 같은 고수준 언어를 사용하는 것이 좋음 
  - 맵리듀스잡으로 변환하는 작업을 하지 않아도 되므로 분석 작업에만 집중할 수 있다는 것 
- 아파치 오지(Apache OOzie)
  - 종속관계가 있는 여러 잡을 흐름에 따라 실행해주는 시스템
  - 워크플로 엔진은 다른 형태의 하둡잡(맵리듀스, 피그, 하이브 등)을 구성하는 작업 흐름을 저장하고 실행하며, 코디네이터 엔진(coordinator engine)은 미리 정의된 일정과 데이터 가용성을 기반으로 워크플로 잡을 실행함.
  - 오지는 확장 가능하도록 설계되었고 하둡 클러스터 내에 수천 개의 워크플로(수십 개의 연속적인 잡으로 구성된)를 시의 적절하게 실행하도록 관리함 
  - 오지는 실패한워크플로를 다시 실행할 때 성공한 부분에 대해서는 다시 실행하지 않기 때문에 효율적이며 따라서 시간 낭비도 없음 
  - 오지의 워크플로는 액션 노드(action node)와 제어흐름노드(control-flow node)로 이루어진 DAG
    - 액션 노드는 HDFS에 저장된 파일을 옮기거나 맵리듀스, 스트리밍, 피그, 하이브 잡을 실행하거나 스쿱 임포트를 수행하거나 쉘 스크립트나 자바 프로그램을 실행하는 등 워크플로의 태스크를 수행함 
    - 제어흐름 노드는 조건부 로직(분기문)과 같은 구문을 통해 액션 사이의 워크플로 실행을 관장함 

# 맵리듀스 작동 방법
- 맵리듀스 잡 실행 상세분석
  - 클라이언트 : 맵리듀스 잡을 제출함
  - YARN 리소스 매니저 : 클러스터 상에 계산 리소스 할당을 제어함
  - YARN 노드 매니저 : 클러스터의 각 머신에서 계산 컨테이너를 시작하고 모니터링함 
  - 맵리듀스 애플리케이션 마스터 : 맵리듀스 잡을 수행하는 각 태스크를 제어함, 애플리케이션 마스터와 맵리듀스 태스크는 컨테이너 내에서 실행되며, 리소스 매니저는 잡을 할당하고 노드 매니저는 태스크를 관리하는 역할을 맡음
  - 분산 파일시스템 : 다른 단계 간에 잡 리소스 파일들을 공유하는 데 사용됨
- 실패
  - 사용자 코드의 버그 때문에 프로세스가 강제로 죽거나 서버에 장애가 발생하는 일이 빈번함 
  - 태스크 실패
    - 가장 흔한 실패의 유형은 맵 또는 리듀스 태스크 내 사용자 코드에서 런타임 예외를 던질 때
    - 예외가 발생하면 태스크 JVM은 종료하기 전에 부모인 애플리케이션 마스터에 에러를 보고함, 이 에러는 최종적으로 사용자 로그에 기록됨
    - 애플리케이션 마스터는 이 태스크 시도를 실패로 표시하고 해당 리소스를 다른 태스크에서 사용 가능하도록 컨테이너를 풀어줌
    - 태스크 JVM이 갑작스럽게 종료하는 것인데, 아마도 맵리듀스 사용자 코드에의해 드러난 특정 상황으로 인해 JVM이 종료되는 JVM 버그, 이때 노드 매니저는 프로세스가 종료되었음을 알게 되고,이를 애플리케이션 마스터에 알려주어 해당 시도가 실패했다고 표시
    - 행이 걸린(멈춘) 태스크는 다르게 처리됨
      - 애프리케이션 마스터는 잠시 동안 진행 상황을 갱신받지 못함을 알게 되며 해당 태스크를 실패로 표시함
      - 태스크 JVM 프로세스는 이 기간 후에 자동으로 강제 종료됨
      - 태스크를 실패로 간주하는 타임아웃 기간은 보통 10분이고 잡 단위(또는 클러스터 단위)로 mapreduce.task.timeout 속성에 밀리초 단위의 값을 설정할 수 있음
      - 타임아웃을 0으로 설정하면 타임아웃을 비활성화하며 따라서 실행 시간이 긴 태스크는 절대로 실패로 표시되지 않음, 행이 걸려 멈춘 태스크는 자신의 컨테이너를 결코 해제하지 않을 것이며 시간이 지남에 따라 클러스터를 느리게 만드는 결과를 초래함
      - 태스크가 주기적으로 진행 상황을 확실히 보고하도록 하는것이 좋음
  - 노드 매니저 실패
    - 노드 매니저가 크래시에 의해 실패하거나 굉장히 느리게 수행 중이라면 리소스 매니저에 하트비트 전송을 중단할 것(혹은 굉장히 드물게 전송함)
- 셔플과 정렬
  - 맵리듀스는 모든 리듀서의 입력이 키를 기준으로 정렬되는 것을 확실히 보장함
  - 정렬히 수행하고 맵의 출력을 리듀서의 입력으로 전송하는 과정을 셔플이라고 함 
  - 스필(spill) : 각 파티션별 메모리 버퍼를 디스크에 한번에 쓰는 것을 말함
- 투기적 실행
  - 태스크의 투기적 실행(speculative execution) : 하둡은 느린 태스크를 진단하거나 고치려 하지 않는 대신 태스크 수행이 예상했떤 것보다 더 느린 상황을 감지하여 또 다른 동일한 예비 태스크를 실행함

# 맵리듀스 기능
- 카운터
  - 내장 카운터
    - 하둡은 모든 잡에 대해 내장 카운터를 제공하며 이들은 다양한 메트릭(지표)을 알려줌
    - 맵리듀스 태스크 카운터
    - 파일시스템 카운터
    - FileInputFormat 카운터
    - FileOutputFormat 카운터
    - 잡 카운터
  - 내장 카운터 그룹
    - 태스크 카운터
      - 태스크가 진행되면서 갱신됨
      - 각 태스크가 실행될 때 해당 태스크에 대한 정보를 수집한 후 잡의 모든 태스크에 대한 값을 취합하여 최종 결과를 알려줌 
      - 각 태스크 시행마다 관리되고 주기적으로 애플리케이션 마스터에 전송되므로 결국 전역적으로 수집됨 
    - 잡 카운터
      - 잡이 진행되면서 갱신됨
- 조인
  - 매퍼에 의해 조인이 수행되면 맵-사이즈 조인(map_side join), 리듀서에 의해 수행되면 리듀스-사이드 조인(reduce-side join) 
  - 맵-사이드 조인
    - 대용량 입력에 대한 맵-사이드 조인은 데이터가 맵 함수에 도달하기 전에 조인이 수행됨, 각 맵의 입력이 특별한 방식으로 분할되고 정렬되어야 함
    - 각 입력 데이터 셋은 반드시 동일한 개수의 파티션으로 분할되어야 하며, 각 원본은 동일한 조인키로 정렬되어 있어야함, 특정 키에 대한 모든 레코드는 동일한 파티션에 존재해야 함 
    - 동일한 개수의 리듀서, 동일한 키, 분리되지 않는 추력 파일(예를 들면 HDFS 블록보다 작거나 gzip으로 압축된 파일)을 가진 여러 잡의 출력을 조이하는 데 사용할 수 있음 
  - 리듀스-사이드 조인
    - 입력 데이터셋을 일부러 특별한 방식으로 구조화할 피룡가 없기 때문, 두 데이터셋 모두 맵리듀스의 셔플 단계를 거쳐야 한다는 비효율적인 면이 존재함
    - 매퍼가 소스에 따라 각 레코드에 태그를 붙이고 조인키를 맵 출력키로 사용함으로써 동일한 키를 가진 레코드는 같은 리듀서와 함께 모이게 된다는 것
      - 다중 입력
        - MultipleInputs 클래스를 사용하여 각 입력 원본을 분석하고 태깅하는 코드를 별도로 작성하는 것이 더 편함
      - 2차 정렬
- 사이드 데이터 분배
  - 사이드 데이터는 잡이 주요 데이터셋을 처리하는 데 필요한 별도의 읽기 전용 데이터
  - 잡 환경 설정 사용
    - Configuration(이전 맵리듀스 API에서는 JobConf)의 다양한 setter 메서드를 사용하여 잡 환경 설정에 임의의 키-값 쌍을 설정할 수 있음, 작은 크기의 메타데이터를 각 태스크에 전달할 때 매우 유용함 
    - 수 킬로바이트가 넘는 데이터를 전송할 때는 적합하지 않음, 맵리듀스 컴포넌트의 메모리 사용량에 부하를 주기 때문
    - 클라이언트, 애플리케이션 마스터, 태스크 JVM은 매번 잡 환경 설정을 읽으며 전혀 사용되지 않는 요소를 포함한 모든 요소를 메모리에 로드함
  - 분산 캐시
    - 실행 시점에 파일과 아카이브의 사본을 태스크 노드에 복사하여 이를 이용하도록 해주는 서비스, 네트워크 대역폭을 줄이기 위해 파일은 잡 단위로 특정 노드에 복사됨

# 하둡 운영
## 하둡 클러스터 설정 
- 하둡 클러스터를 구축하는 방법은 매우 다양함, 자체적으로 구축하는 방법과 하드웨어를 임대하거나 클라우드 호스팅 하는 방식으로 제공되는 하둡 서비스를 이용하는 방법
- 자체적으로 구축하는 방법의 몇 가지 설치 옵션
  - 아파치 타르볼
    - 아파치 하둡 프로젝트 및 관련 프로젝트는 각 릴리즈별로 바이너리 타르볼과 소스를 제공함, 사용자에게 높은 유연성을 제공하지만 사용자가 설치 파일, 설정 파일, 로그 파일의 위치를 직업 결정해야 하고 정확한 권한 설정을 해야 하는 등 상당한 노력이 필요함
  - 패키지
    - 아파치 빅톱 프로젝트와 하둡 벤더(클라우데라,호튼웍스 등)는 RPM과 데비안 패키지를 제공함, 일관된 파일시스템 레이아웃을 제공하며 스택 방식(함께 잘 작동하는 하둡과 라이브 버전을 알 수 있음)의 호환성 테스트를 거쳤음, 퍼펫(Puppet)과 같은 설정 관리 도구를 함께 사용할 수 있음
  - 하둡 클러스터 관리 도구
    - 클라우데라 매니저와 아파치 암바리는 전체 수명 주기에 걸쳐 하둡 클러스터의 설치 및 관리 기능을 제공하는 전용 도구
    - 간단한 웹 사용자 인터페이스를 제공하며, 하둡 클러스터를 구축하려는 사용자와 운영자에게 권장되는 방식
- 클러스터 명세
  - 클러스터 규모 결정 
    - 마스터 노드 시나리오
      - HDFS와 YARN은 마스터 데몬을 활성-대기 쌍으로 실행할 수 있는 설정(고가용성)을 지원함
      - 활성 마스터에 장애가 발생하면 별도의 하드웨어에서 구동되는 대기 마스터가 활성 마스터의 역할을 대신 수행하므로 무중단 서비스가 가능함, HDFS에 고가용성 설정을 하면 대기 네임노드가 기존에 보조 네임노드가 맡았던 체크포인트 작업을 대신 수행함 
- 시스템 로그파일
  - 컴퓨터에서 실행 중인 각 하둡 데몬은 두 개의 로그 파일을 생성함
    - log4j를 통해 출력되는 로그파일
      - 파일의 확장자는 .log, 애플리케이션의 로그 메시지는 대부분 이곳에 기록되므로 문제가 발생하면 제일 먼저 살펴볼 필요가 있음
      - 표준 하둡 log4j 설정은 로그파일의 순환을 위해 일일 순환 파일 추가자(appender)를 사용함
      - 하둡은 오래된 로그파일을 자동으로 삭제하지 않기 때문에 로컬 노드에 저장 공간이 모자라지 않도록 로그파일을 주기적으로 삭제하거나 보관하는 계획을 세워야 함
    - 표준 출력과 표준 에러 로그가 함께 기록되는 로그 파일
      - .out이며, 하둡은 log4j를 이용하여 로그를 저장하기 때문에 보통 비어 있거나 적은 양의 로그만 기록됨, 데몬이 재시작될 때만 순환되고 최근 다섯 개의 로그만 보관함 
      - 두 종류의 로그파일의 이름은 데몬을 수행하는 사용자 이름, 데몬 이름, 머신의 호스트명의 조합
- 중요한 하둡 데몬 속성
  - core-site.xml
  - hdfs-site.yml
  - yarn-site.xml
- 보안
  - 커버로스와 하둡
    - 커버로스를 사용할 때 클라이언트가 이 서비스를 이용하려면 각 단계에서 서버와의 메시지 교환을 수반하는 다음 세 단계를 거쳐야 함
      - 인증 - 클라이언트는 인증 서버에 자신을 인증함, 시간 정보가 포함된 티켓-승인 티켓(Ticket-Granting Ticket -TGT)을 수신함
      - 권한 부여 - 클라이언트는 TGT를 이용하여 티켓 승인 서버에 서비스 티켓을 요청함
      - 서비스 요청 - 클라이언트는 서비스 티켓을 이용하여 클라이언트가 사용할 서비스를 제공하는 서버에 자신을 인증함, 하둡의 경우 이 서버는 네임노드나 리소스 매니저가 될 것 
    - 인증 서버와 함께 티켓 승인 서버는 키 분배 센터(Key Distriubution Center -KDC)를 구성함 

## 하둡 관리 
- 데이터노드 블록 스캐너
  - 모든 데이터노드는 블록 스캐너(block scanner)를 실행하여 데이터노드에 저장된 모든 블록을 주기적으로 점검함, 클라이언트가 블록을 읽기 전에 문제가 있는 블록을 탐지하고 수리할 수 있음 
  - 블록 스캐너는 점검할 블록의 목록을 관리하며 체크섬 오류를 찾기 위해 모든 블록을 확인함, 스캐너는 데이터노드의 디스크 대역폭을 유지하기 위해 조절 메커니즘(throttling mechansim)을 사용함 
- 밸런서
  - 시간이 지남에 따라 데이터노드 사이의 블록의 분포는 불균형 상태가 될 수 있음. 불균형 상태의 클러스터는 맵리듀의 지역성(locality)에 영향을 받게 되므로 자주 사용되는 데이터노드에 큰 부하를 주게 됨, 불균형 상태가 되지 않도록 해야 함 
  - 밸런서 프로그램은 블록을 재분배하기 위해 사용률이 높은 데이터노드의 블록을 사용률이 낮은 데이터노드로 옮기는 하둡 데몬 
- 모니터링
  - 모니터링의 목적은 클러스터가 기대하는 수준의 서비스를 제공하지 못하는 시점을 감지하는 것
  - 주 네임노드, 보조 네임노드, 리소스 매니저와 같은 마스터 데몬이 가장 중요한 모니터링 대상임, 대형 클러스터에서는 특히 데이터노드와 노드 매니저에 장애가 발생할 수 있기 때문에 일부 노드의 장애에 항상 대처할 수 있도록 추가 가용량을 제공해야 함 
  - 로깅
    - 모든 하둡 데몬은 로그파일을 생성하기 때문에 시스템에 어떤 일이 일어났는지 파악하는 데 매우 유용함
    - 로그 수준 설정
      - 문제를 파악할 때 시스템의 특정 컴포넌트의 로그 수준을 임시로 변경할 수 있으며 매우 편리함 
      - 하둡 데몬은 log4j 로그파일(각 데몬의 웹 UI의 /logLevel에서 찾을 수 있음)의 로그 수준을 변경할 수 있는 웹 페이지를 제공함 
  - 메트릭과 JMX
    - 하둡 데몬은 메트릭으로 알려진 이벤트와 측정치에 대한 정보를 수집함, 데이터노드는 기록된 바이트 수, 복제된 블록 수, 클라이언트의 읽기 요청 수(로컬과 원격 포함) 등의 메트릭을 수집함 
    - 메트릭과 카운터의 차이점
      - 수집하는 정보의 범위, 메트릭은 하둡 데몬이 수집하지만 카운터는 맵리듀스 태스크가 수집하고 전체 잡을 위해 집계됨, 서비스의 대상도 다른데, 메트릭은 관리자, 카운터는 맵리듀스 사용자를 위해 정보를 수집함
      - 카운터는 맵리듀스의 기능, 맵리듀스 시스템은 태스크 JVM에서 생성된 카운터의 값을 애플리케이션 마스터로 전달하고 최종적으로 맵리듀스잡을 실행한 클라이언트로 전달되는 것을 보장함 
- 유지 보수
  - 메타데이터 백업
    - 네임노드의 영속적인 메타데이터가 손실되거나 훼손되면 전체 파일시스템을 사용할 수 없게 됨 
  - 데이터 백업
    - HDFS는 관리자와 사용자가 파일시스템의 스냅숏을 생성하는 것을 지원함, 스냅숏은 특정 시점의 파일시스템 서브트리의 읽기 전용 사본, 스냅숀은 데이터를 복사하지 않기 때문에 매우 효율적으로 동작함
    - 스냅숏은 파일의 메타데이터와 블록의 목록만 저장함, 스냅숏을 이용하면 스냅숏을 생성한 특정 시점의 파일시스템을 완전히 되돌리 수 있음 
    - 스냅숏은 데이터 백업을 대체하는 것이 아니라 사용자의 실수로 삭제된 파일을 특정 시점으로 복구하는 데 유용한 도구, 주기적으로 스냅숏을 찍고 시기에 따라 일정 기간 동안 보관하도록 정책을 수립할 수 있음
- 노드의 추가와 퇴역
  - 새로운 노드 추가하기
    - hdfs-site.xml 파일에 네임노드를, yarn-site.xml 파일에 리소스 매니저를 지정하여 환경을 구성하고 데이터노드와 리소스 매니저 데몬을 시작하여 새로운 노드를 간단히 추가할 수 있음, 하지만 추가를 허용하는 노드 목록을 따로 만들어두는 것이 제일 좋은 방식

# 관련 프로젝트
## 에이브로 
- 아파치 에이브로는 특정 언어에 종속되지 않는 언어 중립적 데이터 직렬화 시스템, 하둡 Writable (직렬화 방식)의 주요 단점인 언어 이식성(language portability)을 해결하기 위해 만든 프로젝트 
- 아파치 쓰리프트나 구글의 프로토콜 버퍼와 같은 다른 직렬화 시스템과 차별화된 특성을 가지고 있음, 에이브로의 데이터는 다른 시스템과 비슷하게 언어 독립 스키마로 기술됨, 에이브로에서 코드를 생성하는 것은 선택사항
- 에이브로의 스키마는 보통 JSON으로 작성하며, 데이터는 바이너리 포맷으로 인코딩함 
- 스키마 해석(schema resolution)기능이 있음, 스키마 변형(schema evolution) 메커니즘 - 신중하게 정의된 어떠한 제약조건에서도 데이터를 읽는 데 사용되는 스키마와 데이터를 쓰는 데 사용되는 스키마가 같지 않아도 됨
- 하둡의 시퀀스 파일과 유사한 연속적 객체를 위한 객체 컨테이너 포맷(object container foramt)을 제공함 
- 에이브로 데이터 파일은 스키마가 저장된 메타데이터 섹션을 포함하고 있어 자신을 설명하는 파일이 됨, 파일은 압축과 분할 기능을 제공함 
- 에이브로 자료형과 스키마
  - 모든 프로그래밍 언어는 런타임 직전에 스키마를 결정할 수 없을 때 동적 매핑을 사용함, 자바에서는 이것을 제너릭 매핑(Generic mapping)이라고 함 
  - 자바에서는 코드 생성을 구체적 매핑(Specific mapping)이라고 하며, 데이터를 읽거나 쓰기 전에 스키마 사본이 있을 때 유용한 최적화 방식
  - 자바는 리플렉션을 이용하여 에이브로 자료형을 기존의 자바 자료형으로 매핑하는 리플렉트 매핑(Reflect mapping)을 지원함, 제너릭이나 구체적 매핑에 비해 느리지만 에이브로가 자동으로 스키마를 유추하기 때문에 자료형을 쉽게 정의할 수 있는 장점이 있음 
- 에이브로 맵리듀스 
  - 에이브로 맵리듀스 API는 일반 하둡 맵리듀스 API와 두가지 차이점이 있음
    - 에이브로 자바 자료형에 맞는 래퍼를 사용한다는 것 
    - 잡을 설정하는 데 AvroJob 클래스를 사용한다는 것, AvroJob 클래스는 입력, 맵 출력, 최종 출력 데이터에 대한 에이브로 스키마를 정의하는 데 편리함 

## 파케이
- 중첩된 데이터를 효율적으로 저장할 수 있는 컬럼 기준 저장 포맷
- 컬럼 기준 포맷은 파일 크기와 쿼리 성능 측면 모두에서 효율성이 높은 장점이 있음, 동일한 컬럼의 값을 나란히 모아서 저장하기 때문에 인코딩 효율이 높음, 컬럼 기준 포맷의 파일 크기는 행 기반 포맷에 비해 일반적으로 작음 
- 진정한 컬럼 기반 방식으로 중첩 구조의 데이터를 저장할 수 있다는 것 , 파케이 포맷을 지원하는 수많은 도구가 있다는 것 
- 데이터 처리 컴포넌트(맵리듀스, 피그, 하이브, 캐스케이딩, 크런치, 스파크)는 대부분 파케이 포맷을 지원함 , 유연성은 인메모리 표현까지 확장됨 
- 파케이 파일 포맷
  - 파케이 파일은 헤더, 하나 이상의 블록, 꼬리말 순으로 구성됨 
  - 파케이 파일의 각 블록은 행 그룹을 저장함, 행 그룹은 행에 대한 컬럼 데이터를 포함한 컬럼 청크로 되어 있음, 각 컬럼 청크의 데이터는 페이지에 기록됨 
  - 델타 인코딩(값의 차이를 저장), 연속길이 인코딩(run-length - 동일한 값이 연속으로 나오면 그 값과 빈도를 저장), 사전 인코딩(dictionary - 값의 사전을 만들어 인코딩한 후 사전의 인덱스를 나타내는 정수로 그 값을 저장)을 비롯하여 압축률이 높은 다양한 인코딩을 지원함, 작은 몇개의 값을 한 바이트에 저장하여 공간을 절약하는 비트 패킹(bit packing)과 같은 기술도 적용할 수 있음 
- 파케이 설정
  - 블록 크기를 설정할 때 스캔 효율성과 메모리 사용률 사이의 트레이드오프 관계를 고려해야 함, 블록의 크기를 크게 하면 더 많은 행을 가지므로 순차 I/O의 성능을 높일 수 있어 효율적으로 스캔할 수 있음(각 컬럼 청크를 설정하는 부담을 줄임), 하지만 개별 블록의 읽고 쓸 때 모든 데이터가 메모리에 저장되어야 하기 때문에 너무 큰 블록을 사용하는 것은 한계가 있음 
- 파케이 파일 쓰기와 읽기
  - 다양한 도구와 컴포넌트로 파케이 파일 포맷을 쉽게 통합할 수 있는 장착형(pluggable) 인메모리 데이터 모델이 있음, 자바는 ReadSupport와 WriteSupport로 파케이 파일 포맷을 통합함 

## 플룸
- 플룸은 이벤트 기반의 대용량 데이터를 하둡으로 수집하기 위해 개발됨, 다수의 웹 서버에서 로그파일을 수집하고 해당 파일의 로그 이벤트를 처리하기 위해 HDFS에 위치한 새로운 통합 파일로 옮기는 것은 플룸을 사용하는 전형적인 예
- 플룸을 사용하려면 플룸 에이전트를 실행해야 함, 플룸 에이전트는 채널로 연결된 소스와 싱크를 실행하는 장기 실행 자바 프로세스, 플룸에서는 소스는 이벤트를 만들고 이를 채널로 전달함, 채널은 싱크로 전송할 때까지 이벤트를 저장함, 소스-채널-싱크의 조합이 플룸의 기본 구성요소(building block)
- 분산형 토폴로지에서 실행되는 연결된 에이전트의 집합으로 구성됨, 시스템의 가장자리에 있는 에이전트(웹 서버에서 실행되는)는 데이터를 수집한 다음 이를 집계하는 에이전트로 전송하고 마지막으로 최종 목적지에 데이터를 저장함 
- 트랜잭션과 신뢰성
  - 플룸은 소스에서 채널까지와 채널에서 싱크까지의 전송을 보장하기 위해 분리된 트랜잭션을 사용함 
- HDFS 싱크
  - 플룸의 핵심은 대량의 데이터를 하둡 데이터 저장소에 전달하는 것 
  - 파티셔닝과 인터셉터
    - 파티션을 사용하면 데이터의 일부만 질의할 때 특정 파티션에 국한된 데이터 처리가 가능함
    - 플룸 이벤트 데이터는 주로 시간을 기준으로 파티셔닝됨, 프로세스를 주기적으로 실행하여 완료된 파티션으로 변환할 수 있음(예를 들면 중복 이벤트 제거하기 위해)
    - 인터셉터(interceptor)는 전송 중인 이벤트의 내용을 수정하거나 삭제할 수 있는 컴포넌트, 인터셉터는 소스에 달려 있으며, 이벤트가 채널에 도착하기 전에 실행됨 
  - 파일 포맷
    - 데이터를 저장할 때는 바이너리 포맷을 사용하는 것이 좋음, 텍스트 파일을 사용할 때보다 최종 파일의 크기가 더 작아지기 때문 
- 분기(fan out)
  - 하나의 소스에서 발생하는 이벤트를 여러 개의 채널로 전송하는 것을 뜻하는 용어
- 싱크 그룹
  - 여러 개의 싱크를 마치 하나의 싱크처럼 처리하므로 장애 복구나 부하 분산에 활용할 수 있음, 두 번째 계층의 에이전트 중 하나가 작동하지 않아도 중단 없이 이벤트를 다른 두 번째 계층의 에이전트에 전송할 수 있고 이어서 HDFS에 저장할 수 있음 

## 스쿱
- HDFS는 수많은 소스의 로그와 데이터를 안정적으로 저장할 수 있음, 맵리듀스 프로그램은 다양한 비정형 데이터의 포맷을 파싱하여 적절한 정보를 추출하고 여러 데이터셋을 결합하여 우리가 원하는 결과를 얻을 수 있음 
- 회사의 주요 데이터는 관계형 데이터베이스 관리 시스템(relational database management system - RDBMS)과 같은 구조적인 데이터 저장소에 주로 저장됨
- 아파치 스쿱은 구조화된 데이터 저장소에서 데이터를 추출해서 하둡으로 보내 처리할 수 있도록 해주는 오픈 소스 도구 
- 스쿱2
  - 스쿱 1은 명령행 도구고 자바 API를 제공하지 않았기 때문에 다른 프로그램에 내장하기 어려웠음, 스쿱 1의 모든 커넥터는 모든 출력 포맷에 대한 정의를 포함해야 하므로 새로운 커넥터를 작성하는 것은 매우 어려웠음
  - 스쿱 2는 잡을 실행하는 서버 컴포넌트뿐만 아니라 명령행 인터페이스(command-line-interface - CLI), 웹 UI, REST API, 자바 API 등 다양한 클랑이언트를 제공함 
- 스쿱 커넥터
  - 스쿱은 대용량 데이터 전송 기능이 있는 외부 저장 시스템에 데이터를 임포트하고 익스포트하는 확장 프레임워크
  - 스쿱 커넥터는 이 프레임워크를 사용하여 스쿱이 임포트와 익스포트 하게 해주는 모듈식 컴포넌트 

## 피그 
- 아파치 피그를 이용하면 대용량 데이터셋을 더 높은 추상 수준으로 처리할 수 있음, 피그를 사용하면 다중값이나 중첩된 형태의 데이터 구조를 처리할 수 있고 데이터 변환도 쉽게 할 수 있음 
  - 데이터 흐름을 표현하기 위해 사용하는 피그 라틴 언어
  - 피그 라틴 프로그램을 수행하는 실행 환경. 단일 JVM에서의 로컬 실행 환경과 하둡 클러스터 상의 분산 실행 환경을 지원함 
- 피그 라틴 프로그램은 입력 데이터를 처리하여 출력 결과를 생성하는 일련의 연산(operation) 및 변환(transformation)으로 구성, 전체적으로 보면 각각의 연산은 데이터의 연속적인 흐름을 표현하며, 피그의 실행 환경은 이를 실행 가능한 표현으로 변환한 후 실제 수행함, 피그는 내부적으로 이러한 과정을 일련의 맵리듀스 잡으로 변환함 
- 매퍼와 리듀서를 작성해서 코드를 컴파일하고 패키징한 후 잡을 실행하고 그 결과를 확인하려면 오랜 시간이 걸림, 컴파일과 패키징이 필요 없는 하둡 스트리밍 방식을 사용하더라도 마찬가지, 피그를 사용하면 콘솔 화면에서 대여섯 줄 정도의 피그 라틴 코드만 작성하면 수 테라바이트의 데이터를 곧바로 처리할 수 있음 
- 처리 과정의 모든 부분(로딩, 저장, 필터링, 그룹핑, 조인)은 사용자 정의 함수(user-defined function - UDF)로 변경할 수 있음, 피그의 중첩 데이터 모델 위에서 동작하므로 피그의 연산자와 매우 긴밀히 통합 
- 피그의 프로그램 실행
  - 스크립트
    - 피그 명령어가 포함된 스크립트 파일을 실행함 
  - 그런트
    - 피그 명령어를 실행하는 대화형 쉘, 피그가 실행할 스크립트 파일을 지정하지 않았거나 -e 옵션을 사용하지 않았을 때 시작됨 
  - 내장형
    - 자바에서 JDBC로 SQL 프로그램을 실행하는 것처럼 PigServer 클래스를 사용하면 자바에서 피그 프로그램을 실행할 수 있음 
- 그런트
  - bash 쉘이나 다른 명령행 애플리케이션에서 주로 쓰는 GNU의 Readline과 같은 행 편집 기능이 있음 
- 함수
  - 평가 함수
    - 하나 이상의 표현식을 받아 다른 표현식을 반환하는 함수
  - 필터 함수
    - 평가 함수의 특별한 종류로 논리적인 불린값을 반환함
  - 로드 함수
    - 외부 저장소의 데이터를 관계자에 로드하는 방법을 지정하는 함수
  - 저장 함수
    - 관계자의 내용을 외부 저장소에 저장하는 방법을 지정하는 함수 
- 데이터 그룹과 조인
  - 단편 복제(fragment replicate join) - 작은 입력 데이터를 모든 매퍼에 분산한 후 단편화된 대규모 관계자에 대해 인메모리 룩업 테이블을 이용하여 맵-사이드 조인을 수행함

# 하이브 
- 하둡 기반의 데이터 웨어하우징 프레임워크로, 빠른 속도로 성장하는 페이스북의 소셜 네트워크에서 매일같이 생산되는 대량의 데이터를 관리하고 학습하기 위해 개발됨 
- 하이브는 자바 프로그래밍 기술은 부족하지만 강력한 SQL 기술을 가진 분석가가 페이스북의 HDFS에 저장된 대량의 데이터를 분석할 수 있도록 개발됨 
- SQL은 비즈니스 인텔리전스 분야의 도구에서 사용되는 공통 언어(lingua franca)이기 때문에(예를 들어 ODBC는 공통 인터페이스) 해당 분야의 상용 제품과 쉽게 통합할 수 있음 
- 하이브 설치하기
  - 하이브는 HDFS에 저장된 데이터(디렉터리/파일)에 구조(스키마)를 입히는 방식으로 데이터를 테이블로 구조화시킴, 테이블 스키마와 같은 메타데이터는 메타스토어라 불리는 데이터베이스에 저장됨 
- 하이브 쉘
  - HiveQL 명령어로 하이브와 상호작용하는 하이브의 기본 도구 
- 하이브 설정하기
  - 하이브는 하둡과 같이 xml 설정 파일을 사용하여 환경 설정을 함, 관련 파일은 hive-site.xml이고 하이브의 conf 디렉터리에 위치함 
  - 같은 디렉터리에 하이브가 보유하고 있는 속성과 기본값을 기록한 hive-default.xml 파일도 있음 
- 실행 엔진
  - 테즈와 스파크는 맵리듀스보다 더 높은 성능과 유연성을 제공하는 범용 방향성비순환 그래프(DAG-directed acyclic graph) 엔진, 잡의 임시 출력을 HDFS에 저장하는 맵리듀스와 달리 테즈와 스파크는 임시 출력을 로컬 디스크에 기록하거나 하이브 플래너의 요청으로 메모리에 저장하는 방식으로 복제 오버헤드를 피할수 있음 
- 로깅
  - 하이브의 에러 로그는 로컬 파일시스템의 ${java.io.tmpdir}/${user.name}/hive.log에서 찾을 수 있음, 환경 설정 문제나 다른 유형의 에러를 진단할 때 매우 유용함 
  - 로깅 설정 파일은 conf/hive-log4j.properties고, 로그 수준과 다른 로깅 관련 설정을 변경하고 싶으면 이 파일을 변경하면 됨 
- 메타스토어
  - 하이브 메타데이터의 핵심 저장소, 메타스토어는 서비스와 데이터 보관 저장소로 나뉨 
    - 내장형 메타 스토어(embedded metastore) 설정 - 메타데이터 서비스는 하이브 서비스와 동일한 JVM에서 실행되고 로컬 디스크에 저장되는 내장형 더비(Derby)데이터베이스 인스턴스를 포함함 , 내장형 더비 데이터베이스 인스턴스는 한번에 디스크에 위치한 데이터베이스 파일 하나에만 접근할 수 있음 
    - 로컬 메타스토어(local metastore) - 다중 세션, 즉 다중 사용자를 지원하는 방법은 독립형 데이터베이스를 사용하는 것 
    - 원격 메타스토어(remote metastore) - 하나 이상의 메타스토어 서버가 하이브 서비스와 별도의 프로세스로 실행됨, 데이터베이스 계층이 방화벽의 역할을 대신하고, 따라서 클라이언트는 데이터베이스 자격 증명을 더 이상 얻을 필요가 없기 때문에 관리성과 보안성이 더 높아짐 
- 읽기 스키마와 쓰기 스키마의 비교 
  - 쓰기 스키마(schema on write) - 전통적인 데이터베이스에서 테이블의 스키마는 데이터를 로드하는 시점에 검증됨, 로드 중인 데이터가 스키마에 부합되지 않으면 해당 데이터를 거부함, 데이버테이스 쓰는 시점에 떼이터의 스키마를 검증하기 때문
  - 읽기 스키마(schema on read) - 하이브는 로드 시점이 아니라 쿼리를 실행할 때 그 데이터를 검증함
  - 두 방식은 서로 상충 관계(trade-off) ,읽기 스키마는 데이터베이스 내부 형식으로 데이터를 읽거나 파싱하거나 디스크에 직렬화할 필요가 없기 때문에 초기에 매우 빠른 속도로 데이터를 로드할 수 있음, 로드 조작을 위해서는 단순히 파일을 복사하거나 이동하기만 하면 됨 
  - 쓰기 스키마는 데이터베이스가 컬럼 단위의 데이터 색인과 압축을 제공하기 때문에 더 빠르게 쿼리를 수행할 수 있음, 상대적으로 데이터베이스에 데이터를 로드하는 시간은 더 오래 걸림, 더욱이 쿼리가 정해지지 않아서 로드 시점에 스키마를 지정할 수 없고 색인도 적용할 수 없는 경우도 빈번함 
- 갱신, 트랜잭션, 색인
  - 실제 테이블의 갱신은 아예 새로운 테이블을 만들어 데이터를 변환하는 방식으로 구현된다는 점에 주목, 대량의 데이터셋을 대상으로 실행되는 데이터웨어하우징 애플리케이션에서 작동하는 방식 
  - HDFS 기존 파일의 갱신을 지원하지 않기 때문에 삽입, 변경, 삭제로 인한 갱신 내역은 별도의 작은 델타 파일에 저장됨, 델타 파일은 메타스토어에서 백그라운드로 실행되는 맵리듀스 잡에 의해 기존 테이블과 주기적으로 병합됨 
  - 테이블과 파티션 수준의 잠금을 지원함, 잠금은 특정 프로세스가 테이블을 읽는 도중에 다른 프로세스가 테이블을 삭제하는 것을 방지할 수 있음, 잠금은 주키퍼에 의해 투명하게 관리되므로 사용자가 직접 주키퍼를 조작하여 잠금을 적용하거나 해제할 수는 없음 
  - 하이브는 특정한 경우에 쿼리의 속도를 높일 수 있는 색인을 지원함, 콤패트(compact) 색인과 비트맵(bitmap) 색인을 지원함, 색인은 플러그인 방식으로 구현되었기 때문에 다른 방식의 색인도 추가할 수 있음 
  - 콤팩트 색인은 각 값을 파일 오프셋이 아닌 HDFS 블록 넘버로 저장함, 디스크 공간을 많이 차지 않으면서도 인접한 행 사이에 분포된 특정 컬럼에 대한 값을 색인하는 데 매우 효율적, 비트맵 색인은 특정 값이 출현하는 행을 효율적으로 저장하기 위해 압축된 비트셋(bitset)을 사용함 
- SQL-on-Hadoop 대안
  - 하이브의 대안으로 개발된 유명한 오픈 소스로는 페이스북의 프레스토, 아파치 드릴, 스파크 SQL이 있음, 프레스토와 드릴은 임팔라와 비슷한 아키텍처를 가지고 있지만, 드릴은 HiveQL 대신 SQL:2011을 지원함, 스파크 SQL은 스파크를 기반 엔진으로 이용하고 ,스파크 프로그램에 내장 SQL 쿼리를 허용함
    - 스파크 기반의 하이브는 하이브 프로젝트의 일부이므로 모든 하이브의 기능을 제공함, 스파크SQL은 일정 수준에서 하이브와의 호환성을 제공하는 새로운 SQL 엔진
  - 아파치 피닉스는 HBase 기반의 SQL을 제공함, JDBC 드라이버를 통해 SQL을 요청하면 HBase의 scan() 메서드로 변환되어 해당 쿼리가 처리되므로 서버 측 집계를 수행하는 HBase의 보조프로세서의 장점을 취할 수 있음, 메타데이터 역시 HBase에 저장되는 것을 활용함 
- HiveQL
  - 하이브의 SQL언어인 HiveQL은 SQL-92, MySQL, 오라클 SQL을 혼합한 것 
- 파티션과 버킷
  - 테이블을 파티션으로 구조화할 수 있음, 파티션이란 테이블의 데이터를 날짜와 같은 파티션 컬럼의 값을 기반으로 큰 단위(coarse-grained)로 분할하는 방식, 파티션을 사용하면 데이터의 일부를 매우 빠르게 질의할 수 있음
  - 테이블과 파티션은 효율적인 쿼리를 위해 데이터에 추가된 구조인 버킷으로 더욱 세분화 될 수 있음, 사용자 ID를 기준으로 버킷을 생성하면 전체 사용자 중에서 무작위 데이터 샘플을 뽑아 사용자가 작성한 쿼리가 제대로 실행되는지 빠르게 평가할 수 있음 
  - 파티션
    - 테이블은 다중 차원으로 파티션될수 있음, 예를 들어 먼저 날짜를 기준으로 로그를 파티션하고 그다음에 지역별로 효율적인 쿼리를 수행하기 위해 각 날짜별 파티션에 국가별 서브파티션을 추가할 수 있음 
  - 버킷
    - 테이블을 버킷으로 구조화하는 이유는 두 가지
      - 매우 효율적인 쿼리가 가능하기 때문, 버킷팅은 테이블에 대한 추가 구조를 부여하고, 하이브는 어떤 쿼리를 수행할 때 이 추가 구조를 이용할 수 있음, 특히 동일한 컬럼(조인할 컬럼) 에 대한 버킷을 가진 두 테이블을 조인할 때 맵 조인을 구현하면 매우 효율적임
      - 효율적인 샘플링에 유리, 매우 큰 데이터셋을 대상으로 개발하거나 개선하는 과정에서 데이터셋의 일부만을 쿼리를 수행할 수 있으면 매우 편리함 
- 저장 포맷
  - 하이브는 두 개의 차원, 즉 로우 포맷과 파일 포맷으로 테이블 저장소를 관리함
  - 로우 포맷은 행과 특정 행의 필드가 저장된 방식을 지시함, 직렬자-역직렬자(Serializer-Deserializer)를 혼합한 하이브 전문 용어인 SerDe로 정의됨
  - 테이블을 질의하는 경우와 같이 역질렬화를 수행할 때 SerDe는 파일에 저장된 바이트의 데이터행을 하이브에서 내부적으로 사용되는 객체로 역지렬화하여 그 데이터에 대한 연산을 수행함
- 뷰
  - SELECT 문으로 정의된 일종의 가상 테이블(virtual table), 뷰는 사용자에게 디스크에 실제 저장된 것과 다른 방시긍로 데이터를 보여주는 데 사용됨
- 사용자 정의 함수
  - 정의 UDF, 사용자 정의 집계 함수(user-defined aggregate function - UDAF), 사용자 정의 테이블 생성 함수(user-defined table-generating function - UDTF) 등 세 종류의 UDF를 지원함, 세 종류의 차이점은 입력으로 받는 행과 출력되는 행의 개수가 다르다는 것
    - 정규 UDF는 단일 행을 처리한 후 단일 행을 출력함, 수학 함수나 문자열 함수와 같은 대부분의 함수가 여기에 해당함
    - UDAF는 다수의 입력 행을 처리한 후 단일 행을 출력함, COUNT나 MAX같은 집계 함수가 여기에 해당함
    - UDTF는 단일 로우를 처리한 후 다수의 행(테이블)을 출력함 
