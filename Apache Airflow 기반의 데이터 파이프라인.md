# 서평
- 이 책은 Apache Airflow의 개념과 적용 방법에 대한 설명뿐만 아니라, 실제 서비스 운영 시 고려해야 할 모니터링, 확장, 보안 등에 관한 내용에 관한 내용을 상세하게 안내하고 있으며 , 다양한 클라우드 환경에서 활용하는 방법까지 다루고 있습니다. 또한 Airflow 설치부터 파이프라인 작성, 테스트, 분석, 백필 그리고 배포 실습까지 한 번에 해결 할 수 있어 많은 도움이 되었습니다. 

# 기본편
## Apache Airflow 살펴보기 
- Airflow는 파이썬으로 배치, 스케줄링, 모니터링 등을 한 번에 해결하는 워크플로 관리 플랫폼
- Airflow의 주요 기능은 유연한 파이썬 프레임워크를 사용해 쉽게 데이터 파이프라인을 구축할 수 있게 해 주며, 최신 기술 환경에서 접하게 되는 서로 다른 기술들을 연결할 수있는 다양ㅎ안 빌딩 블록을 제공함 
- DAG에 정의된 특정 시점에 트리거할 수 있을 뿐만 아니라(Cron과 유사한) 최종 시점과 예상되는 다음 스케줄 주기를 상세하게 알려주는 것
- 데이터 파이프라인 그래프
  - 태스크(task) 간의 의존성을 명확하게 확인하는 방법 중 하나는, 데이터 파이프라인을 그래프로 표현하는 것
  - 그래프는 방향성을 가지기 때문에 방향성 그래프(directed graph)라고 함
  - 방향성 비순환 그래프(Directed Acyclic Graph, DAG) : 그래프는 화살표 방향성의 끝점(directed edge)을 포함하되 반복이나 순환을 허용하지 않음(비순환 - acyclic)
  - Airflow(또는 기타 워크플로 관리자)에서는 DAG의 비순환 속성은 태스크 그래프를 효율적으로 해결하고 실행하기 위해 사용됨
  - 그래프 기반 표현은 전체 작업을 하나의 모놀로식(monolithic - 단일) 스크립트 또는 프로세스로 구성되는 것이 아니라 파이프라인을 작은 점진적인 태스크로 명확하게 분리
  - 그래프 기반 표현에서는 실패한 태스크(그리고 그 이후 태스크)만 재실행하면 되므로 효율적으로 구성할 수 있음
- 백필(Backfilling) 
  - 하나의 플로(Airflow에서는 DAG)를 특정 옵션(기간) 기준으로 다시 실행할 수 있는 기능, 태스크가 며칠 동안 실패하거나 새롭게 만든 플로를 과거의 특정 시점부터 순차적으로 실행하고 싶을 때 수행함
- 파이프라인 스케줄링 및 실행
  - Airflow 스케줄러 : DAG를 분석하고 현재 시점에서 DAG의 스케줄이 지난 경우 Airflow 워커에 DAG의 태스크를 예약함
  - Airflow 워커 : 예약된 태스크를 선택하고 실행함
  - Airflow 웹 서버 : 스케줄러에서 분석한 DAG를 시각화하고 DAG 실행과 결과를 확인할 수 있는 주요 인터페이스를 제공함
- Airlfow 웹 인터페이스
  - 개별 DAG의 태스크와 의존성에 대한 그래프 뷰(Graph View) 화면을 제공함
    - DAG의 태스크 내용과 태스크 간의 의존성을 보여줌
  - 트리 뷰(tree view) : DAG(가장 최근 실행과 실행 기록)의 다중 실행 결과를 확인할 수 있음
- Airflow를 선택하는 이유 
  - 파이썬 코드를 이용해 파이프라인을 구현할 수 있기 때문에 파이썬 언어에서 구현할 수 있는 대부분의 방법을 사용하여 복잡한 커스텀 파이프라인을 만들 수 있음
  - 파이썬 기반의 Airflow는 쉽게 확장 가능하고 다양한 시스템과 통합이 가능함
  - 수많은 스케줄링 기법은 파이프라인을 정기적으로 실행하고 점진적(증분, incremental) 처리를 통해, 전체 파이프라인을 재실행할 필요 없는 효율적인 파이프라인 구축이 가능함
  - 백필 기능을 사용하면 과거 데이터를 손쉽게 재처리할 수 있기 때문에 코드를 변경한 후 재생성이 필요한 데이터 재처리가 가능함
  - Airflow의 훌륭한 웹 인터페이스는 파이프라인 실행 결과를 모니터링할 수 있고 오류를 디버깅하기 위한 편리한 뷰를 제공함
  - 오픈 소스 기반의 특정 벤더에 종속되지 않고 Airflow를 사용할 수 있음
- Airflow가 적합하지 않은 경우
  - 반복적이거나 배치 태스크(batch-oriented task)를 실행하는 기능에 초점이 맞춰져있기 때문에, 스트리밍(실시간데이터 처리) 워크플로 및 해당 파이프라인 처리에 적합하지 않을 수 있음
  - 추가 및 삭제 태스크가 빈번한 동적 파이프라인의 경우에는 적합하지 않을 수 있음 
  - 파이썬 언어로 DAG를 구현하기 때문에 파이썬 프로그래밍 경험이 전혀(또는 거의0 없는 팀은 적합하지 않을 수 있음
  - 파이썬 코드로 DAG를 작성하는 것은 파이프라인 규모가 커지면 굉장히 복잡해질 수 있음
  - 워크플로 및 파이프라인 관리 플랫폼이며, 데이터 계보(lineage) 관리, 데이터 버전 관리와 같은 확장 기능은 제공하지 않기 때문에 필요한 경우 언급한 기능을 제공하는 특정 도구를 Airflow와 직접 통합해야함

## Airflow DAG의 구조
- 하나 이상의 단계로 구성된 대규모 작업을 개별 태스크로 분할하고 DAG(Directed Acyclic Graph)로 형성할 수 음
- 다중 태스크를 병렬로 실행할 수 있고 서로 다른 기술을 사용할 수 있음
- 오른쪽 시프트 연산자(binary right shift operator) : "rshift"(>>)를 사용하여 태스크 간의 의존성을 정의함
- 태스크와 오퍼레이터 차이점
  - 오퍼레이터(operator) : 단일 태스크를 나타냄 
    - 단일 작업 수행 역할
    - PythonOperator : 파이썬 함수를 실행하는 데 사용됨
    - EmailOperator : 이메일 발송에 사용됨
    - Simple HttpOperator : HTTP 엔드포인트 호출
  - DAG는 오퍼레이터 집합에 대한 실행을 오케스트레이션(orchestration - 조정,조율)하는 역할을 함, 오퍼레이터의 시작과 정지, 오퍼레이터가 완료되면 연속된 다음 태스크의 시작, 그리고 오퍼레이터 간의 의존성 보장이 포함됨
  - Airflow에서 태스크는 작업의 올바른 실행을 보장하기 위한 오퍼레이터의 래퍼(wrapper) 또는 매니저(manager)로 생각해 볼 수 있음
  - 사용자는 오퍼레이터를 활용해 수행할 작업에 집중할 수 있으며, Airflow는 태스크를 통해 작업을 올바르게 실행할 수 있음
  - DAG와 오퍼레이터는 Airflow 사용자가 이용함, 태스크는 오퍼레이터의 상태를 관리하고 사용자에게 상태 변경(예:시작/완료)을 표시하는 Airflow의 내장 컴포넌트
- Airflow는 스케줄러, 웹 서버, 데이터베이스의 세 가지 핵심 컴포넌트로 구성됨
- 도커 컨테이너 : 파이썬 패키지를 설정하고 라이브러리의 충돌을 방지할 수 있도록 재사용 격리 환경을 생성해 줄 수 있는 인기 있는 도구 
- Airflow는 모든 태스크의 로그를 수집하기 때문에 결과나 실패에 대한 문제를 확인할 수 있음
- 실패 : 외부 서비스 중단, 네트워크 연결 문제, 디스크 손상 등 종종 발생 

## Airflow의 스케줄링
- Cron 기반의 스케줄 간격 설정하기 
  - 더 복잡한 스케줄 간격 설정을 지원하기 위해서 cron(macOS 및 리눅스와 같은 유닉스 기반 OS에서 사용하는 시간 기반 작업 스케줄러)과 동일한 구문을 사용해 스케줄러 간격을 정의함 
  - *****(분,시간,일,월,요일 )
  - 0 * * * * : 매시간(정시에 실행)
  - 0 0 * * * : 매일(자정에 실행)
  - 0 0 * * 0 : 매주(일요일 자정에 실행)
  - 0 0 1 * * : 매월 1일 자정
  - 45 23 * * SAT : 매주 토요일 23시 45분
  - 0 0 * * MON-FRI = 매주 월요일부터 금요일 자정에 실행
  - 0 0,12 * * * : 매일 자정 및 오후 12시에 실행
- 빈도 기반의 스케줄 간격 설정하기
  - timedelta(표준 라이브러리인 datatime 모듈에 포함된) 인스터스를 사용하면 됨
- 실행 날짜를 사용하여 동적 시간 참조하기
  - 시간 기반 프로세스(time-based process) 워크플로의 경우, 주어진 작업이 실행되는 시간 간격을 아는 것이 중요함
  - execution_date : DAG가 실행되는 날짜와 시간을 나타냄
    - DAG를 시작하는 시간의 특정 날짜가 아니라 스케줄 간격으로 실행되는 시작 시간을 나타내는 타임스탬프
    - 스케줄 간격의 종료 시간은 next_execution_date라는 매개변수를 사용
    - 과거의 스케줄 간격의 시작을 정의하는 previous_execution_date 매개변수를 제공
- 파티셔닝(partitioning) : 데이터 세트를 더 작고 관리하기 쉬운 조각으로 나누는 작업
  - 데이터 세트의 작은 부분을 파티션(partitions)이라 함 
- 고정된 스케줄 간격으로 태스크 실행
  - 시작 날짜, 스케줄 간격 및 종료 날짜(선택 사항)의 세 가지 매개 변수를 사용하여 DAG를 실행하는 시점을 제어할 수 있음
- 백필(backfilling) : 과거 데이터 세트를 로드하거나 분석하기 위해 DAG의 과거 기록을 실행함
- 태스크 디자인
  - 원자성(atomicity)
    - 원자성 트랜잭션은 모두 발생하거나 전혀 발생하지 않는, 나눌 수 없고 돌이킬 수 없는 일련의 데이터베이스와 같은 작업으로 간주됨
    - Airflow의 태스크는 성공적으로 수행하여 적절한 결과를 생성하거나 시스템 상태에 영향을 미치지 않고 실패하도록 정의함 
    - 모든 것이 완료되거나 완료되지 않도록 보장함
  - 멱등성(idempotency)
    - 동일한 태스크를 여러 번 호출해도 결과에 효력이 없어야 함
    - 입력 변경 없이 태스크를 다시 실행해도 전체 결과가 변경되지 않아야 함
    - 멱등성이 보장되는 태스크는 실행 횟수에 관계없이 동일한 결과를 생성함, 일관성과 장애 처리를 보장함
## Airflow 콘텍스트를 사용하여 태스크 템플릿 작업하기
- 오퍼레이터의 인수 템플릿 작업
  - BashOperator : 실행할 배시(Bash) 명령을 제공하는 인수인 bash_command를 사용함
- Airflow는 날짜 시간에 Pendulum 라이브러리를 사용하며 execution_date는 이러한 Pendulum의 datetime 객체
  - 네이티브 파이썬의 datetime의 호환(drop-in replacement) 객체이므로 파이썬에 적용할 수 있는 모든 메서드를 Pendulum에도 적용할 수 있음 
- PythonOperator 템플릿
  - BashOperator를 사용하여 런타임에 자동으로 템플릿이 지정되는 bash_command 인수(또는 다른 오퍼레이터에서 이름이 지정된 인수)에 문자열을 제공함
  - PythonOperator는 런타임 콘텍스트로 템플릿화할 수 있는 인수를 사용치 않고 별도로 런타임 콘텍스트를 적용할 수 있는 python_callable 인수를 사용함
- 다른 시스템과 연결하기
  - 태스크 간 데이터를 전달하는 방법
    - Airflow 메타스토어를 사용하여 태스크 간 결과를 쓰고 읽음, XCom
    - 영구적인 위치(예 : 디스크 또는 데이터베이스)에 태스크 결과를 기록함
  - Airflow 태스크는 설정에 따라 물리적으로 서로 다른 컴퓨터에서 독립적으로 실행되므로 메모리에서 데이터를 공유할 수 없음, 태스크 간의 데이터는 태스크가 완료된 후 다른 태스크에서 읽을 수 있는 다른 위치에 유지되어야 함
  - XCom이라는 기본 메커니즘을 제공하여 Airflow 메타스토어에서 선택 가능한(picklable) 개체를 저장하고 나중에 읽을 수 있음
    - 피클(Pickle)은 파이썬의 직렬화 프로토콜이며 직렬화는 메모리의 개체를 나중에 다시 읽을 수 있도록 디스크에 저장할 수 있는 형식으로 변환하는 것을 의미함
    - 기본 파이썬 타입(예: string, int, dict, list)에서 빌드된 모든 객체를 피클링이 가능함
    - 피클링이 불가능한 개체 : 데이터베이스 연결, 파일 핸들러
  - PostgresOperator : Postgres와 통신하기 위해 훅(hook)이라고 불리는 것을 인스턴스화함, 인스턴스화된 훅은 연결 생성, Postgres에 쿼리를 전송하고 연결에 대한 종료 작업을 처리함, 오퍼레이터는 사용자의 요청을 훅으로 전달하는 작업만 담당함  

## 태스크 간 의존성 정의하기
- XCom : DAG 실행에서 서로 다른 작업 간에 데이터를 전달할 수 있음
- 다양한 태스크 의존성 패턴
  - 태스크의 선형 체인(linear chain) 유형 : 연속적으로 실행되는 작업
  - 팬아웃/팬인(fan-out/fan-in) 유형 : 하나의 태스크가 여러 다운스트림 태스크에 연결되거나 그 반대의 동작을 수행하는 유형 
    - 팬아웃 : 여러 개의 입력 태스크 연결수 제한
    - 팬아웃 종속성 : 한 태스크를 여러 다운스트림 태스크에 연결하는 것
    - 팬인 구조 : 하나의 태스크가 여러 업스트림 태스크에 영향을 받는 구조는 단일 다운스트림 태스크가 여러 업스트림 태스크에 의존성을 갖음
      - [a , b ] >> c
- 브랜치하기
  - Airflow는 다운스트림 태스크 세트 중 선택할 수 있는 기능을 BranchPythonOperator을 통해 제공
    - PythonOperator와 달리 전달된 호출 가능한 인수는 작업 결과로 다운슽릠 태스크의 ID를 반환함, 반환된 ID는 브랜치 태스크 완료 후 실행할 다운스트림 태스크를 결정함, 태스크 ID 리스트를 반환하는 경우도 있으며, 이 경우 Airflow는 참조된 모든 태스크를 실행함
- 트리거 규칙
  - 트리거 규칙 : 태스크의 의존성 기능(DAG안에서 선행 태스크 조건)과 같이 Airflow가 태스크가 실행 준비가 되어 있는지 여부를 결정하기 위한 필수적인 조건, Airflow의 기본 트리거 규칙은 all_success, 태스크를 실행하려면 모든 의존적인 태스크가 모두 성공적으로 완려되어야 함
- 태스크 간 데이터 공유
  - Airflow의 XCom을 사용하여 태스크 간에 작은 데이터를 공유할 수 있음
  - 태스크 간에 메시지를 교환하여 특정 상태를 공유함
  - XCom 사용 시 고려사항
    - 풀링 태스크는 필요한 값을 사용하기 위해 태스크 간에 묵시적인 의존성(implicit dependency)이 필요함
    - 명시적 의존성 태스크(explicit task)와 달리 DAG에 표시되지 않으며 태스크 스케줄 시에 고려되지 않음
    - XCom에 의해 의존성 있는 작업이 올바른 순서로 실행할 수 있도록 해야함
    - XCom은 오퍼레이터의 원자성을 무너뜨리는 패턴이 될 가능성이 있음 
    - XCom이 저장하는 모든 값을 직렬화(serialized)를 지원해야 한다는 기술적 한계가 존재함, 람다 또는 여러 다중 멀티프로세스 관련 클래스 같은 일부 파이썬 유형은 XCom에 저장할 수 없음 
- Taskflow API를 통해 파이썬 태스크 및 의존성을 정의하기 위한 새로운 데코레이터 기반(decorator-based) API를 추가적으로 제공함
