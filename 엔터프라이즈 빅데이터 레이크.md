# 서평
- 이 책은 데이터 레이크의 아키텍처와 장점, 데이터 레이크를 도입할 때의 어려움과 그런 어려움을 극복하는 방법에 대해 설명하고 있습니다.
- 데이터 레이크를 데이터 웅더이(분석적인 샌드박스)나 데이터 연못(큰 데이터 웨어하우스)을 바탕으로 확장할 때 활용할 수 있는 여러 접근법뿐만 아니라 아예 바닥부터 구축하는 방법까지 다룸. 사내, 클라우드 기반, 가상 등 다양한 데이터 레이크 아키텍처의 장단점을 살펴봄. 미가공, 처리되지 않은 데이터에서부터 잘 관리되고 요약된 데이터까지 유형별로 저장하는 개별 영역을 설정하는 방법과 그런 영역의 접근 권한을 관리하는 방법을 다룸. 사용자가 스스로 데이터를 찾고 이해하고 준비할 수 있도록 하는 셀프서비스를 가능하게 하는 방법, 사용자의 기술 수준에 따라 적합한 인터페이스를 제공하는 방법, 이 모든 과정을 기업의 데이터 관리 정책을 준수하면서 진행하는 방법 등을 설명하고 있습니다. 

# 데이터 레이크 소개 
- 데이터 레이크 성숙도
  - 데이터 웅덩이(data puddle)란 빅데이터 기술을 활용해서 구축한 단일 목적이나 단일 프로젝트용 데이터 마트. 데이터 웅덩이의 데이터는 단일 프로젝트나 팀의 목적을 위해 사용됨 
  - 데이터 연못(data pond) : 데이터 웅덩이 여러 개를 모아 놓은 것, 데이터 마트를 모아 놓기만한 모습. 기존에 있던 데이터 웨어하우스에서 사용하지 않는 부분을 모아 놓은 모습. 낮은 기술 비용과 높은 확장성이라는 명확한 장점은 있지만, 여전히 IT에 많이 의존함 
  - 데이터 레이크(data lake)는 데이터 연못과는 두 가지 주요 측면에서 차이가 있음. 셀프서비스를 지원하다는 점. 비즈니스 사용자가 IT 부서의 도움 없이 필요한 데이터 세트를 찾아 사용할 수 있음. 당장 어떤 데이터를 요구하는 프로젝트가 없더라도 차후에 비즈니스 사용자가 필요할 수 있는 데이터를 저장하는 것을 목표로 한다는 점 
  - 데이터 오션(data ocean)은 해당 데이터가 데이터 레이크에 저장됐는지 여부와 상관없이 데이터가 어디에 있든 기업의 모든 데이터가 셀프서비스와 데이터 주도 결정 과정에 활용될 수 있게 됨 
  - 데이터 연못과 데이터 레이크의 주요 차이는 무엇을 목표로 하는가. 데이터 연못은 원래 있던 데이터 웨어하우스나 데이터 마트를 상대적으로 비용이 저렴하고 확장성이 좋은 방법으로 대체함. 데이터 레이크는 일상적인 생산에 사용할 준비가 완료된 쿼리를 목표로 함. 데이터 레이크는 비즈니스 사용자가 다양한 데이터와 도구를 사용해서 즉흥적인 분석이나 실험을 통해 데이터를 활용해서 스스로 결정할 수 있게 해줌 
- 데이터 웅덩이
  - 데이터 웅덩이는 주로 소규모 팀이나 특수한 사용 사례를 목적으로 구축함
  - 단일팀에게 작업 공간을 제공해서 데이터 과학자가 실험할 수 있게 하려고 사용하는 경우도 많음. 이런 작업 공가능ㄴ 샌드박스(sandbox)라고 부름 
  - 데이터 운덩이는 목적이 한정적이고 저장하는 데이터 유형도 제한적. 크기도 작은 전용 데이터 스트림(data stream)으로 구성되며 구축하고 유지 보수하려면 기술 수준이 높은 팀이나 IT의 적극적인 지원이 필요함 
- 데이터 연못
  - 데이터 연못은 데이터 웅덩이 여러 개를 모아 놓은 것
  - 데이터 연못은 빅데이터 기술로 구축한 데이터 웨어하우스로 생각할 수 있음 
- 성공적인 데이터 레이크 구축
  - 올바른 플랫폼
    - 데이터 레이크를 도입하는 데 가장 많이 활용하는 기술로는 하둡과 같은 빅데이터 기술, 아마존 웹 서비스, 마이크로소프트 애저, 구글 클라우드 플랫폼과 같은 클라우드 솔루션이 있음 
    - 용량 
      - 스케일 아웃(scale out)하도록 설계됨. 성능의 특별한 저하 없이 스케일(즉 규모)을 무한정에 가깝게 늘릴 수 있음 
    - 비용
      - 테이프, WORM 디스크, 하드디스크 드라이브 등을 활용해서 많은 데이터를 비교적 저렴한 저장 장치에 보관할 능력은 예전부터 있었음 
    - 다양성
      - 다양한 파일 형태를 저장할 수 있게 해주는 파일 시스템(file system)이나 오브젝트 스토어(object store)를 사용함. 하둡 HDFS, MapR FS, AWS 등의 단순 저장 서비스(S3, Simple Storage Service) 등이 여기에 해당됨.
      - 데이터 구조가 사전에 정의돼야 하는 (저장 시점 스키마 적용) 관계형 데이터베이스와는 달리 파일 시스템이나 오브젝트 스토어는 데이터를 어떻게 쓰든 상관하지 않음. 읽는 시점 스키마 적용, 마찰 없는 주입이 간으하게 함 
      - 데이터를 스키마에 맞게 데이터베이스가 기대하는 형태로 전환하기 전까지는 데이터를 저장할 수 없는 관계형 데이터베이스와는 달리 데이터를 아무런 처리 없이 저장할 수 있음 
    - 미래 대비
      - 하둡과 기타 빅데이터 플랫폼은 모듈 형태로 활용하기 매우 용이함 
  - 올바른 데이터
  - 올바른 인터페이스 
- 데이터 늪(data swamp)
  - 데이터 늪은 데이터 레이크만큼 커진 데이터 연못이지만 분석가에게 매력적으로 다가가는 데 실패한 경우. 셀프서비스와 관리 시설의 부족 때문 
- 성공적인 데이터 레이크 로드맵
  - 인프라 구축(하둡 클러스터를 설치하고 실행)
  - 데이터 레이크 구조화(다양한 사용자 집단을 위한 영역 설정과 데이터 삽입)
  - 셀프서비스가 가능하도록 데이터 레이크 설정(데이터 자산 카탈로그를 만들고, 권한을 설정하고, 분석가가 쓸 수 있는 도구 제공)
  - 사용자에게 데이터 레이크 공개
- 데이터 레이크 구축
  - 논리 데이터 레이크란 복수의 다차원 시스템에 구성된 가상 데이터 레이크 레이어(data lake layer)
  - 근간이 되는 시스템은 하둡, 관계형, NoSQL 데이터베이스고, 사내 혹은 클라우드에 존재할 수 있음 
- 데이터 레이크 구조화
  - 다중 모델 IT란 간단히 말해 관리는 데이터 사용 방법과 사용자 집단의 요구 사항을 반영할 필요가 있다는 생각 
- 셀프 서비스를 위한 데이터 레이크 설정
  - 데이터 검색과 이해
    - 부족 지식(tribal knowledge) : 해당 지식이 있기는 하지만 지식이 부족 전체에 퍼져 있어 고토으럽고, 오래 걸리고, 잘못된 가능성이 높은 발견 과정을 거쳐 재결합돼야 함 
    - 크라우드소싱 분석(analyst crowdsourcing) : 분석가가 비즈니스 용어로 이뤄진 간단한 문구를 갖고 데이터 세트 관련 문서를 작성하고 필요한 것을 찾는 데 도움이 되는 검색 인덱스(search index)를 만들면서 부족이 가진 지식을 수집할 수 있게 해줌 
    - 워터라인 데이터(Waterline Data) : 분석가가 자신의 데이터 세트를 갖고 일할 때 만드는 정보를 크라우드소싱으로 받아 관련 문서가 없는 다른 데이터 세트에 적용할 수 있게 해줌. 이런 과정을 핑거프린팅(finger printing) 
  - 데이터 접근과 권한 설정 
    - 모든 사람에게 모든 데이터에 관한 권한을 주거나 분석가가 필요성을 증명할 수 있는 경우를 제외하고는 모든 접근 권한을 막는 방법 
    - 메타데이터(metadata) 카탈로그로 모든 데이터 세트에 관한 정보를 공개해서 분석가가 필요한 데이터 세트를 찾고 필요한 접근 권한을 요청하게 하는 것. 권한 요청에는 일반적으로 권한이 왜 필요한지, 데이터를 어떤 프로젝트에 쓸지, 권한이 필요한 기간은 언제인지 등을 명시함 
  - 데이터 준비
    - 트리팩타(Trifacta)와 같은 일부 도구는 변환 방법을 추천하고 분석가가 데이터를 준비하는 과정을 지원하고자 정교한 머신러닝 기술을 적용하고 있음 
- 데이터 레이크 아키텍처
  - 데이터 자주권 규정(ex- 독일에서는 데이터를 갖고 나갈수 없음)과 회사의 압박 등 여러 가지 상황상 복수의 데이터 레이크를 사용하는 것이 더 좋은 방법이라는 것이 받아들여지게 됐음 
  - 대규모 병령 클러스터를 지원하는 것이 얼마나 복잡한지, 경험이 있는 하둡이나 기타 빅데이터 플랫폼 관리자를 찾는 것이 얼마나 어려운지 느끼면서 회사들은 대부분의 하드웨어와 플랫폼 요소를 전문가가 관리해주는 클라우드 기반 데이터 레이크를 사용하기 시작함 
- 상용 클라우드 데이터 레이크
  - 빅데이터 전문성과 짧은 배포 시간뿐만 아니라 낮은 비용과 클라우드가 갖는 탄력성 등의 장점 때문에 클라우드는 매우 매력적
- 논리 데이터 레이크
  - 나중에 누군가가 필요할 경우를 대비해 데이터 레이크에 모든 데이터를 저장하는 대신 카탈로그나 데이터 시각화 소프트웨어를 통해 데이터를 분석가에게 제공함 
  - 완전성과 중복 문제 
    - 완전성 문제를 위해서는 모든 데이터 자산을 다루는 카탈로그를 만들어 분석가가 기업이 가진 어떤 데이터 세트라도 찾고 요청할 수 있게 함 
    - 중복 문제는 다른 곳에 저장되지 않은 데이터를 데이터 레이크에 저장함. 다른 시스템에 저장된 데이터는 필요할 때 데이터 레이크에 옮겨오고, 사용하는 동안 서로 싱크(sync)를 맞춤 
    - 각 데이터 세트는 모든 사용자를 위해 한 번만 옮겨옴 
- 시각화와 카탈로그 기반 논리 데이터 레이크
  - 시각화는 연방(federation)또는 기업 정보 통합(EII, Enterprise Information Integration)이라고 불리기도 함. 물리적 테이블의 위치와 구현 상황을 숨기는 가상 뷰(virtual view) 혹은 테이블을 만드는 것 
  - 기업 전체 카탈로그는 분석가가 기업의 모든 데이터를 찾고 접근할 수 있게 해줄 뿐만 아니라 접근,관리,감시용 단일 접근 통로의 역할도 할 수 있음. 
  - 전체 카탈로그가 없다면 데이터 자산 접근이 여러 군데에서 이뤄져 관리하고 추적하기가 어려움. 또한 전체 카탈로그를 쓰면 모든 접근 요청이 카탈로그를 통해 이뤄짐. 접근 요청은 필요에 따라 특정 기간 동안 허가되며 시스템의 감시를 받음 

# 역사적 관점
- 셀프서비스 데이터 욕구: 데이터 베이스의 탄생
  - 데이터는 주의해서 관리하고, 일관성을 유지해야 하며, 정기적으로 백업해야 함 
  - 데이터베이스 관리 시스템(DBMS, DataBase Management Systems)이라는 별도의 시스템을 통해 제공하게 됌 
  - 관계형 데이터베이스 관리 시스템(RDBMS, Relational DataBase Management Systems)은 사용자가 데이터베이스 데이터를 명확하게 설명할 수 있게 해줌. 사용자는 사람이 읽을 수 있는 형태의 테이블과 필드의 집합인 스키마를 만들게 됨 
  - 저장장치에 데이터를 쓰거나 읽는 것은 메모리에서 하는 것보다 훨씬 느리기 때문에 정규화(normalization)라는 스키마 설계 기법을 사용함. 정규화는 데이터를 가능한 한 작은 단위로 나눠서 데이터베이스 업데이트 때 최소한의 데이터만 저장해도 되도록함
  - 정규화는 정보를 중복해서 입력하는 것을 피하고자 테이블을 더 작은 테이블로 나누는 방법 
- 반드시 해야 하는 분석: 데이터 웨어하우스의 탄생
  - 큰 데이터 웨어하우스를 데이터 마트(data mart)단위로 분할하거나 쿼리 처리에 사용되는 하드웨어의 최적 활용이 가능하게 하는 기기의 발명, 열 기반, 인메모리 데이터베이스 등이 여기에 해당함 
    - ETL(추출, 변환, 로드) 도구와 ELT(추출, 로드, 변환) 도구
    - 데이터 품질(DQ,Data Quality)과 프로파일 도구 
    - 데이터 모델링 도구
    - 비즈니스 용어 사전
    - 메타데이터 저장소
    - 데이터 관리 도구
    - 마스터 데이터 관리(MDM, Master Data Management) 시스템
    - 기업 정보 통합(EII), 데이터 연방, 데이터 가상화 도구
  - 보고서와 분석 결과서 생성을 지원하는 도구
    - 보고용 도구 
    - 온라인 분석 처리(OLAP, OnLine Analytical Processing) 도구
    - 비즈니스 지능(BI, Business Intelligence) 도구 
    - 데이터 시각화 도구
    - 고급 분석 도구 
- 데이터 웨어하우스 생태계
  - 데이터 웨어하우스 생태계(ecosystem)에서 데이터가 어떻게 흘러가는지 보여줌 
  - 최종 사용자가 필요한 보고와 분석을 받을 수 있도록 나머지 도구는 ETL 개발자, 데이터와 시스템 아키텍트, 데이터 모델 전문가, 데이터 관리자, 보고 및 BI 개발자, 데이터베이스 관리자 등 여러 IT 전문가가 사용함 
- 데이터 저장과 쿼리
  - 차원 모델링과 스타 스키마
    - 정규화된 데이터 모델은 중복과 필드 수가 작은 테이블을 만들다 보니 업데이트가 매우 빠르게 이뤄짐 
    - 데이터 웨어하우스에서는 일반적으로 정규화하지 않는 데이터 모델을 선호함. 정규화하지 않은 모델에서는 각 테이블에 가능한 한 많은 관련 특성을 저장함. 그러면 필요한 모든 정보를 취합할 때도 데이터를 한 번만 처리해도 됨 
    - 스타 스키마(star schema) : 1996년에 발간된 랄프 킴벌, 마기 로스 공저의 The Data Warehouse Toolkit(wiley,2004)1판에서 처음 소개됨. 이 스키마는 몇 개의 차원과 팩트 테이블 집하으로 구성됨 
      - 차원 테이블은 분석 대상 요소로 구성됨 
      - 팩트 테이블에는 각 차원과 관련된 모든 활동이 기록됨.
      - 거래 팩트 테이블에는 각 주문 건의 개별 항목 각각을 대변하는 레코드들이 있을 것. 각 기록에는 주문한 고객이 나오는 고객 차원 테이블에서 해당 고객의 고객 키 값, 주문이 이뤄진 시간이 포함된 시간 차원 테이블에서 해당 시간 키 값, 주문한 제품이 포함된 제품 차원 테이블에서의 제품 키 값 뿐만 아니라 거래 자체의 특성(주문과 세부 항목 번호 식별자, 수량, 지불한 가격 등)이 있을 것 
    - 느린 변경 차원
      - 정확한 데이터 분석을 하려면 시간에 따른 개인의 상태를 추적할 필요가 있음 
      - 특수한 구성 개념(construct), 느린 변경 차원이 개발됨 
      - 느린 변경 차원의 목적은 시간에 따른 차원 엔티티(예,개인)의 상태를 추적해서 거래에서 발생한 정보(혹은 팩트)가 당시의 상태를 반영하게 하고, 결국 장기적으로 분석의 정확도를 높이는 것 
      - 느린 변경 차원을 적용하면 ETL 잡(job)과 분석 쿼리 모두 상당히 복잡해지기 때문에 가장 중요한 특성의 이력만 추적하는 것이 좋음 
    - 고도 병렬 처리(MPP) 
      - 스타 스키마를 사용하지 않고 고도 병렬 처리 컴퓨터 여러 대를 사용하는 방법
      - 고도 병렬 처리(MPP, Massively Parallel Processing) 기술을 개발했기 때문에 테라데이터(Teradata)는 대규모 데이터 웨어하우스들이 가장 많이 사용하는 데이터베이스가 될 수 있었음. 전용 하드웨어, 소프트웨어, 네트워크 프로토콜 사용을 통해 테라데이터 데이터 웨어하우스는 하둡 등장 전까지는 업계에서 독보적이었던 확장성을 가질 수 있었음 
      - 테라데이터는 여러 컴퓨터를 병렬로 운영할 수 있었기 때문에 사용자가 데이터를 특정 형태로 모델링하지 않아도 됐음 
    - 데이터 웨어하우스(DW) 기기
      - 데이터 웨어하우스 기기는 전용 하드웨어와 소프트웨어에서 실행되는 고성능 데이터베이스를 표준적인 상용 데이터베이스보다 배포와 관리가 쉽게 하려고함. IBM 네티자(Netezza) 
    - 열 기반 가게
      - 관계형 데이터베이스는 데이터를 행(row, 레코드라고도 함)과 열(column, 필드라고도 함)로 구성된 테이블로 모델링함 
      - 실제로는 기록이 디스크 블록 단위에 딱 맞지 않고, 데이터 구조아 색인 정보도 공간을 차지하기 때문에 더 많이 필요함 
      - 열 기반(columnar) 데이터베이스는 각 행의 모든 데이터를 묶어 저장하지 않고 각 열의 모든 데이터를 묶어 저장함 
      - 어떤 사용자의 모든 정보(300개 열 모두)를 묻는 쿼리라면 행 기반 데이터베이스는 하나의 블록만 읽으면 되지만 열 기반 데이터베이스는 블록 300개를 읽어야 함. 일반적인 관계형 데이터베이스와는 달리 열 기반 데이터베이스는 매우 특수한 쿼리 패턴을 위해 사용함
      - 많이 알려진 수직적 데이터베이스로는 사이베이스 IQ(Sybase IQ)와 버티카(Vertica)가 있음 
    - 인메모리 데이터베이스
      - 디스크 접근 최적화, 캐싱과 사전 캐싱, 블록 읽기 횟수를 줄이기 위한 색인 생성 등 블록 읽기 횟수를 최소화하는 데 상당한 노력이 필요함 
      - 메모리의 가격이 내려가면서 메모리에 더 많은 데이터를 저장하는 것이 가능해졌고, 메모리에 데이터를 저장하고 처리하는 데이터 베이스 시스템이 출현(타임스텐,TimesTen)
      - SAP에서 공급하는 HANA 시스템, 아파치 스파크 프로젝트 외에도 여러 프로젝트가 추진 중 
- 데이터 로딩: 데이터 통합 도구
  - ETL
