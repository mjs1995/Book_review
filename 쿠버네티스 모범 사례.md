# 서평
- 이 책은 쿠버네티스에 애플리케이션을 배포하거나 쿠버네티스 기반 애플리케이션에 적용할 수 있는 패턴과 사례를 배우려는 실무자를 대상으로 하고 있습니다.

# 기본 서비스 설치
- 설정 파일 관리
  - 쿠버네티스에서는 모든 것을 선언적(declarative)으로 표현함. 즉 클러스터 내에서 애플리케이션의 의도한 상태(desired state)를 작성하며(일반적으로 YAML이나 JSON 파일) 애플리케이션의 모든 구성 요소에 대해 상태를 정의함.
  - 클러스터의 상태를 일련의 변경의 합으로 보는 명령적(imperative) 접근방법도 있지만, 쿠버네티스는 선언적 방법을 선호함
  - 쿠버네티스는 YAML과 JSON을 지원함. 일반적으로 애플리케이션의 상태를 선언할 때는 JSON 보다 YAML을 선호함. YAML이 JSON보다 좀 더 간결하고 수정이 쉽기 때문. 대신 YAML은 들여쓰기에 민감함. YAML에서 들여쓰기를 잘못한 탓에 종종 쿠버네티스 설정 오류가 발생함
- 디플로이먼트를 이용한 복제 서비스 생성
  - 이미지 관리 모범 사례
    - 컨테이너 이미지를 구축하고 관리하는 것. 이미지 구축 과정은 공급망 공격(supply chain attack)에 취약함. 공급망 공격이란 신뢰할 수 있는 소스의 의존 이미지에 악의적인 사용자가 코드나 바이너리를 삽입해서, 결국 독자의 애플리케이션에 이를 내장시키는 것 
    - 네이밍(naming)과 관련된 모범 사례도 있음. 이론적으로는 이미지 레지스트리에 존재하는 컨테이너 이미지 버전을 변경할 수 있음. 하지만 실제로는 버전 태그를 변경하지 않는 것이 좋음. 그래서 의미론적 버전과, 이미지가 빌드된 커밋의 SHA 해시(hash)와 결합해 네이밍하는 것을 권장함(예를 들어 v1.0.1-bfeda01f). 만약 이미지 버전 태그를 명시하지 않는다면 latest를 기본으로 사용함
  - 애플리케이션 레플리카 생성
    - 쿠버네티스에서 레플리카셋(ReplicaSet)은 컨테이너화된 애플리케이션의 레플리카를 관리하는 리소스. 하지만 레플리카셋을 사용하는 대신 디플로이먼트(Deployment) 리소스를 사용하는 것을 권장함. 
    - 디플로이먼트는 레플리카셋의 복제 기술과 더불어 버전 과리, 단계적 롤아웃도 지원함. 디플로이먼트를 사용하면 쿠버네티스에 내장된 도구를 이용해 애플리케이션 버전을 변경할 수 있음 
    - 컨테이너의 리소스 요청(request)와 제한(limit)에도 주목해야 함.
      - 요청은 애플리케이션을 실행하는 호스트 장비가 보장해주는 리소스 크기이고 제한은 컨테이너가 사용할 수 있는 최대 리소스 크기
      - 요청과 제한을 동일한 값으로 설정하면 애플리케이션은 대부분 예상대로 동작함. 대신 리소스 이용률을 높일 수 없다는 단점이 있음
      - 애플리케이션이 과도하게 스케줄링되거나 유휴 리소스를 과소비하는 것을 방지한다는 장점은 있지만, 세밀하게 튜닝하지 않으면 리소스를 최대로 사용할 수 없음 
    - 클러스터 항목과 소스 관리 항목이 정확히 일치해야 함. 가장 좋은 방안은 깃옵스(GitOps)로 지속적 통합(CI)과 지속적 배포(CD)를 자동화하여 특정 브랜치만 운영에 배포하는 것. 이 방식으로 소스 관리와 운영을 일치시킬 수 있음 
- HTTP 트래픽을 처리하는 외부 인그레스 설정
  - 외부에 노출하려면 실제로 두 개의 쿠버네티스 리소스를 사용함. 
    - 첫 번째는 전송 제어 프로토콜(TCP, Transmission Control Protocol) 또는 사용자 데이터그램 프로토콜(UDP, User Datagram Protocol) 트래픽을 로드 밸런싱하는 서비스
    - 인그레스 리소스는 HTTP 경로와 호스트 기반의 요청을 지능적으로 라우팅(routing) 할 수 있는 HTTP (S) 로드 밸런싱을 지원함
- 컨피그맵으로 애플리케이션 설정
  - 설정을 사용하면 사용자의 요구사항 변경이나 애플리케이션 코드가 실패했을 때 빠르게(그리고 동적으로) 기능을 활성화하고 비활성화할 수 있음. 하나의 기능 단위로 롤아웃이나 롤백할 수 있음
  - 쿠버네티스에서는 컨피그맵 리소스로 설정을 정의함. 컨피그맵은 설정 정보나 파일을 나타내는 다중 키/값 쌍을 가짐. 이 설정 정보는 파드 내의 컨테이너에 파일이나 환경 변수 형태로 전달됨
- 시크릿 인증 관리
  - 볼륨은 사용자가 지정한 위치에 존재하는 하나의 파일이나 디렉터리로, 실행 중인 컨테이너에 마운트될 수 있음. 시크릿은 tmpfs 램 기반의 파일 시스템으로 볼륨을 생성해 컨테이너에 마운트됨. 따라서 장비가 물리적인 피해를 입더라도(클라우드가 아닌 데이터 센터라면 가능) 공격자가 시크릿을 취득하기 어려움 
- 간단한 스테이트풀 데이터베이스 배포
  - 쿠버네티스에서는 노드 상태, 업그레이드, 리밸런싱 등 여러 이유로 파드가 다시 스케줄링됨. 이러한 일이 발생하면 파드는 다른 서버로 옮겨짐. 이때 레디스 인스턴스와 연관된 데이터가 특정 장비나 컨테이너 자체에 존재하는 경우, 컨테이너가 이관되거나 재시작될 때 해당 데이터가 손실됨. 이를 방지하려면 스테이트풀 작업을 실행할 때, 원격 퍼시스턴트볼륨(PersistentVolume)을 사용하여 애플리케이션 상태를 처리해야 함 
  - 시크릿과는 달리 퍼시스턴트볼륨은 일반적으로 원격 스토리지에 존재하며, 파일 기반의 네트워크 파일 시스템(Network File System,NFS), 서버 메시지 블록(Server Message Block, SMB), 블록 기반의 iSCSI, 클라우드 기반 디스크 등 다양한 네트워크 프로토콜을 통해 마운트됨.
  - 일반적으로 데이터베이스와 같은 애플리케이션의 경우 성능이 더 좋은 블록 기반 디스크를 선호하지만, 성능이 주요 고려 대상이 아니라면 유연성이 높은 파일 기반 디스크가 낫음
  - 퍼시스턴트볼륨클레임(PersistentVolumeClaim), 클레임은 리소스 요청을 생각하면 됨. 레디스가 50GB가 필요하다고 추상적으로 선언하면 쿠버네티스 클러스터는 적절한 퍼시스턴트볼륨을 제공할 방법을 결정함
    - 디스크 명세(specification)가 다를 수 있는 여러 클라우드나 온프레미스 사이에서 이식할 수 있도록 스테이트풀셋을 작성할 수 있음
    - 퍼시스턴트볼륨 타입은 오직 하나의 파드에 마운트될 수 있지만, 볼륨클레임을 사용해 작성한 템플릿은 복제가 가능하며 따라서 각 파드는 자신만의 퍼시스턴트볼륨을 할당받을 수 있음 
- 인그레스를 이용해 트래픽을 정적 파일 서버로 전달
  - 정적 파일 서버는 HTML, CSS, 자바스크립트, 그림 파일을 제공하는 역할을 함 
- 서비스 배포 모범 사례
  - 대부분의 서비스는 디플로이먼트 리소스로 배포되어야 함. 디플로이먼트는 중복과 확장을 위해 레플리카를 생성함
  - 디플로이먼트는 로드 밸런서인 서비스를 통해 노출됨. 서비스는 클러스터 내부(기본값) 혹은 외부에 노출될 수 있음. HTTP 애플리케이션을 노출하려면 인그레스 컨트롤러를 사용할 수 있으며 요청 라우팅과 SSL도 추가할 수 있음 
  - 애플리케이션의 설정을 다양한 환경에서 재사용하려면 애플리케이션을 파라미터화해야 함. 헬름과 같은 패키징 도구는 이러한 파라미터화를 위한 최고의 선택 

# 개발자 워크플로
- 쿠버네티스는 소프트웨어를 안정적으로 운영하기 위해 만들어졌음. 애플리케이션 지향 API, 자체 복구 속성, 소프트웨어의 다운타임 없이 롤아웃할 수 있는 디플로이먼트 등 유용한 도구를 제공함
- 목표
  - 개발자와 클러스터 사이의 상호작용 단계
    - 온보딩
      - 개발자에게 계정을 생성해주고 첫 배포까지 지원해줌. 개발자가 최대한 이른 시일 안에 적응하도록 돕는게 목표.
      - 핵심 성과 지표(KPI, key performance indicator)도 세워야함. 
      - 빈손으로 시작한 사용자가 30분 내에 애플리케이션의 최신 버전을 실행하도록 만들기가 좋은 예
    - 개발
      - 개발자는 매일 개발을 함. 이 단계의 목표는 빠른 반복과 디버그
      - 개발자는 클러스터에 코드를 빠르고 반복적으로 푸시함. 문제가 발생했을때는 쉽게 코드를 테스트하고 디버그하길 원함
    - 테스트
      - 코드 제출과 병합 전 검증 작업을 진행함
      - 목표
        - 풀 리퀘스트를 제출하기 전에 자신의 환경에서 이를 테스트할 수 있어야 함. 
        - 코드가 리포지터리에 병합되기 전에 모든 테스트가 자동으로 실행돼야 함
- 개발 클러스터 구축
  - 쿠버네티스에서 개발을 시작할 때, 먼저 대규모 단일 개발 클러스터와 개발자별 클러스터 중 하나를 선택해야 함
  - 장단점
    - 개발자별 클러스터의 가장 치명적인 단점은 큰비용과 낮은 효율성, 관리해야 할 클러스터의 수가 많다는 것. 
      - 추가로 각 클러스터의 사용률이 굉장히 저조할 가능성이 존재함
      - 리소스를 추적하여 사용하지 않는 리소스를 삭제하는 것이 어려움. 
      - 개발자별 클러스터의 장점은 단순하다는 것. 각 사용자는 고립된 클러스터를 독립적으로 관리하게 되므로 서로에게 피해를 주지 않음
    - 대규모 단일 개발 클러스터는 굉장히 효율적
      - 개발자 수가 같을 때 1/3 가격 (혹은 더 적은)의 공용 클러스터로 처리가 가능함. 또한 모니터링과 로깅처럼 개발자에게 필요한 공용 클러스터 서비스를 설치하는 것이 더욱 수월함.
      - 단점은 사용자 관리 프로세스와 잠재적인 개발자 간의 간섭. 새로운 사용자와 네임스페이스를 추가하는 과정은 간단하지 않기 때문에, 새로운 개발자를 위한 온보딩 프로세스를 만들어야 함
      - 한 사용자가 너무 많은 리소스를 사용하는 경우, 개발 클러스터가 먹통이 되어 다른 애플리케이션과 개발자에게 리소스가 할당되지 않는다는 문제도 있음. 이때 쿠버네티스 리소스 관리나 역할 기반 접근 제어(RBAC, role-based access control)로 개발자 사이의 충돌을 줄일 수 있음
      - 개발자가 리소스를 생성해야 하며 누수를 일으키거나 방치되지 않았는지 신경을 써야 함. 그래도 개발자가 직접 클러스터를 생성하는 것보다 쉬움
    - 대규모 단일 클러스터를 추천함
      - 개발자들 사이의 간섭 문제가 있지만 충분히 관리가 가능함. 궁극적으로 비용 대비 효율성이 높으며 클러스터에 조직 단위 기능을 쉽게 추가할 수 있다는 것이 큰 장점
      - 그 대신 개발자 온보딩 프로세스, 리소스 관리, 가비지 컬렉션에 투자해야함. 처음에는 대규모 단일 클러스터를 시도해보고 추후 조직이 거대해지면(혹은 이미 거대하다면) 수백 명의 사용자를 가진 거대 클러스터 대신 팀이나 그룹 단위(10~20명) 클러스터를 고려해보기. 비용과 관리 측면에서 좋은 방법
- 다중 개발자를 위한 공용 클러스터 구축
  - 네임스페이스는 서비스 배포 범위를 제한하여 특정 사용자의 프런트엔드 서비스가 다른 사용자의 프런트엔드 서비스에 피해를 주지 않도록 만듬. 또한 RBAC의 범위도 제한하기 때문에 특정 개발자가 실수로 다른 개발자의 작업 결과를 삭제하는 것을 막을 수 있음
  - 공용 클러스터에서 개발자의 작업 공간으로 네임스페이스를 사용하는 것은 좋은 생각임
  - 사용자 온보딩
    - 사용자가 네임스페이스를 할당받기 위해서는 먼저 쿠버네티스 클러스터 자체에 접근할 수 있어야 함
      - 첫 번째로 증명(certificate) 기반 인증을 사용할 수 잇음. 사용자를 위한 새로운 증명을 생성하여 로그인에 사용할 수 있는 kubeconfig 파일을 전달해주는 것
      - 두 번째로 외부 신원 시스템, 예를 들면 마이크로소프트 애저 액티브 디렉터리(Azure Active Directroy) 또는 AWS 아이덴티티 엔드 액세스 매니지먼트(IAM)를 사용해 클러스터에 접근하도록 설정할 수 있음
    - 일반적으로 신원 데이터를 한곳에서 관리할 수 있는 외부 신원 시스템을 사용하는 것이 모범 사례. 외부 신원 시스템을 사용할 수 없는 상황이라면 증명을 사용해야 함. 다행히 쿠버네티스 증명 API를 사용하여 이를 생성하고 관리할 수 있음 
  - 네임스페이스 생성과 보안
    - 배포할 컴포넌트를 빌드하는 팀의 연락처와 같은 메타데이터를 네임스페이스에 추가하고는 함. 일반적으로 애너테이션(annotation) 형태
    - Jinjia 등의 템플릿을 이용해 YAML 파일로 네임스페이스를 먼저 생성하고 애너테이션을 넣을 수 있음 
    - ```shell
      ns='my-namespace'
      kubectl create namespace ${ns}
      kubectl annotate namespace ${ns} annotation_key=annotation_value
      ```
    - 보안을 위해 특정 사용자에게만 네임스페이스 접근 권한을 부여할 수 있음. 네임스페이스 안의 사용자에게 롤바인딩을 하는 것. 이를 위해 네임스페이스 안에 롤바인딩 객체를 생성함
    - ```yaml 
      apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: example
        namespace: my-namespace
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: edit
      subjects:
      - apiGroup: rbac.authorization.k8s.io
        kind: User
        name: myuser
      ```
    - 롤바인딩을 생성하려면 kubectl create -f role-binding.yaml을 실행하면 됨. 이것은 재사용도 가능함. 네임스페이스를 변경하고 바인딩의 네임스페이스를 업데이트하면됨 
    - 사용자가 다른 롤바인딩을 가지고 있지 않다면 사용자는 클러스터에서 이 네임스페이스에만 접근할 수 있음. 
    - 전체 클러스터에 읽기 권한을 부여하는 것도 실용적임. 이를 통해 개발자는 자신의 작업에 영향을 미치는 다른 사람의 작업을 볼 수 있음. 하지만 클러스터의 시크릿 리소스에 접근할 수 있기 때문에 읽기 권한을 부여할 때에는 주의해야 함
    - 특정 네임스페이스의 리소스 사용량을 제한하고 싶다면 리소스쿼터를 이용함
      - 예를 들어 다음 리소스쿼터는 네임스페이스에 속한 파드의 요청과 제한 모두를 10코어와 100GB 메모리로 제한함
      - ```yaml
        apiVersion: v1
        kind: ResourceQuota
        metadata:
          name: limit-compute
          namespace: my-namespace
        spec:
          hard:
            requests.cpu: "10"
            requests.memory: 100Gi
            limits.cpu: "10"
            limits.memory: 10Gi
        ```
  - 네임스페이스 관리
    - 개발자에게 네임스페이스를 할당하는 방법
      - 온보딩 과정의 일부로 개발자에게 네임스페이스를 부여하는 것. 그러면 온보딩 이후 자연스레 애플리케이션을 개발하고 관리할 수 있는 전용 작업 공간을 가지게 됨.
      - 영구적인 네임스페이스에서 작업한 산출물이 방치될 수 있으므로 개별 리소스를 파악하고 가비지 컬렉션하는 것이 어려울 수 있음. 대안으로 타임 투 리브(TTL, time to live)로 네임스페이스를 임시로 생성하여 할당하는 방법이 있음.
      - 개발자는 클러스터 내의 리소스를 한시적이라고 생각할 것이며 TTL이 만료되었을 때 네임스페이스 삭제의 자동화를 쉽게 구축할 수 있음 
    - 쿠버네티스와 통합한다면 kubectl 도구를 이용하여 동적으로 새로운 네임스페이스를 생성하고 할당할 수 있는 사용자 리소스 정의(CRD, Custom Resource Definition)를 구현할 수 있음
      - 네임스페이스를 조사하여 TTL이 만료된 것들을 삭제하는 간단한 스크립트를 만들면 됨 
- 반복적 개발
  - 새로운 코드를 배포하는 가장 쉬운 방법은 이전 디플로이먼트 객체를 지우고 새로운 이미지를 가리키는 새로운 디플로이먼트를 생성하는 것
  - 기존 디플로이먼트를 직접 수정할 수도 있지만 이렇게 하면 디플로이먼트 리소스 롤아웃 로직이 동작하게 됨 
  - ```bash
    kubectl delete -f ./ex.yaml
    perl -pi -e 's/${old_version}/${new_version}/' ./ex.yaml
    kubectl create -f ./ex.yaml
    ```
- 개발 환경 설정 모범 사례 
  - 개발자 경험을 온보딩, 개발, 테스팅의 세 단계로 나누어 생각해봄. 구축한 개발 환경이 이 모든 단계를 지원하는지 확인해야 함 
  - 개발 클러스터를 구축할 때 대규모의 단일 클러스터와 개발자별 클러스터 중 하나를 선택할 수 있음. 각각의 장단점이 있지만 일반적으로 대규모의 단일 클러스터가 더 나은 방법
  - 클러스터에 사용자를 추가할 대, 사용자의 신원을 추가하고 자신의 네임스페이스에만 접근하도록 하세요. 사용할 수 있는 리소스의 크기는 제한해야 함
  - 네임스페이스를 관리할 때, 오랫동안 사용하지 않은 리소스를 어떻게 회수할지 고민하세요. 사용하지 않는 리소스를 삭제하지 않는 것은 개발자의 나쁜 버릇. 정리를 자동화해야 함
  - 모든 사용자를 위해 로그와 모니터링 같은 크러스터 수준의 서비스를 고민하세요. 때로는 헬름 차트와 같은 템플릿을 사용하는 모두를 위해, 데이터베이스와 같은 클러스터 수준의 의존 관계가 필요함

# 모니터링과 로깅
- 메트릭 vs 로그
  - 메트릭 : 특정 기간에 측정한 일련의 숫자
  - 로그 : 시스템을 탐색적으로 분석하기 위해 사용됨 
- 모니터링 기술
  - 블랙박스 모니터링은 애플리케이션 외부에 초점을 둠. 일반적으로 CPU, 메모리, 스토리지 등의 시스템 컴포넌트를 모니터링함
  - 클러스터가 정상인지 테스트해보기 위해 수행하는 파드 스케줄링이 블랙박스 모니터링의 예시 
  - 화이트박스 모니터링은 애플리케이션 상태에 초점을 둠. 예를 들면 전체 HTTP 요청, 500 오류 수, 요청 레이턴시 등. 이를 통해 시스템 상태가 왜 이런지를 이해할 수 있음 
- 모니터링 패턴
  - 브렌던 그레그의 USE 패턴
    - U(utilization): 사용률
    - S(saturation): 포화도
    - E(error): 오류율
    - 이 패턴은 애플리케이션 수준의 모니터링에 사용하기에는 한계가 있어 인프라 모니터링에만 활용됨. USE 패턴은 모든 리소스에 대해 사용률, 포화도, 오류율을 확인함.
    - 시스템의 리소스 제약과 오류율을 빠르게 파악할 수 있음. 예를 들어 클러스터 노드의 네트워크 상태를 점검하기 위해 사용률, 포화도, 오류율을 모니터링해서 네트워크 병목이나 네트워크 스택의 오류를 쉽게 알 수 있음 
  - 톰 윌키의 RED 패턴
    - R(request): 요청
    - E(error): 오류율
    - D(duration): 소요 시간
  - 구글의 네 가지 황금 신호(four golden signals)
    - 레이턴시: 요청을 처리하는 데 걸리는 시간
    - 트래픽: 시스템에 존재하는 요청의 양
    - 오류율: 요청 실패율
    - 포화도: 서비스의 사용률
  - USE와 RED는 상호보완적임. USE는 인프라 컴포넌트에 초점을 맞추며 RED는 애플리케이션의 최종 UX를 모니터링하는 데 중점을 둠 
- 쿠버네티스 메트릭 개요
  - 쿠버네티스 클러스터는 컨트롤 플레인 컴포넌트와 워커 노드 컴포넌트로 구성됨.
    - 컨트롤 플레인 컴포넌트는 API 서버, etcd, 스케줄러, 컨트롤러 관리자로 이루어져 있음 
    - 워커 노드는 kubelet, 컨테이너 런타임, kube-proxy, kube-dns, 파드로 이루어져 있음. 클러스터와 애플리케이션이 정상인지 확인하려면 모든 컴포넌트를 모니터링해야 함 
  - cAdvisor
    - 컨테이너 어드바이저 또는 cAdvisor는 쿠버네티스 오픈 소스 프로젝트로 노드에서 실행 중인 컨테이너의 리소스와 메트릭을 수집함. cAdvisor 쿠버네티스 kubelet에 내장되어 클러스터의 모든 노드에서 실행됨
    - 리눅스 cgroups(control groups) 트리를 통해 메모리와 CPU 메트릭을 수집함. cgroups는 CPU, 디스크 IO, 네트워크 IO 리소스를 고립시킬 수 있는 리눅스 커널 기능으로 리눅스 커널에 내장된 statfs를 이용해 디스크 메트릭을 수집함
  - 메트릭 서버
    - 리소스 메트릭 API의 표준 구현체가 메트릭 서버. 메트릭 서버는 CPU와 메모리 같은 리소스 메트릭을 수집함. kubelet API로부터 수집하며 메모리에 저장함. 이 메트릭은 스케줄러, 수평 파드 오토스케일러(HPA), 수직 파드 오토스케일러(VPA)에서 사용됨
    - 사용자 정의 메트릭 API를 이용해 모니터링 시스템에서 임의의 메트릭을 수집할 수 있음. 모니터링 솔루션은 사용자 정의 어댑터를 구축해 리소스 메트릭을 확장할 수 있음 
  - kube-state-metrics
    - kube-state-metrics가 해결해 줄 질문
      - 파드
        - 클러스터에 몇 개의 파드가 배포되었나요?
        - 대기 중인 파드는 몇 개인가요?
        - 파드 요청을 처리할 수 있을 만큼의 충분한 리소스가 있나요?
      - 디플로이먼트
        - 의도한 상태에서 수행 중인 파드는 몇 개인가요?
        - 몇 개의 레플리카가 가용한가요?
        - 어떤 디플로이먼트가 업데이트되었나요?
  - 톰 윌키의 RED 패턴
    - R(request): 요청
    - E(error): 오류율
    - D(duration): 소요 시간
  - 구글의 네 가지 황금 신호(four golden signals)
    - 레이턴시: 요청을 처리하는 데 걸리는 시간
    - 트래픽: 시스템에 존재하는 요청의 양
    - 오류율: 요청 실패율
    - 포화도: 서비스의 사용률
  - USE와 RED는 상호보완적임. USE는 인프라 컴포넌트에 초점을 맞추며 RED는 애플리케이션의 최종 UX를 모니터링하는 데 중점을 둠 
- 쿠버네티스 메트릭 개요
  - 쿠버네티스 클러스터는 컨트롤 플레인 컴포넌트와 워커 노드 컴포넌트로 구성됨.
    - 컨트롤 플레인 컴포넌트는 API 서버, etcd, 스케줄러, 컨트롤러 관리자로 이루어져 있음 
    - 워커 노드는 kubelet, 컨테이너 런타임, kube-proxy, kube-dns, 파드로 이루어져 있음. 클러스터와 애플리케이션이 정상인지 확인하려면 모든 컴포넌트를 모니터링해야 함 
  - cAdvisor
    - 컨테이너 어드바이저 또는 cAdvisor는 쿠버네티스 오픈 소스 프로젝트로 노드에서 실행 중인 컨테이너의 리소스와 메트릭을 수집함. cAdvisor 쿠버네티스 kubelet에 내장되어 클러스터의 모든 노드에서 실행됨
    - 리눅스 cgroups(control groups) 트리를 통해 메모리와 CPU 메트릭을 수집함. cgroups는 CPU, 디스크 IO, 네트워크 IO 리소스를 고립시킬 수 있는 리눅스 커널 기능으로 리눅스 커널에 내장된 statfs를 이용해 디스크 메트릭을 수집함
  - 메트릭 서버
    - 리소스 메트릭 API의 표준 구현체가 메트릭 서버. 메트릭 서버는 CPU와 메모리 같은 리소스 메트릭을 수집함. kubelet API로부터 수집하며 메모리에 저장함. 이 메트릭은 스케줄러, 수평 파드 오토스케일러(HPA), 수직 파드 오토스케일러(VPA)에서 사용됨
    - 사용자 정의 메트릭 API를 이용해 모니터링 시스템에서 임의의 메트릭을 수집할 수 있음. 모니터링 솔루션은 사용자 정의 어댑터를 구축해 리소스 메트릭을 확장할 수 있음 
  - kube-state-metrics
    - kube-state-metrics가 해결해 줄 질문
      - 파드
        - 클러스터에 몇 개의 파드가 배포되었나요?
        - 대기 중인 파드는 몇 개인가요?
        - 파드 요청을 처리할 수 있을 만큼의 충분한 리소스가 있나요?
      - 디플로이먼트
        - 의도한 상태에서 수행 중인 파드는 몇 개인가요?
        - 몇 개의 레플리카가 가용한가요?
        - 어떤 디플로이먼트가 업데이트되었나요?
      - 노드
        - 워커 노드의 상태는 어떠한가요?
        - 클러스터에서 할당할 수 있는 CPU 코어는 몇 개인가요?
        - 스케줄되지 않은 노드가 있나요?
      - 잡
        - 잡이 언제 시작되었나요?
        - 잡이 언제 완료되었나요?
        - 몇 개의 잡이 실패했나요?
