# 서평
- 데이터 분석이 잘 이뤄지기 위해서는 분석에 필요한 형태로 잘 정리된 데이터가 필요하고, 원하는 분석 결과를 얻기 위해서는 적합한 기간의 정확한 데이터가 필요함, 이러한 데이터들은 잘 구성된 데이터 파이프라인 안에서만 만들어질수 있음 
- 이 책은 데이터 파이프라인의 전반적인 소개와 패턴을 다룸, 데이터 파이프라인 각 단계를 계획할 때부터 구성 후 검증하고 유지 관리하는 전체 과정에서 고려할 점들과 활용할 수 있는 예시 코드를 함께 제공함 

# 데이터 파이프라인 소개
- 데이터 파이프라인이란?
  - 다양한 소스에서 새로운 가치를 얻을 수 있는 대상으로 데이터를 옮기고 변환하는 일련의 과정, 분석,리포팅,머신러닝 능력의 기초가됨
  - 데이터 파이프라인의 복잡성은 원본 데이터의 크기와 상태, 구조 및 분석 프로젝트의 요구사항에 따라서도 달라짐
  - 데이터 추출, 데이터 가공, 데이터 유효성 검사를 포함한 여러 단계로 구성되며, 때로는 데이터를 최종 목적지로 전달하기 전에 머신러닝 모델을 학습하거나 실행하는 단계가 있기도 함
  - 파이프라인에는 여러 시스템과 프로그래밍 언어의 작업이 포함되는 경우가 많음
  - 데이터 팀은 일반적으로 종속성을 공유하고 조정해야 하는 수많은 데이터 파이프라인을 소유하고 유지함 
- 누가 파이프라인을 구축할까?
  - 데이터 엔지니어는 분석 상태계를 뒷받침하는 데이터 파이프라인을 구축하고 유지하는 데 전문적인 역량을 갖추고잇음 
  - 데이터 과학자 및 분석가와 긴밀히 협력하여 데이터를 어떻게 처리해야 하는지 파악하고 요구사항을 확장 가능한 프로덕션 상태로 전환하는 데 도움을 줌 
    - SQL과 데이터 웨어하우징 기초
      - 숙련된 데이터 엔지니어는 고성능의 SQL 작성 방법을 알고 데이터웨어하우징 및 데이터 모델링의 기본 사항을 이해함
    - 파이썬 그리고/또는 자바
    - 분산 컴퓨팅
      - 데이터 양이 많아지고 데이터를 신속하게 처리하고 하는 요구사항이 늘어나면서 데이터엔지니어들은 분산 컴퓨팅 플랫폼을 사용하기 시작, 
      - 분산 컴퓨팅은 여러 시스템의 성능을 결합하여 대량의 데이터를 효율적으로 저장, 처리 및 분석함
        - 하둡 분산 파일 시스템(HDFS)를 통한 분산 파일 스토리지, 맵리듀스를 통한 처리, 피그(pig)를 통한 데이터 분석 등을 포함하는 하둡 에코시스템, 아파치 스파크는 하둡을 빠르게 능가하는 또 다른 인기 분산 처리 프레임워크 
      - 데이터 엔지니어는 이러한 프레임워크를 언제 어떻게 활용해야 하는지 알아야 함 
    - 기본 시스템 관리
      - 리눅스 명령줄에 능숙, 응용 프로그램 로그 분석, 크론 작업 예약, 방화벽 및 기타 보안 설정의 문제 해결과 같은 작업을 수행할 수 있어야함
    - 목표 지향적 사고방식
      - 데이터 엔지니어가 파이프라인을 구축하는 이유를 알 때 더 나은 아키텍처 결정을 내릴 수 있음
- 왜 파이프라인을 구축할 까?
  - 원본 데이터는 정리, 정형화, 정규화, 결합, 집계, 그리고 때로는 마스킹 또는 보안을 위해 정제됨 
- 어떻게 데이터 파이프라인을 구축할까?
  - 오픈소스도 있고, 상업용도 있고, 자체 개발 제품도 있음
  - 파이프라인 구축을 위한 가장 인기 있는 솔루션 및 프레임워크를 살펴보고 조직의 요구 사항과 제약 조건에 따라 어떤 제품을 사용할지 결정하는 방법을 알아봄
  - 파이프라인을 구축하고 이를 안정적이고 안전하게 제시간에 제공하고 처리하는 인프라를 지원해야함 

# 최신 인프라 데이터
- 데이터 소스의 다양성
  - 최신 데이터 인프라의 핵심 구성 요소
    - 데이터 소스의 다양성
    - 데이터 수집 도구
    - 클라우드 데이터 웨어하우스와 데이터 레이크
    - 모델링 도구 및 프레임워크
    - 워크플로 오케스트레이션 플랫폼 
- 소스 시스템 소유권
  - 데이터 수집(data ingestion) : 한 소스에서 데이터를 추출하여 다른 소스로 로드하는 것
  - 소스 시스템이 위치하는 곳이 어디인지를 이해하는 것은 여러 가지로 중요함
    - 타사 데이터 소스에 위치한 데이터에 액세스하려고 한다면 액세스 방법에 제한이 있을수 있음
    - 데이터를 사용자가 필요로 하는 형태에 맞추어 정의하는 등
    - 데이터 수집이 시스템에 의도하지 않는 부하를 가하는지부터 데이터를 점진적(incremental)으로 로드할 수 있는지 여부까지 다양한 과제가 발생하기 때문
- 수집 인터페이스 및 데이터 구조
  - 소스 데이터를 얻는 방법과 형식
    - Postgres 또는 MySQL 데이터베이스와 같은 애플리케이션 뒤에 있는 데이터베이스
    - REST API와 같은 시스템 상단의 추상화 계층
    - Apache Kafka와 같은 스트림 처리 플랫폼
    - 로그, 쉼표로 구분된 값(csv) 파일 및 기타 플랫 파일을 포함하는 공유 네트워크 파일 시스템(NFS) 또는 클라우드 스토리지 버킷
    - 데이터 웨어하우스 또는 데이터 레이크
    - HDFS 또는 HBase 데이터베이스의 데이터 
  - 데이터 구조
    - REST API의 JSON
    - MySQL 데이터베이스의 잘 구성된 데이터
    - MySQL 데이터베이스의 테이블의 열 내의 JSON
    - 반정형화된 로그 데이터
    - CSV,고정 폭 형식(FWF) 및 기타 플랫 파일 형식
    - 플랫 파일의 JSON
    - Kafka의 스트림 출력
  - 분석 프로젝트에 더 적합한 형태로 정형화하기 위해서는 데이터 수집 이 외에도 클렌징, 변환 작업 등의 추가 단계가 파이프라인에 필요할 수 있음
  - JSON과 같은 반정형 데이터가 점점 보편화 되고 있으며 속성-값 구조와 객체의 중첩(nesting)구조의 이점을 가지고 있음
  - 관계형 데이터베이스와 달리 같은 데이터세트 안의 데이터 구조가 모두 동일하다는 보장은 없음, 파이프라인에서 누락되거나 불완전한 데이터를 처리하는 방법은 상황에 따라 달라지며 데이터의 유연성이 증가할수록 점점 더 많이 필요함 
- 데이터 클레징 작업과 유형성 검사
  - 지저분한 데이터의 특성
    - 중복되거나 모호한 레코드
    - 고립된 레코드
    - 불완전하거나 누락된 레코드
    - 텍스트 인코딩 오류
    - 일치하지 않는 형식(ex-대시(-)가 있는 전화 번호와 없는 전화번호)
    - 레이블이 잘못되었거나 레이블이 지정되지 않은 데이터
  - 데이터 생태계에 주요 특성과 접근 방식 
    - 최악을 가정하고 최상을 기대하라 
    - 가장 적합한 시스템에서 데이터를 정리하고 검증하라
    - 자주 검증하라 
- 클라우드 데이터 웨어하우스 및 데이터 레이크
  - 데이터 웨어하우스 
    - 사용자가 원하는 질문에 대답할 수 있는 데이터 분석 활동을 지원하기 위해 서로 다른 시스템의 데이터가 모델링되어 저장되는 데이터베이스
    - 데이터 웨어하우스의 데이터는 리포팅 및 분석 쿼리를 위해 정형화되고 최적화됨
  - 데이터 레이크
    - 데이터가 저장되지만 데이터 웨어하우스처럼 데이터 구조나 쿼리 최적화가 필요 없는 곳임
- 데이터 수집 도구
  - Singer
  - Stitch
  - Fivetran
- 데이터 변환 및 모델링 도구
  - 파이프라인은 머신러닝, 분석 및 리포팅과 같은 새로운 목적을 위해 데이터를 변환하고 모델링하는 작업으로 구성됨
  - 데이터 변환
    - 데이터 변환은 ETL 또는 ELT 프로세스 T(Transform)에 해당하는 광범위한 용어
  - 데이터 모델링
    - 데이터 모델링은 보다 구체적인 데이터 변환 유형, 데이터 모델은 데이터 분석을 위해 데이터를 이해하고 최적화된 형식으로 정형화하고 정의함
    - 데이터 모델은 일반적으로 데이터 웨어하우스에서 하나 이상의 테이블로 표시됨 
- 워크플로 오케스트레이션 플랫폼
  - 조직의 데이터파이프라인의 복잡성과 수가 증가함에 따라 데이터 인프라에 워크플로 오케스트레이션 플랫폼을 도입하는 것이 중요함
  - 파이프라인에서의 작업의 스케줄링 및 흐름을 관리해줌
  - 워크플로 관리 시스템(WMS), 오케스트레이션 플랫폼 또는 오케스트레이션 프레임워크
  - 아파치 에어플로우, Luigi, AWS Glue와 같은플랫폼은 좀 더 일반적인 용도로 설계되어 다양한 데이터 파이프라인에 사용됨
  - 구체적인 사용 사례와 플랫폼(Kubeflow Pipeline의 경우 Docker 컨테이너에 구축된 머신러닝 워크플로)을 위해 설계됨 
- 방향성 비순환 그래프
  - 거의 모든 최신 오케스트레이션 프레임워크는 파이프라인에서 작업의 흐름과 종속성을 그래프로 나타냄
  - 몇 가지 특정 제약 조건
    - 파이프라인 단계는 항상 방향성을 가짐, 하나의 작업 또는 여러 개의 작업으로 시작하고 특정 작업으로 끝남, 모든 종속 작업이 완료되어야만 그 다음 작업이 실행됨
    - 비순환(acyclic) 그래프, 작업이 이전에 완료된 작업을 다음 작업으로 가리킬 수 없다
# 일반적인 데이터 파이프라인 패턴
- ETL과 ELT
  - 추출(extract) : 로드 및 변환을 준비하기 위해 다양한 소스에서 데이터를 수집함 
  - 로드(load) : 원본 데이터(ELT의 경우) 또는 완전히 변환된 데이터(ETL의 경우)를 최종 대상으로 가져옴
  - 변환(transform) : 분석가, 시각화 도구 또는 파이프라인이 제공하는 모든 사용 사례에 유용하게 쓸 수 있게 각 소스 시스템의 원본 데이터를 결합하고 형식을 지정하는 단계 
- ETL을 넘어선 ELT의 등장
  - 이전에는 데이터 팀이 방대한 양의 원본 데이터를 로드하고 이를 사용 가능한 데이터로 변환하는 데 필요한 스토리지나 컴퓨팅 자원이 모두 모여있는 데이터 웨어하우스에 액세스할 수 없었음
  - 당시 데이터 웨어하우스는 트랜잭션 사용 사례에서 잘 작동하는 행 기반 데이터베이스였으나 분석에서 흔히 볼 수 있는 대용량 쿼리에는 적합하지 않음
  - 대부분의 데이터 웨어하우스는 비용 효율적인 방식으로 대규모 데이터세트에 대한 대량변환을 저장하고 실행할 수 있는 확장성이 뛰어난 열 기반 데이터베이스를 기반으로 함
  - 열 기반 데이터베이스의 I/O 효율성, 데이터 압축, 데이터 처리를 위한 여려 병렬 노드에 데이터 및 쿼리를 분산하는 기능 덕분에 상황이 바뀜 
  - Snowflake 또는 Amazon Redshift와 같은 열 기반 데이터베이스는 행이 아닌 열 단위로 디스크 블록에 데이터를 저장함
  - 분석가의 쿼리에 필요한 필터링 및 합산을 수행하기 위해 메모리에 로드할 데이터와 디스크 I/O가 줄어들음
  - 최종 이점은 데이터가 아니라 도일한 데이터 유형이 저장되므로 블록을 남김없이 활용하고 최적으로 압축할 수 있기 때문 
  - 열 기반 데이터베이스의 출현은 데이터 웨어하우스 내에서 대규모 데이터세트를 저장,변환 및 쿼리하는 것이 효율적 
- EtLT 하위 패턴
  - 추출 후 로드하기 전에 간다히 변환하는 것이 여전히 유익함
  - 소문자t 변환 또는EtLT
    - 테이블에서 레코드 중복 제거
    - URL 파라미터를 개별 구성요소로 구문 분석
    - 민감한 데이터 마스킹 또는 난독화 
  - 비즈니스 로직과 완전히 분리되거나 민감한 데이터를 마스킹하는 것과 같이 법적 또는 보안상의 이유로 파이프라인 초기에 필요한 경우가 있음
- 데이터 분석을 위한 ELT
  - ELT의 등장으로 데이터 분석가는 데이터 엔지니어에 의해 차단되지 않고 데이터로부터 가치를 제공할 수 있는 자율성과 권한을 갖게 되었음, 데이터 엔지니어는 분석가가 SQL로 작성된 자체 변환 코드를 작성하고 배포할 수 있도록 지원하는 인프라와 데이터 수집에 집중할 수 있음 
- 데이터 제품 및 머신러닝을 위한 ELT
  - 데이터는 분석, 보고 및 예측 모델 이상의 용도로 사용됨
  - 데이터 제품의 몇 가지 일반적인 예
    - 비디오 스트리밍 홈 화면을 구동하는 콘텐츠 추천 엔진
    - 전자상거래 웹사이트의 개인화된 검색 엔진
    - 사용자가 생성한 레스토랑 리뷰에 대한 감성 분석을 수행하는 애플리케이션 
 - 데이터 수집
    - 수집하는 데이터가 머신러닝 모델이 나중에 학습 또는 검증을 위한 특정 데이터세트로 참조할 수 있도록 버전 지정이 되었는지 확인 
 - 데이터 전처리
  - 데이터를 정리하고 모델에 사용할 준비를 하는 단계
 - 모델 교육
  - 새 데이터를 수집하고 전처리한 후 머신러닝 모델을 다시 학습해야 함 
 - 모델 배포
  - 모델을 운영 환경에 배포하는 것 
  - 데이터세트의 버전 관리뿐만 아니라 학습된 모델의 버전 관리도 필요함 
- 파이프라인에 피드백 통합
  - 좋은 머신러닝 파이프라인에는 모델 개선을 위한 피드백 수집도 포함됨 

# 데이터 수집 : 데이터 추출
- 클라우드 파일 스토리지 설정
  - S3 버킷과 상호 작용하기 위해 Boto3 파이썬 라이브러리를 사용하기 때문에 IAM 사용자를 생성하고 해당 사용자에 대한 액세스 키를 생성하고 파이썬 스크립트에서 사용할 수 있는 구성 파일에 키를 저장해야 함 
- MySQL 데이터베이스에서 데이터 추출
  - SQL을 사용한 전체 또는 증분 추출
  - 이진 로그(binlog) 복제 (스트리밍 데이터 수집을 수행하는 하나의 경로)
  - SQL을 사용한 전체 또는 증분 추출은 구현하기가 훨씬 간단하지만 자주 변경되는 대규모 데이터세트에서는 확장성이 떨어짐
  - 전체 추출과 증분 추출 사이에도 트레이드 오프가 있음
  - 이진 로그 복제는 구현이 더 복잡하지만 원본 테이블의 변경되는 데이터 볼륨이 크거나 MySQL 소스에서 데이터를 더 자주 수집해야 하는 경우에 더 적합함 
- 전체 또는 증분 MySQL 테이블 추출
  - 증분 추출이 최적의 성능에 이상적이지만 어떤 테이블에 대해서는 가능하지 않을 수 있는 몇가지 단점과 이유가 있음
    - 삭제된 행은 캡쳐되지 않음
    - 원본 테이블에는 마지막으로 업데이트된 시간에 대한 신뢰할 수 있는 타임스탬프가 있어야 함 
- MySQL 데이터의 이진 로그 복제
  - 이진 로그는 CDC(변경 데이터 캡처)의 한 형태, 대부분의 원본 데이터 저장소에는 사용할 수 있는 CDC 형식이 있음
  - MySQL 이진 로그는 데이터베이스에서 수행된 모든 작업에 대한 기록을 보관하는 로그
    - 구성된 방식에 따라 모든 테이블의 생성 또는 수정 사항뿐만 아니라 모든 INSERT,UPDATE 및 DELETE 작업도 기록됨
    - 이진 로그의 원래 목적은 다른 MySQL 인스턴스로 데이터를 복제하기 위한 것이지만, 이진 로그의 내용은 데이터 웨어하우스로 데이터를 수집하려는 데이터 엔지니어에게 매우 매력적임 
```sql
SELECT variable_value as bin_log_status
FROM performance_schema.global_variables
WHERE variable_name='log_bin';
```
  - 이진 로깅 형식
    - STATEMENT
    - ROW
    - MIXED
