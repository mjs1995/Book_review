# 서평
- 데이터 분석이 잘 이뤄지기 위해서는 분석에 필요한 형태로 잘 정리된 데이터가 필요하고, 원하는 분석 결과를 얻기 위해서는 적합한 기간의 정확한 데이터가 필요함, 이러한 데이터들은 잘 구성된 데이터 파이프라인 안에서만 만들어질수 있음 
- 이 책은 데이터 파이프라인의 전반적인 소개와 패턴을 다룸, 데이터 파이프라인 각 단계를 계획할 때부터 구성 후 검증하고 유지 관리하는 전체 과정에서 고려할 점들과 활용할 수 있는 예시 코드를 함께 제공함 

# 데이터 파이프라인 소개
- 데이터 파이프라인이란?
  - 다양한 소스에서 새로운 가치를 얻을 수 있는 대상으로 데이터를 옮기고 변환하는 일련의 과정, 분석,리포팅,머신러닝 능력의 기초가됨
  - 데이터 파이프라인의 복잡성은 원본 데이터의 크기와 상태, 구조 및 분석 프로젝트의 요구사항에 따라서도 달라짐
  - 데이터 추출, 데이터 가공, 데이터 유효성 검사를 포함한 여러 단계로 구성되며, 때로는 데이터를 최종 목적지로 전달하기 전에 머신러닝 모델을 학습하거나 실행하는 단계가 있기도 함
  - 파이프라인에는 여러 시스템과 프로그래밍 언어의 작업이 포함되는 경우가 많음
  - 데이터 팀은 일반적으로 종속성을 공유하고 조정해야 하는 수많은 데이터 파이프라인을 소유하고 유지함 
- 누가 파이프라인을 구축할까?
  - 데이터 엔지니어는 분석 상태계를 뒷받침하는 데이터 파이프라인을 구축하고 유지하는 데 전문적인 역량을 갖추고잇음 
  - 데이터 과학자 및 분석가와 긴밀히 협력하여 데이터를 어떻게 처리해야 하는지 파악하고 요구사항을 확장 가능한 프로덕션 상태로 전환하는 데 도움을 줌 
    - SQL과 데이터 웨어하우징 기초
      - 숙련된 데이터 엔지니어는 고성능의 SQL 작성 방법을 알고 데이터웨어하우징 및 데이터 모델링의 기본 사항을 이해함
    - 파이썬 그리고/또는 자바
    - 분산 컴퓨팅
      - 데이터 양이 많아지고 데이터를 신속하게 처리하고 하는 요구사항이 늘어나면서 데이터엔지니어들은 분산 컴퓨팅 플랫폼을 사용하기 시작, 
      - 분산 컴퓨팅은 여러 시스템의 성능을 결합하여 대량의 데이터를 효율적으로 저장, 처리 및 분석함
        - 하둡 분산 파일 시스템(HDFS)를 통한 분산 파일 스토리지, 맵리듀스를 통한 처리, 피그(pig)를 통한 데이터 분석 등을 포함하는 하둡 에코시스템, 아파치 스파크는 하둡을 빠르게 능가하는 또 다른 인기 분산 처리 프레임워크 
      - 데이터 엔지니어는 이러한 프레임워크를 언제 어떻게 활용해야 하는지 알아야 함 
    - 기본 시스템 관리
      - 리눅스 명령줄에 능숙, 응용 프로그램 로그 분석, 크론 작업 예약, 방화벽 및 기타 보안 설정의 문제 해결과 같은 작업을 수행할 수 있어야함
    - 목표 지향적 사고방식
      - 데이터 엔지니어가 파이프라인을 구축하는 이유를 알 때 더 나은 아키텍처 결정을 내릴 수 있음
- 왜 파이프라인을 구축할 까?
  - 원본 데이터는 정리, 정형화, 정규화, 결합, 집계, 그리고 때로는 마스킹 또는 보안을 위해 정제됨 
- 어떻게 데이터 파이프라인을 구축할까?
  - 오픈소스도 있고, 상업용도 있고, 자체 개발 제품도 있음
  - 파이프라인 구축을 위한 가장 인기 있는 솔루션 및 프레임워크를 살펴보고 조직의 요구 사항과 제약 조건에 따라 어떤 제품을 사용할지 결정하는 방법을 알아봄
  - 파이프라인을 구축하고 이를 안정적이고 안전하게 제시간에 제공하고 처리하는 인프라를 지원해야함 

# 최신 인프라 데이터
- 데이터 소스의 다양성
  - 최신 데이터 인프라의 핵심 구성 요소
    - 데이터 소스의 다양성
    - 데이터 수집 도구
    - 클라우드 데이터 웨어하우스와 데이터 레이크
    - 모델링 도구 및 프레임워크
    - 워크플로 오케스트레이션 플랫폼 
- 소스 시스템 소유권
  - 데이터 수집(data ingestion) : 한 소스에서 데이터를 추출하여 다른 소스로 로드하는 것
  - 소스 시스템이 위치하는 곳이 어디인지를 이해하는 것은 여러 가지로 중요함
    - 타사 데이터 소스에 위치한 데이터에 액세스하려고 한다면 액세스 방법에 제한이 있을수 있음
    - 데이터를 사용자가 필요로 하는 형태에 맞추어 정의하는 등
    - 데이터 수집이 시스템에 의도하지 않는 부하를 가하는지부터 데이터를 점진적(incremental)으로 로드할 수 있는지 여부까지 다양한 과제가 발생하기 때문
- 수집 인터페이스 및 데이터 구조
  - 소스 데이터를 얻는 방법과 형식
    - Postgres 또는 MySQL 데이터베이스와 같은 애플리케이션 뒤에 있는 데이터베이스
    - REST API와 같은 시스템 상단의 추상화 계층
    - Apache Kafka와 같은 스트림 처리 플랫폼
    - 로그, 쉼표로 구분된 값(csv) 파일 및 기타 플랫 파일을 포함하는 공유 네트워크 파일 시스템(NFS) 또는 클라우드 스토리지 버킷
    - 데이터 웨어하우스 또는 데이터 레이크
    - HDFS 또는 HBase 데이터베이스의 데이터 
  - 데이터 구조
    - REST API의 JSON
    - MySQL 데이터베이스의 잘 구성된 데이터
    - MySQL 데이터베이스의 테이블의 열 내의 JSON
    - 반정형화된 로그 데이터
    - CSV,고정 폭 형식(FWF) 및 기타 플랫 파일 형식
    - 플랫 파일의 JSON
    - Kafka의 스트림 출력
  - 분석 프로젝트에 더 적합한 형태로 정형화하기 위해서는 데이터 수집 이 외에도 클렌징, 변환 작업 등의 추가 단계가 파이프라인에 필요할 수 있음
  - JSON과 같은 반정형 데이터가 점점 보편화 되고 있으며 속성-값 구조와 객체의 중첩(nesting)구조의 이점을 가지고 있음
  - 관계형 데이터베이스와 달리 같은 데이터세트 안의 데이터 구조가 모두 동일하다는 보장은 없음, 파이프라인에서 누락되거나 불완전한 데이터를 처리하는 방법은 상황에 따라 달라지며 데이터의 유연성이 증가할수록 점점 더 많이 필요함 
- 데이터 클레징 작업과 유형성 검사
  - 지저분한 데이터의 특성
    - 중복되거나 모호한 레코드
    - 고립된 레코드
    - 불완전하거나 누락된 레코드
    - 텍스트 인코딩 오류
    - 일치하지 않는 형식(ex-대시(-)가 있는 전화 번호와 없는 전화번호)
    - 레이블이 잘못되었거나 레이블이 지정되지 않은 데이터
  - 데이터 생태계에 주요 특성과 접근 방식 
    - 최악을 가정하고 최상을 기대하라 
    - 가장 적합한 시스템에서 데이터를 정리하고 검증하라
    - 자주 검증하라 
- 클라우드 데이터 웨어하우스 및 데이터 레이크
  - 데이터 웨어하우스 
    - 사용자가 원하는 질문에 대답할 수 있는 데이터 분석 활동을 지원하기 위해 서로 다른 시스템의 데이터가 모델링되어 저장되는 데이터베이스
    - 데이터 웨어하우스의 데이터는 리포팅 및 분석 쿼리를 위해 정형화되고 최적화됨
  - 데이터 레이크
    - 데이터가 저장되지만 데이터 웨어하우스처럼 데이터 구조나 쿼리 최적화가 필요 없는 곳임
- 데이터 수집 도구
  - Singer
  - Stitch
  - Fivetran
- 데이터 변환 및 모델링 도구
  - 파이프라인은 머신러닝, 분석 및 리포팅과 같은 새로운 목적을 위해 데이터를 변환하고 모델링하는 작업으로 구성됨
  - 데이터 변환
    - 데이터 변환은 ETL 또는 ELT 프로세스 T(Transform)에 해당하는 광범위한 용어
  - 데이터 모델링
    - 데이터 모델링은 보다 구체적인 데이터 변환 유형, 데이터 모델은 데이터 분석을 위해 데이터를 이해하고 최적화된 형식으로 정형화하고 정의함
    - 데이터 모델은 일반적으로 데이터 웨어하우스에서 하나 이상의 테이블로 표시됨 
- 워크플로 오케스트레이션 플랫폼
  - 조직의 데이터파이프라인의 복잡성과 수가 증가함에 따라 데이터 인프라에 워크플로 오케스트레이션 플랫폼을 도입하는 것이 중요함
  - 파이프라인에서의 작업의 스케줄링 및 흐름을 관리해줌
  - 워크플로 관리 시스템(WMS), 오케스트레이션 플랫폼 또는 오케스트레이션 프레임워크
  - 아파치 에어플로우, Luigi, AWS Glue와 같은플랫폼은 좀 더 일반적인 용도로 설계되어 다양한 데이터 파이프라인에 사용됨
  - 구체적인 사용 사례와 플랫폼(Kubeflow Pipeline의 경우 Docker 컨테이너에 구축된 머신러닝 워크플로)을 위해 설계됨 
- 방향성 비순환 그래프
  - 거의 모든 최신 오케스트레이션 프레임워크는 파이프라인에서 작업의 흐름과 종속성을 그래프로 나타냄
  - 몇 가지 특정 제약 조건
    - 파이프라인 단계는 항상 방향성을 가짐, 하나의 작업 또는 여러 개의 작업으로 시작하고 특정 작업으로 끝남, 모든 종속 작업이 완료되어야만 그 다음 작업이 실행됨
    - 비순환(acyclic) 그래프, 작업이 이전에 완료된 작업을 다음 작업으로 가리킬 수 없다
# 일반적인 데이터 파이프라인 패턴
- ETL과 ELT
  - 추출(extract) : 로드 및 변환을 준비하기 위해 다양한 소스에서 데이터를 수집함 
  - 로드(load) : 원본 데이터(ELT의 경우) 또는 완전히 변환된 데이터(ETL의 경우)를 최종 대상으로 가져옴
  - 변환(transform) : 분석가, 시각화 도구 또는 파이프라인이 제공하는 모든 사용 사례에 유용하게 쓸 수 있게 각 소스 시스템의 원본 데이터를 결합하고 형식을 지정하는 단계 
- ETL을 넘어선 ELT의 등장
  - 이전에는 데이터 팀이 방대한 양의 원본 데이터를 로드하고 이를 사용 가능한 데이터로 변환하는 데 필요한 스토리지나 컴퓨팅 자원이 모두 모여있는 데이터 웨어하우스에 액세스할 수 없었음
  - 당시 데이터 웨어하우스는 트랜잭션 사용 사례에서 잘 작동하는 행 기반 데이터베이스였으나 분석에서 흔히 볼 수 있는 대용량 쿼리에는 적합하지 않음
  - 대부분의 데이터 웨어하우스는 비용 효율적인 방식으로 대규모 데이터세트에 대한 대량변환을 저장하고 실행할 수 있는 확장성이 뛰어난 열 기반 데이터베이스를 기반으로 함
  - 열 기반 데이터베이스의 I/O 효율성, 데이터 압축, 데이터 처리를 위한 여려 병렬 노드에 데이터 및 쿼리를 분산하는 기능 덕분에 상황이 바뀜 
  - Snowflake 또는 Amazon Redshift와 같은 열 기반 데이터베이스는 행이 아닌 열 단위로 디스크 블록에 데이터를 저장함
  - 분석가의 쿼리에 필요한 필터링 및 합산을 수행하기 위해 메모리에 로드할 데이터와 디스크 I/O가 줄어들음
  - 최종 이점은 데이터가 아니라 도일한 데이터 유형이 저장되므로 블록을 남김없이 활용하고 최적으로 압축할 수 있기 때문 
  - 열 기반 데이터베이스의 출현은 데이터 웨어하우스 내에서 대규모 데이터세트를 저장,변환 및 쿼리하는 것이 효율적 
- EtLT 하위 패턴
  - 추출 후 로드하기 전에 간다히 변환하는 것이 여전히 유익함
  - 소문자t 변환 또는EtLT
    - 테이블에서 레코드 중복 제거
    - URL 파라미터를 개별 구성요소로 구문 분석
    - 민감한 데이터 마스킹 또는 난독화 
  - 비즈니스 로직과 완전히 분리되거나 민감한 데이터를 마스킹하는 것과 같이 법적 또는 보안상의 이유로 파이프라인 초기에 필요한 경우가 있음
- 데이터 분석을 위한 ELT
  - ELT의 등장으로 데이터 분석가는 데이터 엔지니어에 의해 차단되지 않고 데이터로부터 가치를 제공할 수 있는 자율성과 권한을 갖게 되었음, 데이터 엔지니어는 분석가가 SQL로 작성된 자체 변환 코드를 작성하고 배포할 수 있도록 지원하는 인프라와 데이터 수집에 집중할 수 있음 
- 데이터 제품 및 머신러닝을 위한 ELT
  - 데이터는 분석, 보고 및 예측 모델 이상의 용도로 사용됨
  - 데이터 제품의 몇 가지 일반적인 예
    - 비디오 스트리밍 홈 화면을 구동하는 콘텐츠 추천 엔진
    - 전자상거래 웹사이트의 개인화된 검색 엔진
    - 사용자가 생성한 레스토랑 리뷰에 대한 감성 분석을 수행하는 애플리케이션 
 - 데이터 수집
    - 수집하는 데이터가 머신러닝 모델이 나중에 학습 또는 검증을 위한 특정 데이터세트로 참조할 수 있도록 버전 지정이 되었는지 확인 
 - 데이터 전처리
  - 데이터를 정리하고 모델에 사용할 준비를 하는 단계
 - 모델 교육
  - 새 데이터를 수집하고 전처리한 후 머신러닝 모델을 다시 학습해야 함 
 - 모델 배포
  - 모델을 운영 환경에 배포하는 것 
  - 데이터세트의 버전 관리뿐만 아니라 학습된 모델의 버전 관리도 필요함 
- 파이프라인에 피드백 통합
  - 좋은 머신러닝 파이프라인에는 모델 개선을 위한 피드백 수집도 포함됨 

# 데이터 수집 : 데이터 추출
- 클라우드 파일 스토리지 설정
  - S3 버킷과 상호 작용하기 위해 Boto3 파이썬 라이브러리를 사용하기 때문에 IAM 사용자를 생성하고 해당 사용자에 대한 액세스 키를 생성하고 파이썬 스크립트에서 사용할 수 있는 구성 파일에 키를 저장해야 함 
- MySQL 데이터베이스에서 데이터 추출
  - SQL을 사용한 전체 또는 증분 추출
  - 이진 로그(binlog) 복제 (스트리밍 데이터 수집을 수행하는 하나의 경로)
  - SQL을 사용한 전체 또는 증분 추출은 구현하기가 훨씬 간단하지만 자주 변경되는 대규모 데이터세트에서는 확장성이 떨어짐
  - 전체 추출과 증분 추출 사이에도 트레이드 오프가 있음
  - 이진 로그 복제는 구현이 더 복잡하지만 원본 테이블의 변경되는 데이터 볼륨이 크거나 MySQL 소스에서 데이터를 더 자주 수집해야 하는 경우에 더 적합함 
- 전체 또는 증분 MySQL 테이블 추출
  - 증분 추출이 최적의 성능에 이상적이지만 어떤 테이블에 대해서는 가능하지 않을 수 있는 몇가지 단점과 이유가 있음
    - 삭제된 행은 캡쳐되지 않음
    - 원본 테이블에는 마지막으로 업데이트된 시간에 대한 신뢰할 수 있는 타임스탬프가 있어야 함 
- MySQL 데이터의 이진 로그 복제
  - 이진 로그는 CDC(변경 데이터 캡처)의 한 형태, 대부분의 원본 데이터 저장소에는 사용할 수 있는 CDC 형식이 있음
  - MySQL 이진 로그는 데이터베이스에서 수행된 모든 작업에 대한 기록을 보관하는 로그
    - 구성된 방식에 따라 모든 테이블의 생성 또는 수정 사항뿐만 아니라 모든 INSERT,UPDATE 및 DELETE 작업도 기록됨
    - 이진 로그의 원래 목적은 다른 MySQL 인스턴스로 데이터를 복제하기 위한 것이지만, 이진 로그의 내용은 데이터 웨어하우스로 데이터를 수집하려는 데이터 엔지니어에게 매우 매력적임 
```sql
SELECT variable_value as bin_log_status
FROM performance_schema.global_variables
WHERE variable_name='log_bin';
```
  - 이진 로깅 형식
    - STATEMENT
      - 이진 로그에 행을 삽입하거나 수정하는 행동들에 대해 SQL 문 자체를 기록함, 하나의 MySQL 데이터베이스에서 다른 MySQL 데이터베이스로 데이터를 복제하려는 경우 이 형식이 유용함
    - ROW
      - 테이블의 행에 대한 모든 변경 사항이 SQL문이 아니라 행 자체의 데이터로 이진 로그 행에 표시됨, 이것이 주로 사용하는 기본 형식
    - MIXED
      - 이진 로그에 STATE 형식 레코드와 ROW 형식 레코드를 모두 기록함, 나중에 ROW 데이터만 걸러낼 수도 있지만, 이진 로그를 다른 용도로 사용하지 않는 한 결국 디스크 공간을 추가로 사용하게 되기 때문에 MIXED를 활성화할 필요는 없음 
```sql
SELECT variable_value as bin_log_format
FROM performance_schema.global_variables
WHERE variable_name = 'binlog_format
```
  - 이진 로그에서 가져올 ROW 형식 이벤트의 3가지 유형
    - WRITE_ROWS_EVENT
    - UPDATE_ROWS_EVENT
    - DELETE_ROWS_EVENT
- 카프카 및 Debezium을 통한 스트리밍 데이터 수집
  - MySQL 이진 로그 또는 Postgres WALs와 같은 CDC 시스템을 통해 데이터를 수집할 경우, 훌륭한 프레임워크의 도움이 꼭 필요함
  - Debezium은 여러 오픈소스 서비스로 구성된 분산 시스템으로 일반적인 CDC 시스템에서 행수준 변경을 캡처한 후 다른 시스템에서 사용할 수 있는 이벤트로 스트리밍해주는 시스템
    - 아파치 주키퍼 : 분산 환경을 관리하고 각 서비스의 구성을 처리함
    - 아파치 카프카 : 확장성이 뛰어난 데이터 파이프라인을 구축하는 데 일반적으로 사용되는 분산 스트리밍 플랫폼
    - 아파치 카프카 커넥트 : 데이터를 카프카를 통해 쉽게 스트리밍할 수 있도록 카프카를 다른 시스템과 연결하는 도구, 커넥터는 MySQL 및 Postgres와 같은 시스템용으로 구축되었으며CDC 시스템(이진 로그 및 WAL)의 데이터를 카프카 토픽으로 변환함
  - 카프카는 토픽별로 정리된 메시지를 교환함, 하나의 시스템은 토픽에 게시(publish)할 수 있는 반면, 하나 이상의 시스템은 토픽을 소비(consume)하거나 구독(subscribe)할 수 있음 
  - Debezium 커넥터
    - MongoDB
    - MySQL
    - PostgreSQL
    - Microsoft SQL Server
    - Oracle
    - Db2
    - Cassandra

# 데이터 수집: 데이터 로드
- Redshift 웨어하우스에 데이터 로드
  - S3에서 Redshift로 데이터를 로드하는 가장 효율적인 방법은 COPY 명령을 사용하는 것
  - Copy는 Redshift 클러스터를 쿼리하는 데 사용하는 SQL 클라이언트나 Boto3 라이브러리를 사용하는 파이썬 스크립트에서 SQL문으로 실행할 수 있음 
  - S3 버킷과 상호 작용하기 위해 boto3, Redshift 클러스터에서 COPY 명령을 실행하기 위해 psycopg2, 그리고 pipeline.conf 파일을 읽기 위해 configparser 라이브러리를 가져옴
```python
import boto3
import configparser
import psycopg2
```
- 증분 및 전체 로드
  - COPY 명령은 추출된 CSV 파일의 데이터를 Redshift 클러스터의 테이블에 직접 로드함
  - 만약 CSV 파일의 데이터가 불변(immutable) 소스의 증분 추출에서 온 것이라면(불변 이벤트데이터 또는 기타 삽입 전용 데이터세트와 같은 경우) 더 이상 할 일이 없을 것
  - CSV 파일의 데이터에 업데이트된 레코드나 소스 테이블의 삽입 또는 전체 내용이 포함되어 있으면 해야 할 일이 조금 더 있거나 최소한 고려해야할 사항이 있음 
  - COPY 명령(TRUNCATE 말고!)을 사용하여 데이터를 로드하고 레코드가 마지막으로 업데이트된 시간을 나타내는 타임스탬프에 의존하여 나중에 어떤 레코드가 최신인지 식별하거나 과거 레코드를 다시 볼 수 있음 
- CDC 로그에서 추출한 데이터 로드
  - 증분 추출된 데이터를 로드하는 프로세스와 유사하지만, 이 경우 삽입 및 업데이트된 레코드뿐만 아니라 삭제된 레크드에도 액세스 할 수 있음
  - 데이터 파이프라인에서 데이터 수집의 목표는 소스에서 데이터를 효율적으로 추출하여 대상으로 로드하는 것 
  - 특히 사용 사례에 대한 데이터를 모델링하는 로직은 파이프라인의 변환 단계에서 구현함 
- Snowflake 데이터 웨어하우스에 데이터 로드
  - Snowflake에 데이터를 로드하는 메커니즘은 COPY INTO 명령
  - COPY INTO는 하나의 파일 또는 여러 파일의 내용을 Snowflake 웨어하우스의 테이블에 로드함
  - Snowpipe라는 데이터 통합 서비스를 제공함
    - Snowflake 단계에서 사용할 수 있게 되는 즉시 파일에서 데이터를 로드할 수 있음
    - COPY INTO 명령을 통해 대량 로드를 예약하는 대신 Snowpipe를 사용하여 데이터를 지속해서 로드할 수 있음 
- 파일 스토리지를 데이터 레이크로 사용
  - S3 버킷(또는 다른 클라우드 스토리지) 에서 데이터를 추출하고 데이터 웨어하우스에 로드하지 않을 때도 있음
  - 정형화 또는 반정형화된 형태로 저장된 데이터를 데이터 레이크라고 함 
  - 데이터 레이크는 다양한 형식의 데이터를 원본 형식 또는 때에 따라 비정형 형식으로 저장함, 데이터 레이크는 데이터를 저장하기에는 더 저렴하지만 웨어하우스의 정형화된 데이터와 같은 방식으로 쿼리하는 데 최적화되어 있지 않음
  - Amazon Athena : 사용자가 SQL을 사용하여 S3에 저장된 데이터를 쿼리할 수 있는 AWS 서비스 
  - Amazon Redshift Spectrum : Redshift가 S3의 데이터에 외부 테이블(external table)로 액세스하고 Redshift 웨어하우스의 테이블과 함께 쿼리에서 이를 참조할 수 있도록 하는 서비스 
  - 클라우드 스토리지 기반 데이터 레이크에 대용량 데이터를 저장하는 것이 웨어하우스에 저장하는 것보다 비용이 저렴함, 비정형 또는 반정형 데이터는 미리 정의된 스키마가 없기 때문에 저장된 데이터의 유형이나 속성을 변경하는 것이 웨어하우스 스키마를 수정하는 것보다 훨씬 쉬움, JSON 문서는 데이터 레이크에서 접할 수 있는 반정형 데이터 유형의 예, 데이터 구조가 자주 변경되는 경우 적어도 일정 시간 동안 데이터 레이크에 저장하는 것을 고려 
  - 원본 형태로 데이터 레이크에 대한 액세스 권한을 부여함으로써 데이터를 탐색하고 사용해야 하는 데이터의 속성을결정할 수 있음, 데이터를 웨어하우스의 테이블에 로드하는 것이 합리적인지 여부를 결정할 수 있고 그렇게 함으로써 쿼리를 최적화할 수 있음
  - 데이터 인프라에 데이터 레이크와 데이터 웨어하우스를 모두 가지고 있음, 시간이 지남에 따라 이 둘은 경쟁이 아닌 보완적인 솔루션이 됨 
- 오픈 소스 프레임워크
  - Singer
    - 파이썬으로 작성된 Singer는 탭(tap)을 사용하여 소스에서 데이터를 추출하고 JSON 형태로 대상으로 스트리밍함
    - Singer를 사용하면 데이터 수집을 예약하고 조정하기 위해 별도의 오케스트레이션 프레임워크를 사용해야 함 
- 상업적 대안
  - 단 한줄의 코드도 작성하지 않고 많은 공통 데이터 수집을 가능하게 하는 상용 클라우드 호스팅 제품이 여럿 있음
    - 가장 인기 있는 두 가지 상용 도구는 Stitch와 Fivetran
      - 완전한 웹 기반이며 데이터 엔지니어뿐만아니라 데이터 팀의 다른 데이터 전문가도 접근할 수 있음
      - 세일즈포스, 허브스팟, 구글 애너리틱스, 깃허브 등과 같은 공통 데이터 소스에 대한 수백 개의 사전 구축된 커넥터를 제공함 
- 비용
  - Stitch와 FiveTran은 모두 볼륨 기반 가격 모델을 가지고 있음
  - 볼륨을 측정하는 방법과 각 가격 책정 계층에 포함하는 기타 기능이 다르지만 결국 지불하는 금액은 수집하는 데이터의 양에 따라 결정됨
  - 수집할 데이터 원본이 대량인 경우 비용이 많이 듬
- 공급업체(Vendor) 종속
  - 공급업체에 투자하면 향후 도구나 제품으로 마이그레이션해야 하는 경우 엄청난 양의 작업에 직면하게 됨
- 커스터마이징에 필요한 코딩
  - Stitch의 경우 사용자 지정 Singer 탭을 작성해야함
  - Fivetran의 경우 AWS Lambda, Azure Function 또는 Google Cloud Functions를 사용하여 클라우드 기능을 작성해야 함 
  - 사용자 지정 REST API와 같은 사용자 지정 데이터 소스가 많은 경우 사용자 지정 코드를 작성한 다음 이를 실행하기 위해 Stitch 또는 Fivetran 비용을 내야함
- 보안 및 개인 정보 보호
  - 데이터에 대한 스트리밍 역할을 하고 장기간 저장하지 않지만 기술적으로 여전히 소스 시스템과 대상(일반적으로 데이터 웨어하우스 또는 데이터 레이크)에 모두 액세스할 수 있음
  - FiverTran과 Stitch는 모두 높은 보안 기준을 충족함
  - 위험 허용 범위, 규정 요구 사항, 잠재적인 책임, 새 데이터 프로세서를 검토하고 승인하는 오버헤드로 인해 이를 활용하기 꺼릴 수 있음 

# 데이터 변환하기
- 데이터 변환에는 비문맥적 데이터 조작과 비즈니스 컨텍스트 및 논리를 염두에 둔 데이터 모델링이 모두 포함됨
- 파이프라인의 목적이 비즈니스 통찰력 또는 분석을 생성하는 것이라면 비문맥적 변환 외에도 데이터가 데이터 모델로 추가 변환됨, 데이터 모델을 데이터 분석을 위해 이해되고 최적화된 형식으로 데이터를 정형화하고 정의한다는 것
- 모든 데이터 파이프라인에 공통적인 비문맥적 변환과 대시보드, 보고서 및 비즈니스 문제의 일회성 분석을 지원하는 데이터 모델을 모두 살펴봄 
- 비문맥적 변환
  - EtLT 하위 패턴
    - 테이블의 중복 레코드 제거
      - 데이터 웨어하우스에 수집된 데이터 테이블에 중복 레코드가 존재할 수 있음
        - 증분 데이터 수집에서 실수로 이전 수집 시간 창과 겹치거나 이전 실행에서 이미 수집된 일부 레코드를 선택한 경우
        - 원본 시스템에서 중복 레코드가 실수로 생성된 경우
        - 나중에 채워진(backfilled) 데이터가 테이블에 로드된 후속 데이터와 겹치는 경우 
      - 최소한 하나의 중복이 존재한다는 것을 알았으므로 중복 레코드를 제거할 수 있음, 선택하는 방법은 데이터베이스 최적화 및 SQL 구문의 기본 설정과 관련된 많은 요소에 따라 다름, 둘 다 시도하고 런타임을 비교하는 것이 좋음 
        - 쿼리 시퀀스를 사용, DISTINCT 문을 사용하여 원본에서 테이블의 복사본을 만듬 
        - 윈도우 기능(window function)을 사용하여 중복 행을 그룹화하고 행 번호를 할당하여 삭제할 행과 유지할 행을 식별하는 것, ROW_NUMBER 함수를 사용하여 레코드의 순위를 지정하고 PARTITION BY 문을 사용하여 열별로 레코드를 그룹화함 
    - URL 매개변수를 개별 구성요소로 구문 분석
- URL 파싱
  - URL의 세그먼트를 분석하는 것은 비지니스 컨텍스트가 거의 또는 전혀 관련되지 않은 작업
  - UTM 매개변수
    - UTM(Urchin Tracking Module) 매개변수는 마케팅 및 광고 캠페인을 추적하는 데 사용되는 URL 매개변수이며 대부분의 플랫폼과 조직에서 공통적임 
- 언제 변환할 것인가, 수집 중 혹은 수집 후?
  - 변환은 SQL 이외의 언어를 사용하여 수행하는 것이 가장 쉬움
  - 변환은 데이터 품질 문제를 해결함, 가능한 한 파이프라인 초기에 데이터 품질을 해결하는 것이 가장 좋음 
- 데이터 모델링 기초
  - 주요 데이터 모델링 용어
    - 측정 : 측정하고 싶은 것, 예에는 고객 수와 수익의 달러 가치가 포함된
    - 속성 : 보고서 또는 대시보드에서 필터링하거나 그룹화하려는 항목, 예에는 날짜,고객 이름 및 국가가 포함됨
  - 데이터 모델의 세분성(granularity), 세분성은 데이터 모델에 저장된 세부 정보 수준을 말함 
- 완전히 새로 고침 된 데이터 모델링
  - 팩트(fact)와 차원(dimension)
    - 차원 모델링(Kimball 모델링)
  - 세분성 최적화
    - 데이터 모델의 레코드 수는 모델에 사용된 소스 테이블의 데이터 양, 속성 수 및 세분성의 요인, 항상 필요한 가장 작은 단위의 세분성으로 모델링해야 하며 그 이하로는 모델링해서도 안됨
- 완전히 새로 고침 된 데이터 차원을 천천히 변경
  - 완전히 새로 고침 된 데이터를 사용하면 각 수집 사이에 Customers 테이블의 전체 기록을 유지하고 이러한 변경 사항을 스스로 추적해야 함 
  - Kimball(차원) 모델링에 정의되어 있으며 천천히 변화하는 차원 또는 SCD(slowly changing dimension)
- 증분 수집된 데이터 모델링
  - CTE(Common Table Expression) : 공통 테이블 표현 
- 추가 전용(Append-only) 데이터 모델링
  - 추가 전용 데이터(또는 삽입 전용 데이터)는 데이터 웨어하우스로 수집되는 변경할 수 없는 데이터
  - 테이블의 각 레코드는 변경되지 않는 일종의 이벤트 
- 변경 캡처 데이터 모델링
  - CDC 덕분에 모든 주문의 현재 상태뿐만 아니라 전체 내역도 알 수 있음
  - 저장된 데이터를 모델링하는 방법은 데이터 모델이 답해야 하는 질문에 따라 다름
  - CDC에서 수집한 데이터의 또 다른 일반적인 용도는 변경 자체를 이해하는 것임 
  - CDC를 통해 수집된 데이터가 완전히 새로 로드되지 않고 증분 로드되며 잠재적인 성능 향상을 얻을 수 있음 
  - 성능 향상이 증분 모델로 인한 복잡성 추가를 감수할 가치가 없는 경우가 있음, 이는 CDC 데이터로 작업하는 경우 사실일 때가 많음, 업데이트와 삭제를 모두 처리하는 추가적인 복잡성으로 인해 완전히 새로 로드하는 것을 오히려 더 선호하기도 함 

# 파이프라인 오케스트레이션
- 방향성 비순환 그래프
  - 파이프라인 단계(작업)는 항상 지시(directed)에 따름, 작업 또는 여러 작업으로 시작하여 특정 작업으로 끝남, 실행 경로를 보장하기 위해 필요함, 모든 종속 작업이 성공적으로 완료되기 전에 작업이 실행되지 않게 함 
  - 파이프라인 그래프는 또한 순환적이어야 함, 작업은 이전에 완료된 작업을 가리킬 수 없음, 순환할 수 없다는 것, 가능하다면 파이프라인이 끝없이 실행될 수 있음 
- 아파치 에어플로우 설정 및 개요
  - 에어비앤비(Airbnb)에서 맥심 부시민이 시작한 오픈 소스 프로젝트,사용하기 쉬운 웹 인터페이스, 고급 명령줄 유틸리티, 내장된 스케줄러 및 높은 수준의 사용자 정의 기능은 거의 모든 데이터 인프라에 적합함, 파이썬으로 빌드되었지만 모든 언어 또는 플랫폼에서 실행되는 작업을 실행할 수 있음 
- 에어플로우 데이터베이스
  - 에어플로우는 데이터베이스를 사용하여 각 작업 및 DAG의 실행 기록 및 에어플로우 구성과 관련된 모든 메타데이터를 저장함 
- 스케줄러
- 실행기(Executor)
  - 실행기는 에어플로우가 스케줄러가 실행할 준비가 되었다고 판단한 작업을 실행하는 데 사용함
  - SequentialExecutor는 한 번에 하나의 작업만 실행할 수 있으므로 프로덕션 사용 사례에는 적합하지 않음
  - CeleryExecutor, DaskExecutor 또는 KubernetesExecutor와 같은 다른 실행기를 사용하는 것이 좋음 
- 연산자(Operators)
  - DAG에서 각 노드는 하나의 작업, 에어플로우에서 각 작업은 연산자를 구현함, 연산자는 실제로 스크립트, 명령 및 기타 작업을 실행함
- 파이썬 코드 실행 옵션
  - PythonOperator를 사용하는 대신 BashOperator를 사용하여 파이썬 스크립트를 실행함, PythonOperator로 파이썬 코드를 실행하려면 코드를 DAG 정의 파일에 작성하거나 DAG 정의 파일로 가져와야 함, 오케스트레이션과 이 오케스트레이션이 실행하는 프로세스의 로직을 더 많이 분리하고 싶었음, 에어플로우와 내가 실행하려는 코드 간에 호환되지 않는 파이썬 라이브러리 버전의 잠재적인 문제를 피할 수 있음, 프로젝트(및 Git 저장소)를 분리하여 데이터 인프라 전반에 걸쳐 로직을 유지 관리하는 것이 더 쉬움 
- 추가 파이프라인 작업
  - ELT 파이프라인의 기능 작업 외에도 프로덕션 품질 파이프라인에는 파이프라인이 완료되거나 실패할 때 슬랙 채널에 알림 보내기, 파이프라인의 다양한 지점에서 데이터 유효성 검사 실행 등의 작업이 요구 될 수 있음
  - 경고 및 알림
    - DAG가 실패하거나 성공한 경우에는 이메일이 받는 것이 더 나은 경우가 많음, 알림을 보내는 데는 여러 가지 옵션 존재
    - airflow.cfg의 [smtp] 섹션에 SMTP 서버에 대한 세부정보를 제공해야함 
  - DAG를 분할해야 하는 경우
    - DAG에서 함께 속하는 작업을 결정하는 것 
    - 데이터 인프라에서 모든 추출,로드,변환, 유효성 검사 및 경고 작업이 포함된 DAG를 만드는 것이 가능하지만 매우 복잡하기도 함 
    - 작업을 여러 DAG로 분할해야 하는 시점과 단일 DAG에 유지해야 하는 시점을 결정하는 세가지 요소
      - 작업을 다른 일정으로 실행해야 하는 경우 여러 DAGS로 나눔
      - 파이프라인이 진정으로 독립적인 경우 별도로 유지함
      - DAG가 너무 복잡해지면 논리적으로 분리할 수 있는지 여부를 결정함 
- 센서로 여러 DAG 조정
  - DAG 간의 공유 종속성에 대한 필요성을 감안할 때 에어플로우 작업은 Sensor라고 하는 특별한 유형의 연산자를 구현할 수 있다, 에어플로우 Sensor는 일부 외부 작업 또는 프로세스의 상태를 확인한 다음 확인 기준이 충족되면 DAG에서 다운스트림 종속성을 계속 실행하도록 설계됨 
  - 두 개의 서로 다른 에어플로우 DAG를 조정해야 하는 경우 ExternalTaskSensor를 사용하여 다른 DAG의 작업 상태 또는 다른 DAG의 전체 상태를 확인할 수 있음 
- 관리형 에어플로우 옵션
  - 완전 관리형으로 제공되는 몇 가지 솔루션이 있음
    - 구글 클라우드의 Cloud Composer와 Astronomer
    - 자체 서버에서 에어플로우를 실행하는 것보다 훨씬 더 많은 월별 요금이 발생하지만 에어플로우의 관리 편리성이 높아짐 
  - 관리형 솔루션을 선택하는 것은 특정 상황에 따라 달라짐
    - 자체 호스팅을 도와줄 수 있는 시스템 운영 팀이 있는가?
    - 관리형 서비스에 지출할 예산이 있는가?
    - 얼마나 많은 DAG와 작업이 파이프라인에 있는가? 더 복잡한 에어플로우 실행기가 필요한 만큼 충분히 높은 규모로 실행하고 있는가?
    - 보안 및 개인 정보 보호 요구 사항은 무엇인가? 외부 서비스가 내부 데이터 및 시스템에 연결하는 것을 허용하는가?
- 기타 오케스트레이션 프레임워크
  - Luigi 및 Dagster와 같은 다른 훌륭한 오케스트레이션 프레임워크가 있음, 머신러닝 파이프라인 오케스트레이션에 맞춰진 Kubeflow Pipelines도 ML 커뮤니티에서 인기가 있음
  - 데이터 모델에 대한 변환 단계의 오케스트레이션과 관련하여 피시타운 애널리틱스(Fishtown Analytics)의 dbt는 탁월한 옵션
# 파이프라인의 데이터 검증
- 아무리 잘 설계된 데이터 파이프라인이라도 반드시 문제가 발생하기 마련, 프로세스, 오케스트레이션 및 인프라를 잘 설계하면 많은 문제를 방지하거나 최소한 완화할 수 있음
- 데이터 자체의 품질과 유효성을 보장하기 위해서는 데이터 검즈에 투자해야 함 
- 일찍 그리고 자주 검증할 것
  - 데이터 분석가(일반적으로 변환 로직을 소유하고 있는 사람)가 데이터를 이해하고 품질 문제가 있는지 확인하는 데 가장 적합하다는 생각으로 작업함
  - 데이터 엔지니어는 한 시스템에서 다른 시스템으로 데이터를 이동하고 파이프라인을 조정하며 데이터 인프라를 유지관리하는 데 초점을 맞춤 
  - 파이프라인 끝에서 데이터 품질 문제를 찾아 처음부터 다시 추적해야 하는 것은 최악의 시나리오
- 소스 시스템 데이터 품질
  - 일반적인 데이터 웨어하우스에 수집되는 소스 시스템의 수를 고려할 때 데이터 수집 중에 잘못된 데이터가 데이터 웨어하우스로 유입될 가능성이 높음
  - 잘못된 데이터는 소스 시스템의 작동 자체에는 영향을 미치지 않을 수 있음
  - 레코드의 연결이 끊어져도 소스 시스템이 정상적으로 작동할 수 있음
  - 아직 발견되지 않았거나 수정되지 않은 버그가 소스 시스템에 실제로 있을 수 있음 
- 데이터 수집 위험
  - 수집 중 추출 또는 로드 단계에서 시스템 중단 또는 시간 초과
  - 증분 수집의 논리적 오류
  - 추출된 파일의 구문 분석(parsing) 문제
- 데이터 분석가 검증 활성화
  - 어떤 도구를 선택하든 엔지니어와 분석가가 최대한 마찰을 줄이면서 검증 테스트를 안정적으로 작성하고 실행할 수 있게 하는 것이 중요함 
- 간단한 검증 프레임워크
  - 유효성 검사기 프레임워크 코드
    - 한 스크립트는 주어진 날짜 동안의 테이블의 행 수를 세고, 두 번째 스크립트는 전날의 행 수를 세며, >= 확인의 비교 연산자는 현재 날짜가 이전 날짜보다 더 많은 행을 가지고 있는지 확인할 수 있음 
  - 수집 후 중복된 레코드
  - 수집 후의 예기치 않은 행 개수
- 머신러닝 파이프라인을 구축하고 텐서플로를 사용하는 경우 텐서플로 데이터 검증(TensorFlow Data Validation)을 고려할 수 있음, 일반적인 검증을 위해서는 Yahoo의 Validator라는 오픈 소스 옵션을 사용할 수 있음

# 파이프라인 유지 관리 모범 사례
- 소스 시스템의 변경 사항 처리
  - 데이터를 수집하는 시스템이 정적이지 않다는 것, 개발자들은 기능을 추가하거나 코드를 리팩터링 하거나 버그를 수정하여 항상 소프트웨어를 변경함
  - 수집할 데이터의 스키마 또는 의미가 수정되면 파이프라인이 실패하거나 부정확해질 위험에 처하게 됨 
- 추상화 도입
  - 가능하면 소스 시스템과 수집 프로세스 사이에 추상화 계층을 도입하는 것이 가장 좋음, 소스 시스템의 소유자가 추상화 방법을 유지하거나 인식하는 것이 중요함
  - 카프카 토픽을 통해 데이터를 게시하는 것은 합의된 스키마를 유지하면서 이벤트를 게시하는 소스 시스템과 이벤트를 구독하는 시스템(수집)의 세부사항을 서로 완전히 분리된 상태로 유지하는 훌륭한 방법
- 데이터 계약 유지 관리 
  - 데이터 계약
    - 소스 시스템의 소유자와 데이터 파이프라인에서 사용하기 위해 해당 시스템에서 데이터를 수집하는 팀 간의 서면 계약, 계약에는 데이터 추출이 어떤 방법(전체, 증분)으로 얼마나 자주 이루어지며, 소스 시스템과 수집 모두에 대한 연락처가 누구(사람, 팀)인지 명시해야 함
    - 데이터 계약은 깃허브 저장소 또는 내부 문서 사이트와 같이 잘 알려져 있고 찾기 쉬운 위치에 저장해야 함, 가능하다면 데이터 계약을 표준화 형태로 지정하여 개발 프로세스에 통합하거나 프로그래밍 방식으로 쿼리할 수 있게함 
    - 사용 법
      - PR이 제출되거나 코드가 브랜치에 커밋될 때 데이터 계약에서 source_table로 나열된 테이블에 대한 변경사항(스키마 또는 로직)을 찾는 Git 후크(hook)를 빌드한다, 테이블이 데이터 수집에 사용되는 테이블의 변경 사항을 조정하기 위해 누구에게 연락해야 하는지를(ingestion_owner) 기고자에게 자동으로 알린다
      - 데이터 계약 자체가 Git 리포지토리에 있는 경우 Git 후크를 추가하여 계약 변경 사항을 확인한다, 가령 수집 실행 빈도가 증가하는 경우 Git 후크를 추가하여 계약 변경 사항을 확인함, 가령 수집 실행 빈도가 증가하는 경우 데이터 계약을 업데이트해야 할 뿐만 아니라 소스 시스템 소유자에게 문의하여 운영 시스템에 부정적인 영향이 없는지를 확인해야 함 
      - 회사의 중앙 문서 사이트에 모든 데이터 계약을 읽을 수 있는 형태로 게시하고 검색 가능하게 만듬
      - 지난 6개월(또는 기타 빈도) 동안 업데이트되지 않은 데이터 계약에 대해 소스 시스템 및 수집 소유자에게 알리고 필요한 경우 검토 및 업데이트하도록 요청하는 스크립트를 작성하고 예약함 
- Schema-on-Read의 고려사항
  - 소스 데이터의 스키마 변경을 처리하는 한 가지 방법은 데이터를 쓸 때 스키마를 정의하는 방식(Schema-on-write)에서 데이터를 읽을 때 스키마를 정의하는 방식(schema-on-read)으로 설계를 이동하는 것
  - Schema-on-write
    - 소스에서 데이터를 추출할 때 구조(스키마)가 정의되고 데이터가 데이터 레이크 또는 S3 버킷에 기록됨, 그런 다음 수집의 로드 단계가 실행되면 데이터가 예측 가능한 형식의 정의된 테이블 구조로 로드될 수 있다
  - Schema-on-read
    - 스키마에 대한 엄격한 정의 없이 데이터가 데이터 레이크, S3 버킷 또는 기타 스토리지 시스템에 기록되는 패턴, 데이터의 스키마는 읽을 때까지 알 수 없으므로 schema-on-read라고 함 
    - 이 패턴은 스토리지에 데이터를 쓰는 데는 매우 효율적이지만 로드 단계에 복잡성을 추가하고 파이프라인에서 몇 가지 주요 의미를 갖음, 기술적인 관점에서 S3 버킷에서 이러한 방식으로 저장된 데이터를 읽는 것은 매우 쉬움 
    - 로드 단계에서 스키마가 유연한 데이터를 읽는 데 사용하는 모든 도구와 통합되는 데이터 카탈로그를 활용함, 데이터 카탈로그는 데이터 레이크 및 웨어하우스의 데이터에 대한 메타데이터를 저장함, 데이터세트의 구조와 정의를 모두 저장할 수 있음, AWS Glue 데이터 카탈로그 및 Apache Atlas는 널리 사용되는 데이터 카탈로그  
    - 로드 단계의 논리는 더욱 복잡해짐, 스키마 변경을 동적으로 처리하는 방법을 고려해야함 
    - 데이터 카탈로그 작성 뿐만 아니라 조직에서 데이터가 사용되는 방식에 대한 표준 및 프로세스를 정의하는 데이터 거버넌스에 대해 진지하게 고민해야함 
