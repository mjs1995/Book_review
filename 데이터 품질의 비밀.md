# 서평
- 이 책은 표면적으로는 데이터를 정제하고 이해하는 방법에 관한 메뉴얼처럼 보일 수 있음. 그러나 여기서 더 나아가, 보다 안정적인 데이터 시스템을 구축하고 그 과정에 조직 및 이해관계자와 데이터 신뢰를 구축하는 모범 사례 ,기술 및 프로세스에 대해서도 설명함
- 필자는 지금까지 수행한 수많은 인터뷰에서 얻은 지식과 모범 사례를 바탕으로 이 책을 집필했음. 이 책을 통해, 데이터 수집부터 분석에 이르는 파이프라인의 각 단계에서 데이터 다운타임을 방지하기 위해 데이터 품질을 유지,관리하는 방법을 제시하고자 함.

# 지금, 데이터 품질에 주목해야 하는 이유 
- 데이터 다운타임은 데이터가 수집되지 않아 누락되거나 부정확하게 측정되는 등의 데이터 손실로 인해 소프트웨어 또는 서비스의 가동이 중지되는 상황을 의미함
  - 데이터 다운타임이란 데이터가 누락됐거나, 부정확하거나,데이터에 오류가 있어 생기는 문제를 말함
- (상대적으로 예측 가능하지만) 중요한 테이블의 5,000개 행이 맥락도 없이 갑자기 500개 행으로 바뀌는 상황
- 대시보드가 손상되어 실행 대시보드에 NULL 값이 출력되는 상황
- 숨겨진 스키마 변경으로 인해 다운스트림 파이프라인이 중단되는 상황
- 이 책은 신뢰할 수 없는 데이터로 고통받고, 내적 비명을 지르며 이 상황을 개선하기 위해 무언가를 하고 싶어 하는 모든 사람들을 위해 출간되었음. 이들은 데이터 엔지니어, 데이터 분석 또는 데이터 과학 분야 출신으로서, 회사의 데이터 파이프라인을 구축, 확장 및 관리하는 데 적극적으로 참여하는 사람들일 것
- 데이터 품질에 관한 다양한 이슈를 확인할 수 있음
  - 구체적으로는 데이터베이스 스키마(컴퓨터 과학에서 데이터베이스 스키마는 데이터베이스에서 자료의 구조, 자료의 표현 방법, 자료 간의 관계를 형식 언어로 정의한 구조) 변경으로 인한 데이터 파이프라인 중단이나 주요 행 또는 열 중복 현상, 대시보드 내 오류값 발생(대시보드는 자동으로 업데이트되는데, 대시보드에 값을 노출하기 위해 필요한 데이터가 수집되지 못한 경우 혹은 포맷에 맞지 않는 데이터가 입력된 경우 등을 일컫음) 등 다양한 문제를 찾아냄
- 프로덕션 데이터는 소스 시스템의 데이터. 프로덕션 데이터는 고객관리 시스템이나 콘텐츠 관련 프로그램과 같은 데이터 소스로부터 수집되어 데이터 레이크 등의 저장 공간에 쌓인 데이터를 말함 
- 다운타임은 가동 중지 시간(소프트웨어 또는 서비스가 사용 가능, 작동, 사용 불가능, 다운되는 빈도)을 의미하며, 그 외 정상적으로 수행되는 시간을 업타임이라고 함 
- 데이터 품질이란?
  - 측정하지 않으면 관리할 수 없고, 관리할 수 없으면 개선할 수도 없다.라는 말처럼 고품질 데이터는 강력한 분석 프로그램을 만드는 필요조건이 된다.
- 데이터 품질의 현재
  - 시스템 개발 라이프사이클을 단축하는 데브옵스를 통해 업계를 선도하는 모범사례를 만들었음. 해당 사례에는 사이트 신뢰성 엔지니어링, 지속적인 통합, 지속적인 배포 및 마이크로서비스에 기반한 아키텍처 등이 포함됨. 요컨대 데브옵스의 목표는 개발과 운영의 원활한 소통과 효율화로 보다 안정적이고 탁월한 성능의 소프트웨어를 출시하는 것
  - 데이터옵스는 데이터 관리의 자동화를 통해 데이터의 안정성과 성능을 개선하고, 데이터 사일로(각 조직 단위 또는 목적별로 IT 인프라르 도입 및 구축해 사용함으로써 부서, 사업, 솔루션별로 데이터가 고립되어 전사 관점의 의사 결정을 막는 현상을 말함)를 줄이며 데이터 분석 속도를 높이고 오류를 감소시키는 프로세스를 말함 
  - 데이터 다운타임 증가
    - 데이터 분석의 확산과 교차 기능 조직(교차 기능 조직은 서로 다른 부서에서 공통의 목표를 위해 구성된 조직을 의미함. 현업 부서에 데이터 분석가나 데이터 과학자가 포함되거나 데이터 조직 내에 엔지니어와 현업을 이해하고 있는 구성원이 참여하는 사례가 확대되고 있음)의 등장으로 클라우드 서비스가 선호되기 시작했음. 클라우드를 활용한 아마존 레드시프트나 스노우플레이크, 구글 빅쿼리와 같은 솔루션을 통해 사용자가 더 쉽고 빠르게 데이터를 처리할 수 있게 되면서 클라우드는 더 큰 인기를 끌게 되었음 
    - 데이터 파이프라인의 복잡성 증가
      - 데이터 소스의 증가, 이종 데이터 간 결합, 경영진의 데이터 사용 증가 등으로 인해 처리과정이 늘어나고 데이터 간 사소하지 않은 종속성(서비스가 점점 더 고도화되면서 특정 행위에서 생성된 데이터가 다른 데이터에도 영향을 주는 종속관계가 넓어지고 깊어짐)이 커지는 등 데이터 파이프라인은 점점 더 복잡해지고 있음
    - 데이터 조직의 전문성 강화 
      - 데이터 분석가는 회사 내 담당자들이 풍부하고 실현 가능한 비즈니스 인사이트를 얻을 수 있도록 데이터셋을 수집,정제하며 모델링과 분석을 담당함. 
      - 데이터 엔지니어는 요구하는 성능을 빠르고 안정적으로 충족할 수 있도록 분석 기술과 시스템을 구축하고 관리함
      - 데이터 과학자는 분석 및 엔지니어링과 더불어 수학 및 통계학, 컴퓨팅 기법을 활용하여 보다 고도화된 능력을 발휘해야 하는 역할을 담당함 
      - 실제로 대기업에선 데이터 거버넌스 리더, 데이터 스튜어드(현업 부서에서 비즈니스 관점으로 데이터의 생성부터 활용까지 프로세스를 총괄하는 역할을 의미함), 운영 분석가와 같은 역할도 생겨나고 있음. 대규모로 데이터 조직을 꾸릴 자원이 부족할 경우 분석-엔지니어와 같은 하이브리드 직무가 활용되기도 함 
    - 분산된 데이터 조직
      - 분산된 데이터 아키텍처란 무엇일까? 중앙의 플랫폼 조직에서는 데이터를 관리하고, 비즈니스 전반의 데이터 분석 기능이나 데이터 과학자들은 분산시키는 구조를 말함
      - 분산 도메인 지향 데이터 아키텍처인 데이터 메시와는 다른 개념
  - 데이터 산업 동향 
    - 데이터 메시
      - 데이터 메시는 마이크로서비스 아키텍처의 데이터 플랫폼 버전으로 볼 수 있음 
      - 데이터 메시는 일종의 사회 기술적 시스템 패러다임(기술 영역의 사회 체계를 일컫음. 복잡해지는 인간 세상에서 정치,사회 체계가 고도화된 것처럼 한 번에 파악하기 어려울 정도로 수많은 서비스를 수행하는 기업의 기술 기반 사회 체계의 발달을 의미함)으로, 사용자가 복잡해지는 아키텍처 및 솔루션과 상호작용할 수 있도록 함
      - 데이터 메시는 도메인 지향 분산형 아키텍처로 셀프 서비스 설계를 활용하여 도메인별로 데이터가 편재성(도메인별로 데이터가 퍼져 있는 현상을 말함)을 갖도록 함. 여기에는 에릭 에반스의 도메인 주도 설계 이론을 활용함
        - 해당 이론은 유연하고 확장성 높은 소프트웨어 개발 접근법을 포함하여, 코드의 구조와 사용된 언어와 비즈니스 도메인은 상응함 
      - 단일 중앙 집중형 데이터 레이크에서 소비,저장,변환,출력을 처리하는 기존 모놀리식 데이터 인프라와 달리, 데이터 메시는 자체 파이프라인을 통해 프로덕트형 데이터(data-as-a-product) 관점에서 분산된 도메인별로 데이터 소비자가 활용할 수 있도록 처리하고 지원함 
      - 범용적 상호운용성 레이어를 통해 데이터에 동일한 표준을 적용하고, 개별 도메인과 데이터가 연결될 수 있도록 함 
      - 데이터 메시는 데이터를 프로덕트로 제공할 책임이 있는 도메인 이해관계자들이 데이터 소유권을 공유할 수 있게 하는 동시에, 도메인 내외의 분산된 데이터 간의 커뮤니케이션을 원활하게 함
      - 데이터 인프라는 각 도메인에 데이터 처리에 필요한 솔루션을 제공하는 역할을 함. 반면 도메인은 데이터 수집,정제,집계를 관리하며, 해당 데이터는 비즈니스 인텔리전스 애플리케이션에 사용될 수 있는 자산이 됨. 
      - 각 도메인은 각각의 파이프라인을 소유할 책임이 있는데, 원시 데이터를 저장하고 카탈로그화하며 유지 관리하고 접근 권한을 관리하는 것은 모든 도메인이 보유한 일련의 기능. 
      - 특정 도메인에 데이터가 제공되어 데이터가 변환되면 도메인 소유자는 분석이든, 운영이든 필요에 따라 데이터를 활용할 수 있음 
      - 자마크 데가니가 정리한 데이터 메시는 신뢰할 수 있는 고품질 데이터와 범용적인 거버넌스에 의존한 도메인 지향 분산 데이터 아키텍처로 이루어져 있음 
      - 데이터 메시 패러다임은 데이터를 신뢰할 수 있으며, 범용적 상호운용성이 도메인 간에 적용될 때에만 성공적일 수 있음. 데이터를 신뢰할 수 있는 유일한 방법은 테스트하고 모니터링하는 것 
    - 데이터 레이크하우스의 등장
      - 구조화된 데이터 저장소인 데이터 웨어하우스와 보다 자유도가 높은 데이터 레이크 모두 고품질 데이터가 필요함. 점점 더 많은 데이터 조직이 증가하는 비즈니스 요구 사항을 소화하기 위해 데이터 웨어하우스와 데이터 레이크를 둘 다 사용하고 있음. 이것은 데이터 레이크하우스가 부각되는 이유 
      - 데이터 레이크하우스라는 개념은 클라우드 플랫폼 분야에서 아마존의 레드시프트 스펙트럼이나 데이터브릭스의 레이크하우스와 같은 기능이 추가되면서 처음 등장 했음. 당시 데이터 레이크에도 SQL 질의 및 스키마와 같은 데이터 웨어하우스의 기능이 추가되는 추세였음. 사실상 오늘날에는 웨어하우스와 레이크의 기능적 차이가 줄어들고 있으며, 두 형태의 장점을 결합한 데이터 레이크하우스가 확산되고 있음 
      - 레이크하우스로 마이그레이션한다는 것은 데이터 파이프라인이 점점 더 복잡해지고 있음을 시사함

# 신뢰할 수 있는 데이터 시스템 구축을 위한 블록 조립
- 운영 데이터와 분석 데이터의 차이
  - 운영 데이터
    - 운영상 생성된 데이터, 즉 조직에서 일상적인 운영을 통해 생성된 데이터. 특정 시점의 인벤토리 스냅샷, 고객 인상 및 거래 기록은 모두 운영 데이터의 예
  - 분석 데이터
    - 분석적으로 사용되는 데이터, 즉 데이터 기반 의사 결정에 활용되는 데이터를 말함. 마케팅 전환율, 클릭률, 글로벌 지역별 광고 노출 등이 분석 데이터의 예
    - 운영 데이터는 시스템 및 프로세스의 신속한 업데이트르 위해 실제 비즈니슨 프로세스의 데이터를 기록하는 반면 분석 데이터는 좀 더 강력하고 효율적인 분석을 하는 데 사용됨
    - 운영 데이터로 비즈니스를 운영하고 분석 데이터로 비즈니스를 관리함. 분석 데이터가 운영 데이터와 다른 방식으로 비즈니스 인텔리전스를 주도한다는 점을 고려할 때, 이 데이터가 조직의 성공에 더 중요하거나 중심적이라고 생각할 수 있음. 
    - 분석 데이터는 변환, 집계된 운영 데이터에 의존하기 때문에 항상 그런 것만은 아님 
- 차이는 어떻게 만들어 지는가?
  - 운영 데이터는 데이터 파이프라인의 분석 데이터에서 거의 대부분 업스트림으로 나타남. 이는 분석 데이터가 운영 데이터 스토어의 집계 또는 확장을 포함하는 경우가 많기 때문
  - 운영 데이터와 분석 데이터의 차이가 중요한 이유 중 하나는 처리량과 대기 시간의 균형 때문임. 처리량-대기 시간 제약은 고정된 처리 능력을 가진 모든 시스템에 영향을 미침. 일반적으로 처리량은 일정 단위 시간 내에 처리되는 데이터의 양을 의미하며, 대기 시간은 데이터 처리가 끝날 때까지의 지연을 의미함 
- 데이터 웨어하우스 vs 데이터 레이크
  - 데이터 웨어하우스는 데이터를 구조화된(행-열) 형식으로 저장함. 해당 데이터는 고도로 변환(정의된 전처리 절차의 결과)되며 이처럼 변환된 데이터는 적어도 이론적으로 확실한 존재 이유가 있음 
  - 데이터 웨어하우스: 스키마 수준의 테이블 타입
    - 데이터 웨어하우스에는 쓰기 스키마 액세스 권한이 필요함. 데이터 웨어하우에 데이터가 들어오는 즉시 데이터의 구조를 설정한다는 의미. 데이터의 추가 변환에 따라 모든 단계에서 명시적으로 새로운 구조를 만들어야 함 
    - 데이터 웨어하우스는 완벽하게 통합되고 관리되는 솔루션이므로, 즉시 구축하고 운영할 수 있음. 데이터 웨어하우스는 데이터 레이크와 달리 일반적으로 더 많은 구조와 스키마가 필요하므로 데이터 위생(data hygiene, 데이터가 정확하고 일관되며, 오류가 없는지 확인하는 데 사용되는 관행이나 프로세스(누락 값 채우기, 중복 레코드 식별 및 제거 등)를 말함)이 개선됨. 데이터를 읽고 사용할 때 복잡성이 줄어듬 
    - 현대의 데이터 웨어하우스는 쓰기 스키마와 아키텍처, 루커 및 태블로와 같은 비즈니스 인텔리전스 도구를 통합하여 이 방법론을 실현함. 데이터 웨어하우스의 데이터에는 존재 이유가 있고, 그 이유는 비즈니스 목표와 일치해야 함 
      - 아마존 레드시프트
        - 최초의 클라우드 데이터 웨어하우스이자 그만큼 자주 사용되는 아마존 레드시프트는 AWS를 기반으로 하며, 소스 커넥터를 활용해 원시 데이터 소스의 데이터를 관계형 스토리지로 파이프라인을 통해 전달함.
        - 레드시프트의 열 기반 저장 구조와 병렬 처리는 분석 작업의 부하를 줄여줌
      - 구글 빅쿼리
        - 레드시프트와 마찬가지로, 자사의 독점 클라우드 플랫폼인 GCP를 활용하고 열 기반 저장 구조를 사용하며, 빠른 쿼리를 위해 병렬 처리를 활용함
        - 빅쿼리는 레드시프트와 비교했을 때 사용 패턴에 따라 확장되는 서버리스(serverless) 솔루션으로 더 잘 사용할 수 있음 
      - 스노우플레이크
        - 운영을 위해 클라우드에 의존하는 레드시프트 또는 GCP와 달리, 스노우플레이크의 클라우드 데이터 웨어하우징 기능은 AWS, 구글, 애저 및 기타 퍼블릭 클라우드의 인프라를 기반으로 함 
        - 스노우플레이크에서는 사용자가 컴퓨팅 및 스토리지 비용을 별도로 지불할 수 있으므로 좀 더 유연한 비용 구조를 원하는 팀에게 데이터 웨어하우스를 제공할 수 있음
    - 데이터 품질 관리와 관련해 염두에 두어야 할 몇가지 약점
      - 제한된 유연성
        - 데이터 웨어하우스는 가장 유연한 데이터 스토리지 솔루션은 아님. 상대적으로 확장성이 낮다는 말이 아님
        - 웨어하우스의 형식으로 제한되어 있음을 말함. 데이터 웨어하우스에 대한 항목은 명확한 스키마가 있는 표 형식으로 강제 지정되어야 함. 
        - JSON과 같은 반구조화된 데이터와 그 쿼리는 대개 적합하지 못하고, 나쁜 데이터의 경우 강제 변환되지 않으면 쉽게 소실되기도 함 
      - SQL 전용 지원
        - 데이터 웨어하우스에 요청할 때는 SQL과 같은 쿼리 언어를 사용해야 함. 파이썬과 같은 명령형 언어는 강력한 라이브러리 생태계를 보유하므로, 머신러닝에 유용한 데이터 조작 처리를 지원하지 않음 
        - 머신러닝을 구현할 때 추가 프로세싱을 하려면 SQL을 활용해 데이터를 데이터 웨어하우스 밖으로 이동시켜야 함. 데이터 이동은 종종 데이터를 망가뜨리고 데이터의 볼륨과 신선도에 부정적인 영향을 미치며 스키마 이상(anomaly) 현상의 원인이 되기도 함
      - 워크플로에서의 마찰(Frictional workflows)
        - 프로덕트의 이터레이션(개발이 진행되는 기간을 말함.이때 일부 기능을 배포함으로써 고객 및 이해관계자들은 프로덕트르 빠르게 사용하고 피드백을 줄 수 있음. 기간은 프로젝트마다 다른데, 일반적으로 1주일에서 4주일 사이)이 반복됨에 따라 긴말하게 협업하는 소규모의 데이터 과학자 팀은 쓰기 스키마 시스템이 제공하는 명확함이 유익하기보다는 번거롭다고 느낄 것
        - 신속하게 작업하고자 할 때는 데이터 구조에 대한 기준이 느슨해지는 것이 좋음. 하지만 이러한 구조는 지속적으로 변화할 것이고, 데이터 웨어하우스는 지속적인 스키마 변경을 달가워하지 않음 
  - 데이터 레이크: 파일 수준의 조작
    - 데이터 웨어하우스와 달리, 데이터 레이크 아키텍처를 사용하면 읽기 스키마에 접근할 수 있음. 즉, 데이터를 사용할 준비가 되었을 때 데이터 구조를 추론함
    - 데이터 레이크는 데이터 웨어하우스의 DIY 버전으로, 팀 시스템 요구 사항에 따라 다양한 메타데이터, 저장소, 컴퓨팅 기술을 선택할 수 있음.
    - 데이터 레이크는 다수의 데이터 엔지니어가 운영하는 맞춤형 플랫폼을 구축하려는 데이터 조직에 이상적
    - 구축기의 데이터 레이크는 주로 아파치 하둡 맵리듀스와 HDFS를 기반으로 구축, SQL 엔진으로 데이터를 쿼리하기 위해 아파티 하이브를 활용. 아파치 스파크는 데이터 레이크를 훨씬 더 쉽게 유지할 수 있도록 만들었고, 데이터 레이크의 대규모 데이터셋에 걸쳐 분산된 계싼을 위한 일반화된 프레임워크를 제공했음 
    - 데이터 레이크의 일반적인 특징
      - 분산된 저장소 및 컴퓨팅 : 비용 절감에 상당히 도움. 실시간 스트리밍 및 쿼리 데이터를 풍부하게 만들며, 해당 데이터 구문 분석을 용이하게 함 
      - 분산 컴퓨팅 지원 
      - 사용자 지정 및 상호운용성
      - 오픈 소스 기술 기반 구축 : 공급업체에 대한 의존도를 줄이고 훌륭한 맞춤형 기능을 제공
      - 비정형 또는 거의 정형화되지 않은 데이터 처리 능력 
      - 정교한 비SQL 프로그래밍 모델 지원
      - 데이터 무결성 
      - 늪지화 : 시간이 지남에 따라 데이터 레이크가 기술적 부채와 암묵적 지식을 발생시키는 경향
      - 엔드포인트 증가
    - 데이터 레이크하우스의 특성
      - 우수한 성능의 SQL : 프레스토 및 스파크 
      - 스키마 : 파케와 같은 형식은 데이터 레이크 테이블에 더 엄격한 스키마를 도입 
      - 원자성, 일관성, 격리성, 내구성 : 델타 레이크와 아파치 후디와 같은 레이크 기술은 쓰기/읽기 트랜잭션의 신뢰성 높임 
      - 관리 서비스 : 데이터 브릭스나 아마존 아테나의 완전 관리형 레이크 
    - 데이터 웨어하우스와 데이터 레이크 간 동기화 
      - 서로 다른 데이터 웨어하우스와 데이터 레이크는 데이터 통합 레이어로 연결
        - AWS 글루, 파이브트랜, 마틸리온과 같은 데이터 통합 툴은 서로 다른 소스에서 데이터를 수집하여 이 데이터를 통합한 후 업스트림 소스로 변환 
        - 데이터 통합의 전형적인 사용 사례는 레이크 속 데이터를 수집하여 구조화된 형식으로 데이터 웨어하우스에 적재하는 것 
- 데이터 품질 지표 수집 
  - 데이터 품질 지표
    - 데이터가 중단되었는지 여부를 평가할 때 체크 내용
      - 데이터가 최신 상태인가?
      - 데이터가 완전한가?
      - 필드가 예상 범위 내에 있는가?
      - NULL 비율이 예상보다 높은 것은 아닌가?
      - 스키마가 변경되었는가?
  - 데이터 품질 지표를 가져오는 방법
    - 확장성
      - 많은 수의 테이블과 대규모의 데이터셋을 추적하는 작업은 까다로울 수 있음. 요청을 한 번에 처리하과, 규모에 맞게 쿼리를 최적화하여, 중복을 제거하고, 다양한 방법으로 스키마를 정규화할 뿐만 아니라 이 모든 정보를 확장 가능한 데이터 저장소에 저장하여 말이 되도록 만들어야 함 
      - 시간에 지남에 따라 운영과 업데이트 및 유지 관리할 수 있는 전용 데이터 파이프라인을 구축해야 함 
    - 그 외 스택에 걸쳐 모니터링 하기
      - 옵저버빌리티 : 데이터 수집 및 분석 등을 통해 언제, 무엇이 어디에서 일어나고 있는지 관측할 수 있도록 시스템을 유지하는 능력 또는 사고방식 
  - 쿼리 로그를 통한 데이터 웨어하우스의 데이터 품질 파악
    - 쿼리 로그 활용
      - 누가 이 데이터에 접근하는가?
      - 업스트림 또는 다운스트림은 어디에서 오는가?
      - 평균적으로 이 특정 변환이 얼마나 자주 실행되는가?
      - 몇 개의 행이 영향을 받는가?
    - 수집된 메타데이터 활용
      - 이 테이블은 언제 마지막으로 쿼리되었는가?
      - 업데이트는 지금까지의 흐름과 유사했는가? 아니면 패턴을 깨는가?
      - 데이터 웨어하우스의 하루 적재량은 어느 정도인가?
      - 두 달 전보다 점진적으로 더 오래 걸리는가?
      - 누가 (또는 어떤 봇이) 이 리소스에 접근할 수 없는가?
  - 쿼리 로그를 사용한 데이터 레이크의 품질 파악
    - 아마존 S3의 경우 객체 관리를 위해 객세 삽입 시간과 페이로드 크기를 저장해야 함. 메타데이터를 활용하면 이 객체의 마지막 업데이트 시기는 언제인가? 이 유형의 파일은 평균 크기가 얼마인가? 최근에 증가 추세인가?와 같은 질문에 답할 수 있음 
    - 현대 데이터 레이크에서 제공하는 시스템 메타데이터
      - 객체 삽입 시간
      - 객체 크기(바이트)
      - (인식되는 경우) 객체 파일 형식
      - 암호화 사용 여부 
- 데이터 카탈로그 설계
  - 설계 목적
    - 데이터를 어디에서 찾아야 하는가?
    - 이 데이터는 중요한가?
    - 이 데이터는 무엇을 나타내는가?
    - 이 데이터가 관련성이 있고 그 관련성은 중요한가?
    - 이 데이터를 어떻게 사용할 수 있는가?
- 데이터 카탈로그 구축
  - 데이터 소스와 연결 대상, 마지막 업데이트 시기, 소유자, 비고 
  - 데이터의 위치, 소유권, 잠재적인 사용 사례에 대한 맥락과 통찰력을 제공하는 데이터(메타데이터)의 모음 
  - 데이터 조직은 데이터 웨어하우스의 모든 테이블을 수동으로 검색하거나 자동화된 SQL 파서를 사용하여 작업을 수행할 수도 있음
    - Sqlparse, ANTLR, 아파치 캘사이트, MySQL의 SQL 파서 모두 널리 사용 
  - GraphQL, REST, Cube.js와 같은 오픈 소스 쿼리 언어 도구를 사용하면 데이터베이스에서 SQL을 쿼리하고, 아문센, 아파치 아틀라스, 데이터 허브, CKAN과 같은 카탈로그 시각화 서비스에 렌더링 할 수 있음 
  - 데이터 검색은 데이터의 이상적인 상태뿐 아니라 각 도메인에 걸친 데이터의 현재 상태에 대해서 다음과 같은 질문에 답할 수 있음
    - 가장 최근에 데이터셋은 무엇인가? 사용되지 않을 가능성이 있는 데이터셋은 무엇인가?
    - 테이블이 마지막으로 업데이트된 것은 언제인가?
    - 도메인에서 주어진 필드의 의미는 무엇인가?
    - 누가 이 데이터에 접근할 수 있는가? 마지막으로 사용한 것은 언제, 누구인가?
    - 이 데이터의 업스트림과 다운스트림의 의존성을 무엇인가?
    - 프로덕트가 될 수 있을 만한 수준으로 데이터 품질이 높은가? 
    - 도메인 비즈니스 요구 사항에 중요한 데이터는 무엇인가?
    - 가정은 무엇이며, 가정이 충족되었는가? 
  - 셀프 검색 서비스 및 자동화 
  - 데이터 진화에 따른 확장성
  - 분산 검색을 위한 데이터 계보 : 데이터 검색은 데이터 자산 간의 업스트림 및 다운스트림 의존성을 매핑하기 위해 테이블 및 필드 레벨 계보에 크게 의존함 

# 데이터 수집,정제,변환,테스트
- 데이터 수집
  - 데이터 소스의 3가지 범주
    - 애플리케이션 로그 데이터
      - 타임스탬프
      - 로그 레벨
    - API응답
      - API는 두 프로그램 사이의 매개체로, 특정 형식의 요청과 이에 대한 응답이 필요함. 이때 목적에 맞는 데이터는 반구조화된 데이터(데이터의 형식과 구조가 변경될 수 있는 데이터로, 데이터의 구조 정보를 함께 제공하는 파일 형식의 데이터)
    - 센서 데이터 
      - 고려사항 : 노이즈, 고장 모드 
- 데이터 정제
  - 오류값 제거
  - 데이터셋 특징 평가
  - 정규화 : L1(맨하탄) 정규화 및 L2(유닛) 정규화, 평균차분 및 단위 분산이 있으며 최상의 선택이 무엇인지는 데이터 사용 사례에 따라 달라짐 
  - 데이터 재구성 : 보간법, 외삽법, 서로 비슷한 데이터의 범주화/분류  
  - 시간대 변환 
  - 유형 변환 
- 배치 처리 vs 실시간 처리
  - 배치 처리 : 일정 기간 동안 데이터를 수집하여 대량의 데이터를 별개의 패킷으로 배치
  - 실시간 처리는 프로세스는 길지만 데이터를 거의 즉시 처리함 
- 실시간 처리를 위한 데이터 품질
  - 배치 처리와 실시간 처리의 주요 차이점은 배치장 처리되는 데이터의 양과 속도
    - 배치 처리는 지연이 발생하더라도 가능한 한 많은 데이터를 수집하는 것과 관련
    - 실시간 처리는 가능한 한 빨리 데이터를 수집하는 것과 관련이 있으므로 손실이 발생할 수 있음
  - 데이터 품질은 배치 스트리밍 시스템에서 더 높은 경향이 있지만, 데이터를 실시간으로 스트리밍할 때 오류(및 낮은 데이터 품질)의 중요성을 훨씬 커짐
  - 아마존 키네시스
    - 키네시스 용량은 온디맨드(사용한 만큼, 언제든지 즉 수요를 중심으로 결정하는 시스템이나 전략 등을 총칭)로 확장되므로 데이터 볼륨이 증가하기 전에 리소스를 프로비저닝 상태로 만들어 두기 때문에 확장해야 할 필요성이 줄어듬
    - 장점
      - 온디맨드 가용성 : 로드가 증가하면 리소스 그룹을 확장할 수 있음 
      - 비용 효율성 : 리소스 사용량에 비례하여 늘어남. 데이터 처리량이 처리 주기를 어떻게 설정하느냐에 따라 크게 변할 수 있는 스트리밍 서비스에 특히 중요함 
      - 철저한 SDK : 훨씬 더 많은 언어로 개발 가능하도록 지원(카프카는 자바만 지원) 
      - AWS 인프라에 통합 : 아마존의 헤게모니에는 장점(타사 또는 오픈 소스 대안보다 S3,레드시프트 및 기타 아마존 데이터 서비스와 통합하기 훨씬 쉬움)
  - 아파치 카프카
    - 카프카 스트림즈는 카프카 클러스터와의 스트리밍 데이터를 지원하는 클라이언트 라이브러리 
    - 데이터 스트리밍 및 통합 레이어와 스트리밍 분석을 제공함. 카프카 스트리밍 서비스는 낮은 대기 시간에 최적화되어 있으며, 네트워크 제한 처리량에 따라 2밀리초의 낮은 대기시간을 자랑함
    - 이점
      - 오픈 소스 커뮤니티
      - 향상된 커스터마이징 : 데이터 보존 기간을 수동으로 지정할 수 있는 등 사용자 맞춤 구성이 가능함(키네시스는 이 기간을 7일로 고정)
      - 높은 처리량 : 카프카는 초당 3만 개의 레코드 처리량을 지원. 키네시스는 초당 수천 개의 레코드만 지ㅜ언 
  - 신속한 가치 창출을 원하는 소규모 데이터 조직이라면 설정이 쉬운 관리형 솔루션인 아마존 키네시스를 선택하여 서비스형 소프트웨어(SaaS) 프로덕트의 이점을 누릴 수 있음
  - 보다 구체적인 요구 사항을 가진 대규모 조직은 오픈 소스 아파치 카프카를 활용하는 것에 적합함 
- 데이터 정규화
  - 운영 데이터 변환 레이어를 데이터 정규화 단계
  - 데이터 변환은 하나 이상의 소스 형식에서 목적지 형식으로 데이터를 이동하는 프로그램 
  - 정규화는 데이터 파이프라인을 통과하는 과정에서 첫 번째로 겪는 변환. 노이즈, 모호성 및 이질성이 최대치인 진입점 데이터에서 정규화가 발생
  - 이종 데이터 소스 처리
    - 대기 시간에 최적화 : 최종 상태와 관계없이 파이프라인으로 즉시 푸시되므로 데이터 배치는 불완전할 것으로 예상   
    - 비위계적인 형식 : 깨끗한 창고 + 스키마 + 테이블 저장 방싟보다는 S3 버킷과 같은 일부 중앙 저장소에 데이터를 한번에 쏟아부을(덤핑) 가능성이 높음
    - 원시 파일 형식 : 애플리케이션 로그 데이터와 센서 데이터를 표 형식으로 변환할 필요 없음
    - 선택적 데이터 필드 : JSON 필드 부재 의미 추론
    - 이질성 
- 분석 데이터 변환 실행
  - 변환 과정의 데이터 품질 보장
    - 소스 데이터를 변환하는 몇 가지 이유
      - 대상 위치의 스키마 요구 사항에 맞게 필드 이름을 변경할 수 있음
      - 소스 데이터를 필터링, 집계 및 요약, 중복 제거 또는 정제 및 통합할 수 있음 
      - 유형 및 단위 변환을 모두 수행해야 할 수도 있음. 서로 다른 통화 필드를 모두 미국 달러와 부동소수점 유형으로 표준화하는 것이 그 예
      - 이 단계에서 중요한 데이터 필드나 업계 또는 정부 규정을 충족하기 위해 암호화를 수행할 수 있음 
      - 궁극적인 목적을 달성하기 위해 가장 중요한 것은 해당 단계에서 데이터 거버넌스 감사나 데이터 품질 감사를 수행할 수 있다는 것 
- 테스트 및 경고 알람 시스템
  - 가장 일반적인 데이터 품질 테스트
    - NULL 값 : 알 수 없는 값(NULL)이 있는가?
    - 용량 : 데이터를 전부 받았는가? 너무 많이 받았는가, 아니면 너무 적게 받았는가?
    - 분포 : 데이터가 허용 범위 내에 있는가? 값이 지정된 열의 범위 내에 있는가?
    - 유니크함 : 중복된 값이 있는가?
    - 정보 불변속성 : 두 개체는 근본적으로 서로 다른가? 예를 들면, 순수익은 항상 매출에서 비용을 뺀 값인가? 
  - 데이터를 테스트하는 가장 적합한 도구
    - dbt 테스트
      - 확인해야할 사항
        - 기술 부채 및 유지
        - 테스트 피로 및 암묵적 지식
        - 제한된 가시성
    - Great Expectations
      - 장점
        - 일반적인 사용 편의성 
        - 슬랙 연동
      - 제한 사항
        - 파이썬으로 제한된 사용
        - 변환/작업 오케스트레이션 도구와 분리 
    - 디쿠 단위 테스트 
      - 이점
        - AWS와 연동
        - 뛰어난 확장성 : 스칼라 실행시
        - 스테이트풀 계산 
        - 이상 탐지 기능 내장
      - 단점
        - 스칼라의 학습 곡선 : PyDeeque 고려 
        - 통합 테스트로서는 제한된 적용 
        - 직관적인 UI 부족 
- 아파치 에어플로를 활용한 데이터 품질 관리
  - 아파치 에어플로, 루이지, 마틸리온, 스티치와 같은 툴을 사용하면 데이터 파이프라인 전반에서 워크플로를 프로그래밍 방식으로 작성, 예약 및 모니터링하는 조정 레이어에서 데이터 품질을 보다 효율적으로 관리할 수 있음 
  - 워크플로의 여러 체크포인트를 고려할 때 데이터 구조에 장애가 발생하거나 잘못된 변경이 일어나는 상황은 사실 흔함 
  - DAG의 가장 큰 일반적인 데이터 다운타임 유형은 쿼리 성능 저하 및 잘못된 파이썬 코드 
  - 사용자는 작업에 소요되는 최대 시간 동안 SLA를 예약할 수 있음. 작업이 더 오래 실행되면 아파치 에어플로 UI에서 SLA 누락으로 표시되거나 슬랙, 마이크로소프트 팀, 이메일 또는 기타 선호하는 채널을 통해 통신할 수 있음 
  - SLA 목록
    - SLA가 누락되었을 때 트리거되도록 sla_miss_callback을 포함할 수 있음 
    - 다섯 가지 매개변수
      - dag : 태스크가 SLA를 충족하지 못한 DAG 실행에 대한 상위 DAG 개체
      - task_list : 마지막 sla_miss_callback 이후 SLA를 충족하지 못한 모든 작업 목록
      - blocking_task_list : DAG에서 실행 중인 태스크 중 성공 상태가 아닌 태스크는 sla_miss_callback 실해, 즉 실패한 실행
      - slas : 태스크 목록의 태스크와 연결된 SlaMiss 개체 목록
      - blocking_tls : blocking_task_list 임계값 작업과 연결된 TaskInstanace 객체 목록
  - 오케스트레이션 레이어에서 데이터 사고를 방지하기 위한 또 다른 방법 : 실행 중인 파이프라인에서 서킷 브레이크 방법론 적용
    - 서킷 브레이크는 데이터가 품질 임계값들을 충족하지 못할 때, 파이프라인 작동을 중지해 버리는 것을 의미함 
    - 서킷 폐쇄 : 데이터가 파이프라인을 통해 흐르고 있다
    - 서킷 개방 : 데이터가 파이프라인을 통해 흐르고 있지 않다 
    - 서킷 브레이커를 사용하기 위한 세 가지 핵심 솔루션
      - 데이터 계보
      - 파이프라인 전반의 데이터 프로파일링
      - 프로파일링을 통해 발견된 문제를 통해 서킷을 자동으로 트리거하는 기능 
    - 아파치 에어플로에서는 데이터가 새로고침 프로그램, 볼륨 및 스키마 임계값 요구 사항을 충족하지 못할 경우 조정 레이어에서 실제로 데이터 파이프라인을 중지하여 데이터 품질 문제를 예방할 수 있음 
    - 방법
      - DAG의 캐치업 매개변수를 False로 설정
      - DAG 내부에 LatestOnlyOperator 연산자를 포함하여 DAG 실행을 중지함
      - 맞춤형 파이썬 코드를 오케스트레이션 레이어에 삽입하여 중단하게 만들고, 데이터 옵저버빌리티 플랫폼 또는 데이터 카탈로그에서 직접 근본 원인 분석과 관련된 메타데이터를 표시함 
  - SQL 검사 연산자 
  - 데이터 인제스천 : 여러 소스로부터 수집된 데이터를 적절한 표준 형태로 변환하여 저장하는 단계

# 데이터 파이프라인 모니터링 및 이상 탐지 
- 볼륨, 신선도, 분포 및 기타값에 대한 예상 임계값이 기대에 미치지 못하는 경우를 식별하는 이상 탐지 기술을 사용 - 모니터링 
- 알려진 미지와 알려지지 않은 미지
  - 예측할 수 있는 것(알려진 미지)
    - NULL 값, 특정 신선도 문제, 시스템 정기 업데이트로 유발된 스키마 변경과 같은 쉽게 예측할 수 있는 문제
    - 모니터링 및 이상탐지가 많은 부분 처리 
  - 예측할 수 없는 것(알려지지 않은 미지)
    - 가장 포괄적인 테스트로도 설명할 수 없는 데이터 다운타임. 특정 테스트에서 다루는 섹션뿐 아니라 전체 데이터 파이프라인에서 발생하는 문제를 의미함 
    - 테스트 및 서킷 브레이커가 많은 부분 처리 
- 이상 탐지 알고리즘 구축
  - 신선도 모니터링 : 데이터가 언제 마지막으로 업데이트되었는지를 보여주는 강력한 지표 
  - 분포 : 데이터의 모든 예상값과 각 값이 발생하는 빈도를 알려줌. 데이터에 NULL 값이 얼마나 자주 나타나는가? 
- 스키마 및 계보를 위한 모니터 구축
  - 스키마 변경은 데이터 구조가 변경될 때 발생하며, 수동으로 디버깅하기 어려운 데이터 이상.
    - 새로운 API 엔드포인트 추가
    - 아직 사용되지 않았으며 앞으로도 사용되지 않을 것으로 추정되는 필드
    - 열,행 또는 전체 테이블의 추가 또는 제거 
  - 계보의 시각화
    - 데이터 옵저버빌리티의 핵심 요소 중 가장 중요한것 
    - 영향을 받을 수 있는 다운스트림 소스와 근본적인 원인이 될 수 있는 업스트림 소스를 알려줌으로써 문제를 맥락화함 
- 파이썬과 머신러닝으로 이상 탐지 확장
  - 머신러닝 탐지기는 실시간으로 데이터를 학습하고 적용할 수 있으며, 사람의 눈에는 보이지 않는 복잡한 계절성을 찾아낼 수 있음 
- 이상 탐지의 심화 과정: 기타 유용한 접근법
  - 규칙 정의 처리 : 특정 메트릭 값에 대해 명시적인 중단을 설정하고 임곗값을 기준으로 이상을 결정하는 것 
  - 자기회귀 모델 : 이전 시간에서 데이터를 가져와 회귀(선형) 모델에 입력하고 다음 타임스탬프 데이터가 있을 위치에 대한 예측 결과를 출력함 
  - 지수평활 : 시계열에서 추세와 계절성을 제거하여 보다 가벼운 접근 방식(ARIMA)이 대신할 수 있도록 하는 방법 
  - 클러스터링 : 유사항 데이터 포인트를 한데 모아 경고함으로써 이상을 탐지함
