# 서평
- 이 책은 스파크의 기초적인 내용부터 처리,운용,관리,모니터링, 그리고 머신러닝, 그래프에 이르기까지 다양한 내용을 종합적으로 잘 설명하고 있습니다.

# 빅데이터와 스파크 간단히 살펴보기
- 스파크
  - 하둡의 장점뿐만 아니라 여러 오픈소스가 가진 장점을 함께 가지고 있음
  - 아파치 하이브의 장점인 HQL을 사용할 수 있을 뿐만 아니라 SQL에 친숙한 사용자를 위해 스파크 SQL을 제공함
  - 아파치 스톰의 스트리밍 처리 기술을 지원하는 스파크 스트리밍을 제공함
  - 머신러닝 처리를 위한 MLlib 라이브러리를 제공함
  - 스파크 GraphX를 제공
- 아파치 스파크
  - 통합 컴퓨팅 엔진이며 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합 ( 빅데이터를 위한 통합 컴퓨팅 엔진과 라이브러리 집합)
  - (파이썬,자바,스칼라,R)을 지원하며 SQL뿐만 아니라 스트리밍, 머신러닝에 이르기까지 넓은 범위의 라이브러리를 제공함
  - 단일 노트북 환경에서부터 수천 대의 서버로 구성된 클러스터까지 다양한 환경에서 실행함
  - 빅데이터 애플리케이션 개발에 필요한 통합 플랫폼을 제공하자는 목적
  - 간단한 데이터 읽기에서부터 SQL 처리, 머신러닝 그리고 스트림 처리에 이르기까지 다양한 데이터 분석 작업을 같은 연산 엔진과 일관성 있는 API로 수행할 수 있도록 설계
  - 클라우드 기반의 애저 스토리지,아마존 S3, 분산 파일 시스템인 아파치 하웁, 키/값 저장소인 아파치 카산드라, 메시지 전달 서비스인 아파치 카프카 등의 저장소를 지원함
  - SQL과 구조화된 데이터를 제공하는 스파크 SQL, 머신러닝을 지원하는 MLlib, 스트림 처리 기능을 제공하는 스파크 스트리밍과 새롭게 선보인 구조적 스트리밍 그리고 그래프 분석 엔진인 GraphX 라이브러리르 제공 
  - 스파크는 스칼라로 구현되어 자바 가상 머신 기반으로 동작함
  - 코드 스니펫(snippet) : 스니펫은 재사용이 가능한 소스 코드나 기계어의 작은 부분을 뜻하는 프로그래밍 용어

## 스파크 애플리케이션
- 드라이버(driver) 프로세스와 다수의 익스큐터(executor) 프로세스로 구성됨
- 정보의 유지 관리, 사용자 프로그램이나 입력에 대한 응답, 전반적인 익스큐터 프로세스의 작업과 관련된 분석, 배포 그리고 스케줄링 역할을 수행
- 익스큐터 : 드라이버 프로세스가 할당한 작업을 수행, 드라이버가 할당한 코드를 실행하고 진행 상황을 다시 드라이버 노드에 보고하는 두 가지 역할을 수행
- 언어 API
  - 스칼라 : 스파크는 스칼라로 개발되어 있으므로 스칼라가 스파크의 기본 언어
  - 자바 : 스파크가 스칼라로 개발되어 있지만, 스파크 창시자들은 자바를 이용해 스파크 코드를 작성할 수 있도록 심혈을 기울임
  - 파이썬 : 스칼라가 지원하는 거의 모든 구조를 지원함
  - SQL : ANSI SQL:2003 표준 중 일부를 지원함
  - 스파크 코어에 포함된 SparkR, R 커뮤니티 기반 패키지인 sparklyr
- 스파크 API
  - 저수준의 비구조적(unstructured) API
  - 고수준의 구조적(structured) API
- DataFrame
  - 스키마(schema) : 컬럼과 컬럼의 타입을 정의한 목록
  - 스프레드시프트는 한 대의 컴퓨터에 존재, 스파크 DataFrame은 수천 대의 컴퓨터에 분산(단일 컴퓨터에 저장하기에는 데이터가 너무 크거나 계산에 너무 오랜 시간이 걸릴 수 있기 때문)
- 파티션
  - 스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라 불리는 청크 단위로 데이터를 분할함
  - 파티션 : 클러스터의 물리적 머신에 존재하는 로우의 집합
- 트랜스포메이션
  - 스파크의 핵심 데이터 구조는 불변성(immutable) : 한번 생성하면 변경할 수 없음
  - DataFrame을 변경하려면 원하는 변경 방법을 스파크에 알려줘야 하는데 이를 트랜스포메이션
  - 좁은 의존성(narrow dependency) : 각 입력 파티션이 하나의 출력 파티션에만 영향을 미침, 파티션이 하나의 출력 파티션에만 영향을 미침 
  - 넓은 의존성(wide dependency) : 하나의 입력 파티션이 여러 출력 파티션에 영향을 미침, 스파크가 클러스터에서 파티션을 교환(셔플-shuffle)
  - 좁은 트랜스포메이션을 사용하면 스파크에서 파이프라이닝(pipelining)을 자동으로 수행함, DataFrame에 여러 필터를 지정하는 경우 모든 작업이 메모리에서 일어남
- 지연 연산(lazy evaluation)
  - 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미
  - 특정 연산 명령이 내려진 즉시 데이터를 수정하지 않고 원시 데이터에 적용할 트랜스포메이션의 실행 계획을 생성함
- 액션
  - 콘솔에서 데이터를 보는 액션
  - 각 언어로 된 네이티브 객체에서 데이터를 모으는 액션
  - 출력 데이터소스에 저장하는 액션
  - 액션을 지정하면 스파크 잡(job) 시작, 스파크 잡은 필터(좁은 트랜스포메이션)를 수행한 후 파티션별로 레코드 수를 카운트(넓은 트랜스포메이션), 각 언어에 적합한 네이티브 객체에 결과를 모음
- 스파크 UI
  - 스파크 잡의 진행 상황을 모니터링 할 때 사용
  - 스파크 잡의 상태, 환경 설정, 클러스터 상태의 정보를 확인
  - 스파크 잡을 튜닝하고 디버깅할 때 매우 유용

## 스파크 기능
- 스파크 라이브러리 : 그래프 분석, 머신러닝, 그리고 스트리밍 등 다양한 작업을 지원하며, 컴퓨팅 및 스토리지 시스템과의 통합을 돕는 역할
- spark-submit : 대화형 셸에서 개발한 프로그램을 운영용 애플리케이션으로 쉽게 전환, 애플리케이션 코드를 클러스터에 전송해 실행시키는 역할
- 스파크 애플리케이션은 스탠드얼른,메소스, YARN 클러스터 매니저를 이용해 실행됨
- 타입 안정성을 제공하는 구조적 API
  - Dataset
    - 자바와 스칼라의 정적 데이터 타입에 맞는 코드
    - 정적 타입 코드(statically typed code)를 지원하기 위해 고안된 스파크의 구조적 API
      - 정적 타입 코드 : 자료형이 고정된 언어, 자바,스칼라,C,C++ 등의 프로그래밍 언어가 정적 타입에 해당
      - 동적 타입 프로그래밍 언어 : 파이썬, 자바스크립트 등 
    - Dataset은 타입 안정성을 지원하며 동적 타입 언어인 파이썬과 R에서는 사용할 수 없음
    - Dataset 클래스는 내부 객체의 데이터 타입을 매개변수로 사용
  - DataFrame
    - 다양한 데이터 타입의 테이블형 데이터를 보관할 수 있는 Row 타입의 객체로 구성된 분산 컬렉션
  - Dataset API
    - DataFrame의 레코드르 사용자가 자바나 스칼라로 정의한 클래스에 할당하고 자바의 ArrayList 또는 스칼라의 Seq 객체 등의 고정 타입형 컬렉션으로 다룰 수 있는 기능을 제공함
    - 타입 안정성을 지원하므로 초기화에 사용한 클래스 대신 다른 클래스를 사용해 접근할 수 없음
    - 다수의 소프트웨어 엔지니어가 잘 정의된 인터페이스로 상호작용하는 대규모 애플리케이션을 개발하는 데 특히 유용함
- 구조적 스트리밍
  - 스파크 2.2 버전에서 안정화(production-ready)된 스트림 처리용 고수준 API
  - 구조적 API로 개발된 배치 모드의 연산을 스트리밍 방식으로 실행, 지연 시간을 줄이고 증분 처리
  - 배치 처리용 코드를 일부 수정하여 스트리밍 처리를 수행하고 값을 빠르게 얻을 수 있다는 장점
  - 프로토타입을 배치 잡으로 개발한 다음 스트리밍 잡으로 변환할 수 있으므로 개념 잡기가 수월함
  - 셔플 파티션 수 : 셔플 이후에 생성될 파티션 수, 기본값은 200이지만 로컬 모드에서는 그렇게 많은 익스큐터가 필요하지 않으므로 5로 변경
    - spark.conf.set("spark.sql.shuffle.partitions","5")
  - 스트리밍 코드
    - read 메서드 대신 readStream 메서드 사용
    - maxFilesPerTrigger : 한 번에 읽을 파일수를 설정함
  - MLlib : 대용량 데이터를 대상으로 전처리(preprocessing), 멍잉(munging - 데이터 랭글링(data wrangling), 원본 데이터를 다른 형태로 변환하거나 매핑하는 과정), 모델 학습(model training) 및 예측(prediction)
- 저수준 API
  - RDD를 통해 자바와 파이썬 객체를 다루는 데 필요한 다양한 기본 기능을 제공
  - 스파크의 거의 모든 기능은 RDD를 기반
  - 드라이버 시스템의 메모리에 저장된 원시 데이터를 병렬처리(parallelize)하는 데 RDD를 사용

# 구조적 API: DataFrame, SQL, Dataset
## 구조적 API 개요
- 구조적 API : 비정형 로그 파일로부터 반정형 csv파일, 매우 정형적인 파케이(Parquet) 파일까지 다양한 유형의 데이터를 처리
  - 세 가지 분산 컬렉션 API
    - Dataset : 타입형
    - DataFrame : 비타입형
    - SQL 테이블과 뷰
  - 배치작업을 스트리밍 작업으로 손쉽게 변환함
- 스파크는 트랜스포메이션의 처리 과정을 정의하는 분산 프로그래밍 모델
  - 사용자가 정의한 다수의 트랜스포메이션은 지향성 비순환 그래프(DAG)로 표현되는 명령을 만들어냄
  - 액션은 하나의 잡을 클러스터에서 실행하기 위해 스테이지와 태스크로 나누고 DAG 처리 프로세스를 실행함
  - 트랜스포메이션과 액션으로 다루는 논리적 구조가 바로 DataFrame과 Dataset
- 스키마 : 분산 컬렉션에 저장할 데이터 타입을 정의하는 방법
- DataFrame의 컬럼명과 데이터 타입을 정의함
- 카탈리스트(Catalyst) 엔진 : 실행 계획 수립과 처리에 사용하는 자체 데이터 정보를 가지고 있음, 다양한 실행 최적화 기능을 제공함
- DataFrame은 Row 타입으로 구성된 Dataset
  - Row 타입 : 스파크가 사용하는 연산에 최적화된 인메모리 포맷의 내부적인 표현 방식
    - 가비지 컬렉션(garbage collection)과 객체 초기화 부하가 있는 JVM 데이터 타입을 사용하는 대신 자체 데이터 포맷을 사용하기 때문에 매우 효율적인 연산이 가능함 
- 컬럼 : 정수형이나 문자열 같은 단순 데이터 타입, 배열이나 맵 같은 복합 데이터 타입, null 값을 표현함
- 로우 : 데이터 레코드(Dataframe의 각 로우는 하나의 레코드)
- 구조적 API의 실행 과정
  - 구조적 API 쿼리가 사용자 코드에서 실제 실행 코드로 변환되는 과정
    - DataFrame/Dataset/SQL을 이용해 코드를 작성함
    - 정상적인 코드라면 스파크가 논리적 실행 계획으로 변환함
    - 스파크는 논리적 실행 계획을 물리적 실행 계획으로 변환하며 그 과정에서 추가적인 최적화를 할 수 있는지 확인
    - 스파크는 클러스터에서 물리적 실행 계획(RDD 처리)을 실행
## 구조적 API 기본 연산
- DataFrame : Row 타입의 레코드(테이블의 로우 같은)와 각 레코드에 수행할 연산 표현식을 나타내는 여러 컬럼(스프레드시트의 컬럼 같은)으로 구성됨
- 스키마 : 각 컬러명과 데이터 타입을 정의함(DataFrame의 컬럼명과 데이터 타입을 정의함)
- 파티셔닝 : DataFrame이나 Dataset이 클러스터에서 물리적으로 배치되는 형태를 정의함
- 파티셔닝 스키마 : 파티션을 배치하는 방법을 정의함
- selectExpr 메서드 : 새로운 DataFrame을 생성하는 복잡한 표현식을 간단하게 만드는 도구
- 리터럴(literal) : 프로그래밍 언어의 리터럴값은 스파크가 이해할 수 있는 값으로 변환함
- 최적화 기법
  - 자주 필터링하는 컬럼을 기준으로 데이터를 분할함
  - 파티셔닝 스키마와 파티션 수를 포함해 클러스터 전반의 물리적 데이터 구성을 제어함
  - repartition 메서드 : 무조건 전체 데이터를 셔플함, 향후에 사용할 파티션 수가 현재 파티션 수보다 많거나 컬럼을 기준으로 파티션을 만드는 경우에만 사용
    - 특정 컬럼을 기준으로 자주 필터링한다면 자주 필터링되는 컬럼을 기준으로 파티션을 재분배하는것이 좋음
  - coalesce 메서드 : 전체 데이터를 셔플하지 않고 파티션을 병합하는 경우 사용
- taLocalIterator : 이터레이터(iterator,반복자)로 모든 파티션의 데이터 드라이버에 전달함 , 데이터셋의 파티션을 차례로 반복 처리함

## 다양한 데이터 타입 다루기
- DataFrame(Dataset) 메서드
  - Dataset의 하위 모듈은 다양한 메서드를 제공함
    - DataFrameStatFunctions : 다양한 통계적 함수를 제공
    - DataFrameNaFunctions : null 데이터를 다루는 데 필요한 함수를 제공
- Column 메서드
  - Column은 alias나 contains 같이 컬럼과 관련된 여러가지 메서드를 제공
  - org.apache.spark.sql.functions 패키지는 데이터 타입과 관련된 다양한 함수를 제공함
- 불리언 구문 : and,or,true,false
- initcap 함수 : 주어진 문자열에서 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경 
- 날짜와 타임스탬프 데이터
  - 날짜(date) : 달력 형태의 날짜
  - 타임스탬프(timestamp) : 날짜와 시간 정보를 모두 가짐
  - inferSchema 옵션이 활성화된 경우 날짜와 타임스탬프를 포함해 컬럼의 데이터 타입을 최대한 정확하게 식별하려 시도함
- null 값
  - 명시적으로 null 값을 제거, 전역 또는 컬럼 단위로 null 값을 특정 값으로 채워 넣음
  - coalesce 함수 : 인수로 지정한 여러 컬럼 중 null이 아닌 첫번 째 값을 반환함
- 사용자 정의함수
  - UDF(user defined function) : 파이썬이나 스칼라 그리고 외부 라이브러리를 사용해 사용자가 원하는 형태로 트랜스포메이션을 만들 수 있게 함 
  - 모든 워커 노드에서 생성된 함수를 사용할 수 있도록 스파크에 등록 -> 드라이버에서 함수를 직렬화하고 네트워크를 통해 모든 익스큐터 프로세스로 전달함
  - 스칼라나 자바 -> JVM 환경에서만 사용 가능
  - 워커 노드에서 파이썬 프로세스를 실행하고 파이썬이 이해할 수 있는 포맷으로 모든 데이터를 직렬화(JVM 사용 언어에도 존재했던 부분), 파이썬 프로세스에 있는 데이터의 로우마다 함수를 실행하고 마지막으로 JVM과 스파크에 처리 결과를 반환함
    - 직렬화에 큰 부하가 발생
    - 데이터가 파이썬으로 전달되면 스파크에서 워커 메모리를 관리할 수 없음, JVM과 파이썬이 동일한 머신에서 메모리 경합을 하면 자원에 제약이 생겨 워커가 비정상적으로 종료될 가능성이 있음

## 집계 연산
- count 메서드가 트랜스포메이션이 아닌 액션, 결과를 즉시 반환함, count메서드는 데이터셋의 전체 크기를 알아보는 용도로 사용하지만 메모리에 DataFrame 캐싱 작업을 수행하는 용도로 사용됨
- approx_count_distinct : 어느 정도 수준의 정확도를 가지는 근사치만으로도 유의미, 최대 추정 오류율(maximum estimation error), 대규모 데이터셋을 사용할 때 훨씬 더 좋아짐
- 비대칭도와 첨도(kurtosis)
  - 데이터의 변곡점(extreme point)을 측정하는 방법
  - 비대칭도(skewness)는 데이터 평균의 비대칭 정도를 측정하고, 첨도는 데이터 끝 부분을 측정함
  - 확률변수(random variable)의 확률 분포(probability distribution)로 데이터를 모델링할 때 특히 중요함
- 복합 데이터 타입의 집계
  - collect_list() : 특정 컬럼의 값을리스트로 수집
  - collect_set() : 셋 데이터 타입으로 고윳값만 수집 
- 윈도우 함수
  - 랭크 함수(ranking function)
  - 분석 함수(analytic function)
  - 집계 함수(aggregate function)
- 사용자 정의 집계 함수(user-defined aggregation function, UDAF)
  - 직접 제작한 함수나 비즈니스 규칙에 기반을 둔 자체 집계 함수를 정의하는 방법
  - 입력 데이터 그룹에 직접 개발한 연산을 수행
  - 입력 데이터의 모든 그룹의 중간 결과를 단일 AggregationBuffer에 저장해 관리함

## 조인
- 스파크의 조인 수행 방식
  - 노드간 네트워크 통신 전략
    - 스파크는 조인 시 두가지 클러스터 통신 방식을 활용함
    - 전체 노드간 통신을 유발 셔플 조인(shuffle join), 
    - 그렇지 않은 브로드캐스트 조인(broadcast join) , 작은 DataFrame을 클러스터의 전체 워커 노드에 복제하는 것, 대구모 노드 간 통신이 발생하지만 그 이후로는 노드 사이에 추가적인 통신이 발생하지 않음, 모든 단일 노드에서 개별적으로 조인이 수행되므로 CPU가 가장 큰 병목 구간, 너무 큰 데이터를 브로드캐스트하면 고비용의 수집 연산이 발생하므로 드라이버 노드가 비정상적으로 종료됨
    - 내부 최적화 기술은 시간이 흘러 비용 기반 옵티마이저(cost-based optimizer, CBO)가 개선되고 더 나은 통신 전략이 도입되는 경우 바뀔 수 있음
  - 노드별 연산 전략
