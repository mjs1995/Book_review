# 서평
- 이 책은 스파크의 기초적인 내용부터 처리,운용,관리,모니터링, 그리고 머신러닝, 그래프에 이르기까지 다양한 내용을 종합적으로 잘 설명하고 있습니다.

# 빅데이터와 스파크 간단히 살펴보기
- 스파크
  - 하둡의 장점뿐만 아니라 여러 오픈소스가 가진 장점을 함께 가지고 있음
  - 아파치 하이브의 장점인 HQL을 사용할 수 있을 뿐만 아니라 SQL에 친숙한 사용자를 위해 스파크 SQL을 제공함
  - 아파치 스톰의 스트리밍 처리 기술을 지원하는 스파크 스트리밍을 제공함
  - 머신러닝 처리를 위한 MLlib 라이브러리를 제공함
  - 스파크 GraphX를 제공
- 아파치 스파크
  - 통합 컴퓨팅 엔진이며 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합 ( 빅데이터를 위한 통합 컴퓨팅 엔진과 라이브러리 집합)
  - (파이썬,자바,스칼라,R)을 지원하며 SQL뿐만 아니라 스트리밍, 머신러닝에 이르기까지 넓은 범위의 라이브러리를 제공함
  - 단일 노트북 환경에서부터 수천 대의 서버로 구성된 클러스터까지 다양한 환경에서 실행함
  - 빅데이터 애플리케이션 개발에 필요한 통합 플랫폼을 제공하자는 목적
  - 간단한 데이터 읽기에서부터 SQL 처리, 머신러닝 그리고 스트림 처리에 이르기까지 다양한 데이터 분석 작업을 같은 연산 엔진과 일관성 있는 API로 수행할 수 있도록 설계
  - 클라우드 기반의 애저 스토리지,아마존 S3, 분산 파일 시스템인 아파치 하웁, 키/값 저장소인 아파치 카산드라, 메시지 전달 서비스인 아파치 카프카 등의 저장소를 지원함
  - SQL과 구조화된 데이터를 제공하는 스파크 SQL, 머신러닝을 지원하는 MLlib, 스트림 처리 기능을 제공하는 스파크 스트리밍과 새롭게 선보인 구조적 스트리밍 그리고 그래프 분석 엔진인 GraphX 라이브러리르 제공 
  - 스파크는 스칼라로 구현되어 자바 가상 머신 기반으로 동작함
  - 코드 스니펫(snippet) : 스니펫은 재사용이 가능한 소스 코드나 기계어의 작은 부분을 뜻하는 프로그래밍 용어

## 스파크 애플리케이션
- 드라이버(driver) 프로세스와 다수의 익스큐터(executor) 프로세스로 구성됨
- 정보의 유지 관리, 사용자 프로그램이나 입력에 대한 응답, 전반적인 익스큐터 프로세스의 작업과 관련된 분석, 배포 그리고 스케줄링 역할을 수행
- 익스큐터 : 드라이버 프로세스가 할당한 작업을 수행, 드라이버가 할당한 코드를 실행하고 진행 상황을 다시 드라이버 노드에 보고하는 두 가지 역할을 수행
- 언어 API
  - 스칼라 : 스파크는 스칼라로 개발되어 있으므로 스칼라가 스파크의 기본 언어
  - 자바 : 스파크가 스칼라로 개발되어 있지만, 스파크 창시자들은 자바를 이용해 스파크 코드를 작성할 수 있도록 심혈을 기울임
  - 파이썬 : 스칼라가 지원하는 거의 모든 구조를 지원함
  - SQL : ANSI SQL:2003 표준 중 일부를 지원함
  - 스파크 코어에 포함된 SparkR, R 커뮤니티 기반 패키지인 sparklyr
- 스파크 API
  - 저수준의 비구조적(unstructured) API
  - 고수준의 구조적(structured) API
- DataFrame
  - 스키마(schema) : 컬럼과 컬럼의 타입을 정의한 목록
  - 스프레드시프트는 한 대의 컴퓨터에 존재, 스파크 DataFrame은 수천 대의 컴퓨터에 분산(단일 컴퓨터에 저장하기에는 데이터가 너무 크거나 계산에 너무 오랜 시간이 걸릴 수 있기 때문)
- 파티션
  - 스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라 불리는 청크 단위로 데이터를 분할함
  - 파티션 : 클러스터의 물리적 머신에 존재하는 로우의 집합
- 트랜스포메이션
  - 스파크의 핵심 데이터 구조는 불변성(immutable) : 한번 생성하면 변경할 수 없음
  - DataFrame을 변경하려면 원하는 변경 방법을 스파크에 알려줘야 하는데 이를 트랜스포메이션
  - 좁은 의존성(narrow dependency) : 각 입력 파티션이 하나의 출력 파티션에만 영향을 미침, 파티션이 하나의 출력 파티션에만 영향을 미침 
  - 넓은 의존성(wide dependency) : 하나의 입력 파티션이 여러 출력 파티션에 영향을 미침, 스파크가 클러스터에서 파티션을 교환(셔플-shuffle)
  - 좁은 트랜스포메이션을 사용하면 스파크에서 파이프라이닝(pipelining)을 자동으로 수행함, DataFrame에 여러 필터를 지정하는 경우 모든 작업이 메모리에서 일어남
- 지연 연산(lazy evaluation)
  - 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미
  - 특정 연산 명령이 내려진 즉시 데이터를 수정하지 않고 원시 데이터에 적용할 트랜스포메이션의 실행 계획을 생성함
- 액션
  - 콘솔에서 데이터를 보는 액션
  - 각 언어로 된 네이티브 객체에서 데이터를 모으는 액션
  - 출력 데이터소스에 저장하는 액션
  - 액션을 지정하면 스파크 잡(job) 시작, 스파크 잡은 필터(좁은 트랜스포메이션)를 수행한 후 파티션별로 레코드 수를 카운트(넓은 트랜스포메이션), 각 언어에 적합한 네이티브 객체에 결과를 모음
- 스파크 UI
  - 스파크 잡의 진행 상황을 모니터링 할 때 사용
  - 스파크 잡의 상태, 환경 설정, 클러스터 상태의 정보를 확인
  - 스파크 잡을 튜닝하고 디버깅할 때 매우 유용

## 스파크 기능
- 스파크 라이브러리 : 그래프 분석, 머신러닝, 그리고 스트리밍 등 다양한 작업을 지원하며, 컴퓨팅 및 스토리지 시스템과의 통합을 돕는 역할
- spark-submit : 대화형 셸에서 개발한 프로그램을 운영용 애플리케이션으로 쉽게 전환, 애플리케이션 코드를 클러스터에 전송해 실행시키는 역할
- 스파크 애플리케이션은 스탠드얼른,메소스, YARN 클러스터 매니저를 이용해 실행됨
- 타입 안정성을 제공하는 구조적 API
  - Dataset
    - 자바와 스칼라의 정적 데이터 타입에 맞는 코드
    - 정적 타입 코드(statically typed code)를 지원하기 위해 고안된 스파크의 구조적 API
      - 정적 타입 코드 : 자료형이 고정된 언어, 자바,스칼라,C,C++ 등의 프로그래밍 언어가 정적 타입에 해당
      - 동적 타입 프로그래밍 언어 : 파이썬, 자바스크립트 등 
    - Dataset은 타입 안정성을 지원하며 동적 타입 언어인 파이썬과 R에서는 사용할 수 없음
    - Dataset 클래스는 내부 객체의 데이터 타입을 매개변수로 사용
  - DataFrame
    - 다양한 데이터 타입의 테이블형 데이터를 보관할 수 있는 Row 타입의 객체로 구성된 분산 컬렉션
  - Dataset API
    - DataFrame의 레코드르 사용자가 자바나 스칼라로 정의한 클래스에 할당하고 자바의 ArrayList 또는 스칼라의 Seq 객체 등의 고정 타입형 컬렉션으로 다룰 수 있는 기능을 제공함
    - 타입 안정성을 지원하므로 초기화에 사용한 클래스 대신 다른 클래스를 사용해 접근할 수 없음
    - 다수의 소프트웨어 엔지니어가 잘 정의된 인터페이스로 상호작용하는 대규모 애플리케이션을 개발하는 데 특히 유용함
- 구조적 스트리밍
  - 스파크 2.2 버전에서 안정화(production-ready)된 스트림 처리용 고수준 API
  - 구조적 API로 개발된 배치 모드의 연산을 스트리밍 방식으로 실행, 지연 시간을 줄이고 증분 처리
  - 배치 처리용 코드를 일부 수정하여 스트리밍 처리를 수행하고 값을 빠르게 얻을 수 있다는 장점
  - 프로토타입을 배치 잡으로 개발한 다음 스트리밍 잡으로 변환할 수 있으므로 개념 잡기가 수월함
  - 셔플 파티션 수 : 셔플 이후에 생성될 파티션 수, 기본값은 200이지만 로컬 모드에서는 그렇게 많은 익스큐터가 필요하지 않으므로 5로 변경
    - spark.conf.set("spark.sql.shuffle.partitions","5")
  - 스트리밍 코드
    - read 메서드 대신 readStream 메서드 사용
    - maxFilesPerTrigger : 한 번에 읽을 파일수를 설정함
  - MLlib : 대용량 데이터를 대상으로 전처리(preprocessing), 멍잉(munging - 데이터 랭글링(data wrangling), 원본 데이터를 다른 형태로 변환하거나 매핑하는 과정), 모델 학습(model training) 및 예측(prediction)
- 저수준 API
  - RDD를 통해 자바와 파이썬 객체를 다루는 데 필요한 다양한 기본 기능을 제공
  - 스파크의 거의 모든 기능은 RDD를 기반
  - 드라이버 시스템의 메모리에 저장된 원시 데이터를 병렬처리(parallelize)하는 데 RDD를 사용

# 구조적 API: DataFrame, SQL, Dataset
## 구조적 API 개요
- 구조적 API : 비정형 로그 파일로부터 반정형 csv파일, 매우 정형적인 파케이(Parquet) 파일까지 다양한 유형의 데이터를 처리
  - 세 가지 분산 컬렉션 API
    - Dataset : 타입형
    - DataFrame : 비타입형
    - SQL 테이블과 뷰
  - 배치작업을 스트리밍 작업으로 손쉽게 변환함
- 스파크는 트랜스포메이션의 처리 과정을 정의하는 분산 프로그래밍 모델
  - 사용자가 정의한 다수의 트랜스포메이션은 지향성 비순환 그래프(DAG)로 표현되는 명령을 만들어냄
  - 액션은 하나의 잡을 클러스터에서 실행하기 위해 스테이지와 태스크로 나누고 DAG 처리 프로세스를 실행함
  - 트랜스포메이션과 액션으로 다루는 논리적 구조가 바로 DataFrame과 Dataset
- 스키마 : 분산 컬렉션에 저장할 데이터 타입을 정의하는 방법
- DataFrame의 컬럼명과 데이터 타입을 정의함
- 카탈리스트(Catalyst) 엔진 : 실행 계획 수립과 처리에 사용하는 자체 데이터 정보를 가지고 있음, 다양한 실행 최적화 기능을 제공함
- DataFrame은 Row 타입으로 구성된 Dataset
  - Row 타입 : 스파크가 사용하는 연산에 최적화된 인메모리 포맷의 내부적인 표현 방식
    - 가비지 컬렉션(garbage collection)과 객체 초기화 부하가 있는 JVM 데이터 타입을 사용하는 대신 자체 데이터 포맷을 사용하기 때문에 매우 효율적인 연산이 가능함 
- 컬럼 : 정수형이나 문자열 같은 단순 데이터 타입, 배열이나 맵 같은 복합 데이터 타입, null 값을 표현함
- 로우 : 데이터 레코드(Dataframe의 각 로우는 하나의 레코드)
- 구조적 API의 실행 과정
  - 구조적 API 쿼리가 사용자 코드에서 실제 실행 코드로 변환되는 과정
    - DataFrame/Dataset/SQL을 이용해 코드를 작성함
    - 정상적인 코드라면 스파크가 논리적 실행 계획으로 변환함
    - 스파크는 논리적 실행 계획을 물리적 실행 계획으로 변환하며 그 과정에서 추가적인 최적화를 할 수 있는지 확인
    - 스파크는 클러스터에서 물리적 실행 계획(RDD 처리)을 실행
## 구조적 API 기본 연산
- DataFrame : Row 타입의 레코드(테이블의 로우 같은)와 각 레코드에 수행할 연산 표현식을 나타내는 여러 컬럼(스프레드시트의 컬럼 같은)으로 구성됨
- 스키마 : 각 컬러명과 데이터 타입을 정의함(DataFrame의 컬럼명과 데이터 타입을 정의함)
- 파티셔닝 : DataFrame이나 Dataset이 클러스터에서 물리적으로 배치되는 형태를 정의함
- 파티셔닝 스키마 : 파티션을 배치하는 방법을 정의함
- selectExpr 메서드 : 새로운 DataFrame을 생성하는 복잡한 표현식을 간단하게 만드는 도구
- 리터럴(literal) : 프로그래밍 언어의 리터럴값은 스파크가 이해할 수 있는 값으로 변환함
- 최적화 기법
  - 자주 필터링하는 컬럼을 기준으로 데이터를 분할함
  - 파티셔닝 스키마와 파티션 수를 포함해 클러스터 전반의 물리적 데이터 구성을 제어함
  - repartition 메서드 : 무조건 전체 데이터를 셔플함, 향후에 사용할 파티션 수가 현재 파티션 수보다 많거나 컬럼을 기준으로 파티션을 만드는 경우에만 사용
    - 특정 컬럼을 기준으로 자주 필터링한다면 자주 필터링되는 컬럼을 기준으로 파티션을 재분배하는것이 좋음
  - coalesce 메서드 : 전체 데이터를 셔플하지 않고 파티션을 병합하는 경우 사용
- taLocalIterator : 이터레이터(iterator,반복자)로 모든 파티션의 데이터 드라이버에 전달함 , 데이터셋의 파티션을 차례로 반복 처리함

## 다양한 데이터 타입 다루기
- DataFrame(Dataset) 메서드
  - Dataset의 하위 모듈은 다양한 메서드를 제공함
    - DataFrameStatFunctions : 다양한 통계적 함수를 제공
    - DataFrameNaFunctions : null 데이터를 다루는 데 필요한 함수를 제공
- Column 메서드
  - Column은 alias나 contains 같이 컬럼과 관련된 여러가지 메서드를 제공
  - org.apache.spark.sql.functions 패키지는 데이터 타입과 관련된 다양한 함수를 제공함
- 불리언 구문 : and,or,true,false
- initcap 함수 : 주어진 문자열에서 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경 
- 날짜와 타임스탬프 데이터
  - 날짜(date) : 달력 형태의 날짜
  - 타임스탬프(timestamp) : 날짜와 시간 정보를 모두 가짐
  - inferSchema 옵션이 활성화된 경우 날짜와 타임스탬프를 포함해 컬럼의 데이터 타입을 최대한 정확하게 식별하려 시도함
- null 값
  - 명시적으로 null 값을 제거, 전역 또는 컬럼 단위로 null 값을 특정 값으로 채워 넣음
  - coalesce 함수 : 인수로 지정한 여러 컬럼 중 null이 아닌 첫번 째 값을 반환함
- 사용자 정의함수
  - UDF(user defined function) : 파이썬이나 스칼라 그리고 외부 라이브러리를 사용해 사용자가 원하는 형태로 트랜스포메이션을 만들 수 있게 함 
  - 모든 워커 노드에서 생성된 함수를 사용할 수 있도록 스파크에 등록 -> 드라이버에서 함수를 직렬화하고 네트워크를 통해 모든 익스큐터 프로세스로 전달함
  - 스칼라나 자바 -> JVM 환경에서만 사용 가능
  - 워커 노드에서 파이썬 프로세스를 실행하고 파이썬이 이해할 수 있는 포맷으로 모든 데이터를 직렬화(JVM 사용 언어에도 존재했던 부분), 파이썬 프로세스에 있는 데이터의 로우마다 함수를 실행하고 마지막으로 JVM과 스파크에 처리 결과를 반환함
    - 직렬화에 큰 부하가 발생
    - 데이터가 파이썬으로 전달되면 스파크에서 워커 메모리를 관리할 수 없음, JVM과 파이썬이 동일한 머신에서 메모리 경합을 하면 자원에 제약이 생겨 워커가 비정상적으로 종료될 가능성이 있음

## 집계 연산
- count 메서드가 트랜스포메이션이 아닌 액션, 결과를 즉시 반환함, count메서드는 데이터셋의 전체 크기를 알아보는 용도로 사용하지만 메모리에 DataFrame 캐싱 작업을 수행하는 용도로 사용됨
- approx_count_distinct : 어느 정도 수준의 정확도를 가지는 근사치만으로도 유의미, 최대 추정 오류율(maximum estimation error), 대규모 데이터셋을 사용할 때 훨씬 더 좋아짐
- 비대칭도와 첨도(kurtosis)
  - 데이터의 변곡점(extreme point)을 측정하는 방법
  - 비대칭도(skewness)는 데이터 평균의 비대칭 정도를 측정하고, 첨도는 데이터 끝 부분을 측정함
  - 확률변수(random variable)의 확률 분포(probability distribution)로 데이터를 모델링할 때 특히 중요함
- 복합 데이터 타입의 집계
  - collect_list() : 특정 컬럼의 값을리스트로 수집
  - collect_set() : 셋 데이터 타입으로 고윳값만 수집 
- 윈도우 함수
  - 랭크 함수(ranking function)
  - 분석 함수(analytic function)
  - 집계 함수(aggregate function)
- 사용자 정의 집계 함수(user-defined aggregation function, UDAF)
  - 직접 제작한 함수나 비즈니스 규칙에 기반을 둔 자체 집계 함수를 정의하는 방법
  - 입력 데이터 그룹에 직접 개발한 연산을 수행
  - 입력 데이터의 모든 그룹의 중간 결과를 단일 AggregationBuffer에 저장해 관리함

## 조인
- 스파크의 조인 수행 방식
  - 노드간 네트워크 통신 전략
    - 스파크는 조인 시 두가지 클러스터 통신 방식을 활용함
    - 전체 노드간 통신을 유발 셔플 조인(shuffle join), 
    - 그렇지 않은 브로드캐스트 조인(broadcast join) , 작은 DataFrame을 클러스터의 전체 워커 노드에 복제하는 것, 대구모 노드 간 통신이 발생하지만 그 이후로는 노드 사이에 추가적인 통신이 발생하지 않음, 모든 단일 노드에서 개별적으로 조인이 수행되므로 CPU가 가장 큰 병목 구간, 너무 큰 데이터를 브로드캐스트하면 고비용의 수집 연산이 발생하므로 드라이버 노드가 비정상적으로 종료됨
    - 내부 최적화 기술은 시간이 흘러 비용 기반 옵티마이저(cost-based optimizer, CBO)가 개선되고 더 나은 통신 전략이 도입되는 경우 바뀔 수 있음
  - 노드별 연산 전략

## 데이터소스
- 스파크의 핵심 데이터소스
  - CSV, JSON, 파케이, ORC, JDBC/ODBC 연결, 일반 텍스트 파일
- 데이터 소스
  - 카산드라, HBase, 몽고디비, AWS Redshift, XML, 기타 수많은 데이터소스
- 스파크에서 데이터를 읽을 때는 기본적으로 DataFrameReader를 사용
  - 포맷
  - 스키마
  - 읽기 모드
  - 옵션 
- 읽기 모드 : 스파크가 형식에 맞지 않는 데이터를 만났을 때의 동작 방식을 지정하는 옵션
  - permissive : 오류 레코드의 모든 필드를 null로 설정하고 모든 오류 레코드를 _corrupt_record라는 문자열 컬럼에 기록함
  - dropMalformed : 형식에 맞지 않는 레코드가 포함된 로우를 제거함
  - failFast : 형식에 맞지 않는 레코드를 만나면 즉시 종료함
- 쓰기 API 구조
  - DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()
- 저장 모드 : 스파크가 지정된 위치에서 동일한 파일이 발견했을 때의 동작 방식을 지정하는 옵션
  - append : 해당 경로에 이미 존재하는 파일 목록에 결과 파일을 추가함
  - overwrite : 이미 존재하는 모든 데이터를 완전히 덮어씀
  - errorIfExists : 해당 경로에 데이터나 파일이 존재하는 경우 오류를 발생시키면서 쓰기 작업이 실패함
  - ignore : 해당 경로에 데이터나 파일이 존재하는 경우 아무런 처리도 하지 않음
- CVS 파일(comma-separated values)
  - 콤마(,)로 구분된 값을 의미
  - 각 줄이 단일 레코드가 되며 레코드의 각 필드를 콤마로 구분하는 일반적인 텍스트 파일 포맷
- JSON 파일
  - 자바스크립트 세상에서 온 파일 형식, 자바스크립트 객체 표기법(JSON, JavaScript Object Notation)
  - 줄로 구분된 JSON을 기본적으로 사용함
  - multiLine 옵션을 사용해 줄로 구분된 방식과 여러 줄로 구성된 방식을 선택적으로 사용
  - 줄로 구분된 JSON이 인기있는 이유는 구조화되어 있고, 최소한의 기본 데이터 타입이 존재하기 때문
- 파케이 파일
  - 다양한 스토리지 최적화 기술을 제공하는 오픈소스로 만들어진 컬럼 기반의 데이터 저장 방식, 분석 워크로드에 최적화
  - 저장소 공간을 절약할 수 있고 전체 파일을 읽는 대신 개별 컬럼을 읽을 수 있으며, 컬럼 기반의 압축 기능을 제공함
- ORC 파일
  - 하둡 워크로드를 위해 설계된 자기 기술적(self-describing)이며 데이터 타입을 인식할 수 있는 컬럼 기반의 파일 포맷
  - 대규모 스트리밍 읽기에 최적화되어 있을 뿐만 아니라 필요한 로우를 신속하게 찾아낼 수 있는 기능이 통합되어 있음
- SQL 데이터베이스
  - 데이터베이스의 데이터를 읽고 쓰기 위해서는 스파크 클래스패스(classpath)에 데이터베이스의 JDBC(Java DataBase Connectivity) 드라이버를 추가하고 적절한 JDBC 드라이버 jar 파일을 제공함 
  -JDBC 데이터 소스 옵션
    - url : 접속을 위한 JDBC URL
    - dbtable : 읽을 JDBC 테이블을 설정함
    - driver : 지정한 URL에 접속할 때 사용할 JDBC 드라이버 클래스명을 지정함
    - partitionColumn, lowerBound, upperBound : numPartitions도 반드시 지정, 다수의 워커에서 병렬로 테이블을 나눠 읽는 방법을 정의함, partitionColumn은 반드시 해당 테이블의 수치형 컬럼이여야 함, lowerBound와 upperBound는 테이블의 로우를 필터링하는 데 사용되는 것이 아니라 각 파티션의 범위를 결정하는 데 사용함, 테이블의 모든 로우는 분할되어 반환됨
    - numPartitions : 테이블의 데이터를 병렬로 읽거나 쓰기 작업에 사용할 수 있는 최대 파티션 수를 결정함
    - fetchsize : 한 번에 얼마나 많은 로우를 가져올지 결정하는 JDBC의 패치 크기를 결정함
    - batchsize : 한 번에 얼마나 많은 로우를 저장할지 결정하는 JDBC의 배치 크기를 설정함
    - isolationLevel : 현재 연결에 적용되는 트랜잭션 격리 수준을 정의함
    - truncate : JDBC writer 관련 옵션
    - createTableOptions : 테이블 생성 시 특정 테이블의 데이터베이스와 파티션 옵션을 설정할 수 있음
    - createTableColumnTypes : 테이블을 생성할 때 기본값 대신 사용할 데이터베이스 컬럼 데이터 타입을 정의함
- 데이터베이스 병렬로 읽기
  - 스파크는 파일 크기, 파일 유형 긔고 압축 방식에 따른 분할 가능성에 따라 여러 파일을 읽어 하나의 파티션으로 만들거나 여러 파티션을 하나의 파일로 만드는 기본 알고리즘을 가지오 있음
  - numPartitions 옵션을 사용해 읽기 및 쓰기용 동시 작업 수를 제한할 수 있는 최대 파티션 수를 설정할 수 있음
- 슬라이딩 윈도우 기반의 파티셔닝
  - 스파크는 데이터베이스에 병렬로 쿼리를 요청하며 numPartitions에 설정된 값만큼 파티션을 반환함, 파티션에 값을 할당하기 위해 상한값과 하한값을 수정함
- 고급 I/O 개념
  - 쓰기 작업 전에 파티션 수를 조절함으로써 병렬로 처리할 파일 수를 제어
  - 버켓팅과 파티셔닝을 조절함으로써 데이터의 저장 구조를 제어함
  - 분할 가능한 파일 타입과 압축 방식
    - 특정 파일 포맷은 기본적으로 분할을 지원함
    - 스파크에서 전체 파일이 아닌 쿼리에 필요한 부분만 읽을 수 있으므로 성능 향상에 도움이 됨
    - 하둡 분산 파일 시스템(HDFS) 같은 시스템을 사용한다면 분할된 파일을 여러 블록으로 나누어 분산 저장하기 때문에 훨씬 더 최적화
  - 병렬로 데이터 읽기
    - 사용 가능한 익스큐터를 이용해 병렬(익스큐터 수를 넘어가는 파일은 처리 중인 파일이 완료될 때까지 대기)로 파일을 읽음
  - 병렬로 데이터 쓰기
    - 파일이나 데이터 수는 데이터를 쓰는 시점에 DataFrame이 가진 파티션 수에 따라 달라질 수 있음
    - 기본적으로 데이터 파티션당 하나의 파일이 작성됨
- 파티셔닝
  - 파티셔닝은 어떤 데이터를 어디에 저장할 것인지 제어할 수 있는 기능
  - 파티셔닝된 디렉터리 또는 테이블에 파일을 쓸 때 디렉터리별로 컬럼 데이터를 인코딩해 저장함
  - 데이터를 읽을 때 전체 데이터셋을 스캔하지 않고 필요한 컬럼의 데이터만 읽을 수 있음
- 버켓팅(bucketing)
  - 각 파일에 저장된 데이터를 제어할 수 있는 또 다른 파일 조직화 기법
  - 동일한 버킷 ID를 가진 데이터가 하나의 물리적 파티션에 모두 모여있기 때문에 데이터를 읽을 때 셔플을 피할 수 있음
  - 데이터가 이후의 사용 방식에 맞춰 사전에 파티셔닝되므로 조인이나 집계 시 발생하는 고비용의 셔플을 피할 수 있음
  - csvFile.write.format("parquet").mode("overwrite").bucketBy(numberBuckets(10), columToBucketBy("count)).saveAsTable("bucketedFiles")
- 파일 크기 관리
  - 작은 파일을 많이 생성하면 메타데이터에 엄청난 관리 부하가 발생함 -> 작은 크기의 파일 문제
  - 몇 개의 로우가 필요하더라도 전체 데이터 블록을 읽어야 하기 때문에 비효율적, 너무 큰 파일도 좋지 않음
  - maxRecordsPerFile 옵션에 파일당 레코드 수를 지정, 각 파일에 기록될 레코드를 수를 조절할 수 있으므로 파일 크기를 더 효과적으로 제어함

## 스파크 SQL
- 스파크 SQL을 사용해 데이터베이스에 생성된 뷰(view)나 테이블에 SQL쿼리를 실행함
- 시스템 함수를 사용하거나 사용자 정의 함수를 정의
- 스파크 SQL은 DataFrame과 Dataset API에 통합, 데이터 변환 시 SQL과 DataFrame의 기능을 모두 사용할 수 있으며 두 방식 모두 동일한 실행 코드로 컴파일됨
- SQL
  - SQL 또는 구조적 질의 언어(Structured Query Language)는 데이터에 대한 관계형 연산을 표현하기 위한 도메인 특화 언어
  - 스파크가 등장하기 전에는 하이브(Hive)가 빅데이터 SQL 접근 계층에서 사실상의 표준
  - 스파크2.0 버전에는 하이브를 지원할 수 있는 상위 호환 기능으로 ANSI-SQL과 HiveQL을 모두 지원하는 자체 개발된 SQL 파서가 포함됨
  - SQL 분석가들은 쓰르피트 서버(Thrift Server)나 SQL 인터페이스에 접속해 스파크의 연산 능력을 활용함
  - 전체 데이터 처리 파이프라인에 스파크 SQL을 사용함
  - 통합형 API는 SQL로 데이터를 조회하고 DataFrame으로 변환한 다음 스파크의 MLlib이 제공하는 대규모 머신러닝 알고리즘 중 하나를 사용해 수행한 결과를 다른 데이터소스에 저장하는 전체 과정을 가능하게 만듬
  - 스파크 SQL은 온라인 트랜잭션 처리(online transaction processing, OLTP) 데이터베이스가 아닌 온라인 분석용(online analytic processing,OLAP) 데이터베이스로 동작함, 매우 낮은 지연 시간이 필요한 쿼리를 수행하기 위한 용도로 사용할 수 없음
  - 스파크 SQL CLI(Command Line Interface - 명령행 인터페이스)는 로컬 환경의 명령행에서 기본 스파크 SQL 쿼리를 실행할 수 있는 편리한 도구
  - 다른 트랜스 포메이션과 마찬가지로 즉시 실행되지 않고 지연 처리됨, DataFrame을 사용하는 것보다 SQL 코드로 표현하기 훨씬 쉬운 트랜스포메이션이기 때문에 엄청나게 강력한 인터페이스
  - 함수에 여러 줄로 구성된 문자열을 전달할 수 있으므로 여러 줄로 구성된 쿼리를 아주 간단히 표현함
  - 자바 데이터베이스 연결(Java Database Connectivity, JDBC) 인터페이스를 제공함
  - 카탈로그
    - 테이블에 저장된 데이터에 대한 메타데이터뿐만 아니라 데이터베이스, 테이블, 함수 그리고 뷰에 대한 정보를 추상화함
    - 테이블, 데이터베이스 그리고 함수를 조회하는 등 여러 가지 유용한 함수를 제공함
- 스파크 관리형 테이블
  - 메타데이터 : 테이블의 데이터와 테이블에 대한 데이터
  - 스파크는 데이터뿐만 아니라 파일에 대한 메타데이터를 관리함
- 뷰
  - 기존 테이블에 여러 트랜스포메이션 작업을 지정함
  - 뷰를 사용하면 쿼리 로직을 체계화하거나 재사용하기 편하게 만들 수 있음
  - 임시 뷰 : 테이블처럼 데이터베이스에 등록되지 않고 현재 세션에서만 사용할 수 있음
  - 전역적 임시 뷰(global temp view) : 데이터베이스에 상관없이 사용할 수 있으므로 전체 스파크 애플리케이션에서 볼 수 있음, 세션이 종료되면 뷰도 사라짐
- 데이터베이스 : 여러 테이블을 조직화하기 위한 도구
- 서브쿼리
  - 상호 연관 서브쿼리(correlated subquery) : 서브쿼리의 정보를 보완하기 위해 쿼리의 외부 범위에 있는 일부 정보를 사용할 수 있음
  - 비상호 연관 서브쿼리(uncorrelated subquery) : 외부 범위에 있는 정보를 사용하지 않음
  - 스파크는 값에 따라 필터링할 수 있는 조건절 서브쿼리(predicate subquery)도 지원함

## Dataset
- Dataset은 구조적 API의 기본 데이터 타입, 고수준의 구조적 API와 저수준 RDD API가 조합된 형태
- DataFrame은 Row 타입의 Dataset
- Dataset은 자바 가상 머신을 사용하는 언어인 스칼라와 자바에서만 사용할 수 있음, Dataset을 사용해 데이터셋의 각 로우를 구성하는 객체를 정의함
- 인코더 : 도메인별 특정 객체 T를 스파크의 내부 데이터 타입으로 매핑하는 시스템을 의미함
- Dataset을 사용할 시기
  - DataFrame 기능만으로는 수행할 연산을 표현할 수 없는 경우
  - 성능 저하를 감수하더라도 타입 안정성(type-safe)을 가진 데이터 타입을 사용하고 싶은 경우
  - 복잡한 비즈니스 로직을 SQL이나 DataFrame 대신 단일 함수로 인코딩해야 하는 경우
  - 타입이 유효하지 않은 작업은 런타임이 아닌 컴파일 타임에 오류가 발생함
  - Dataset API를 사용하면 잘못된 데이터로부터 애플리케이션을 보호할 수 는 없지만 보다 우하하게 데이터를 제어하고 구조화 함
  - 단일 노드의 워크로드와 스파크 워크로드에서 전체 로우에 대한 다양한 트랜스포메이션을 재사용하려면 Dataset을 사용하는 것이 적합함
  - Dataset을 사용하는 장점 중 하나는 로컬과 분산 환경의 워크로드에서 재사용, 케이스 클래스로 구현된 데이터 타입을 사용해 모든 데이터와 트랜스포메이션을 정의하면 재사용할 수 있음
  - 올바른 클래스와 데이터 타입이 지정된 DataFrame을 로컬 디스크에 저장하면 다음 처리 과정에서 사용할 수 있어 더 쉽게 데이터를 다룰 수 있음
- Dataset 생성
  - 자바:Encoders
    - 자바 인코더(Encoders) : 데이터 타입 클래스를 정의한 다음 DataFrame(Dataset<ROW> 타입의)에 지정해 인코딩
  - 스칼라: 케이스 클래스
    - 스칼라에서 Dataset을 생성하려면 스칼라 case class 구문을 사용해 데이터 타입을 정의해야 함
    - 케이스 클래스의 특징 및 정규 클래스(regular class)
      - 불변성
      - 패턴 매칭으로 분해 가능
      - 참조값 대신 클래스 구조를 기반으로 비교
      - 사용하기 쉽고 다루기 편함
    - 케이스 클래스는 불변성을 가지며 값 대신 구조로 비교할 수 있음
      - 불변성이므로 객체들이 언제 어디서 변경되었는지 추적할 필요가 없음
      - 값으로 비교하면 인스턴스를 마치 원시(primitive) 데이터 타입의 값처럼 비교함, 클래스 인스턴스가 값으로 비교되는지, 참조로 비교되는지 더는 불확실해하지 않아도 됨
      - 패턴 매칭은 로직 분기를 단순화해 버그를 줄이고 가독성을 좋게 만듬
