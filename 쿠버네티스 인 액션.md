# 서평
- 이 책은 크게 3부분으로 구성되어 있습니다. 도커와 쿠버네티스를 간략하게 소개하고 쿠버네티스 클러스터를 설치하는 방법과 간단한 애플리케이션을 실행하는 방법, 쿠버네티스에서 애플리케이션을 실행하기 위해 반드시 이해해야 하는 핵심 개념, 쿠버네티스 클러스터의 내부를 깊이 들여다보며 몇 가지 추가적인 개념을 소개로 구성되어 있습니다. 

# 쿠버네티스 소개
- 쿠버네티스는 조종사, 조타수(선박의 핸들을 잡고 있는 사람)를 뜻하는 그리스어 
- 쿠버네티스 등장 이유 : 구성 요소의 서버 배포를 자동으로 스케줄링하고 구성,관리,장애 처리를 포함하는 자동화가 필요하여 
- 개발자가 운영 팀의 도움 없이도 자신의 애플리케이션을 원하는 만큼 자주 배포할 수 있음. 하드웨어 장애 발생 시 해당 애플리케이션을 자동으로 모니터링하고 스케줄링을 조정해 운영 팀을 도와줌 
- 하드웨어 인프라를 추상화하고 데이터 센터 전체를 하나의 거대한 컴퓨팅 리소스를 제공함. 여러 애플리케이션 구성 요소를 배포할 때 각 구성 요소 서버를 선택하고 배포하며 애플리케이션의 다른 구성 요소를 쉽게 찾고 통신할 수 있게 함 
- 개발자가 모든 유형의 애플리케이션을 배포하고 실행할 수 있는 간단한 플랫폼을 제공할 수 있을 뿐만 아니라 클라우드 공급자의 시스템 관리자가 자신들의 하드웨어에서 실행되는 수만 개의 애플리케이션을 일일이 알 필요가 없게 함 
- 쿠버네티스와 같은 시스템이 필요한 이유
  - 모놀리스 애플리케이션에서 마이크로서비스로 전환
    - 시스템의 증가하는 부하를 처리하려고 CPU, 메모리, 그 밖의 서버 구성 요소를 추가해 서버를 수직 확장(scale up)하거나 서버를 추가하고 애플리케이션의 복사본(또는 복제본)을 실행해 전체 시스템을 수평 확장(scale out)해야 함 
      - 수직 확장은 일반적으로 애플리케이션을 변경할 필요가 없지만 비교적 비용이 많이 들고 실제로는 확장에 한계(상한)가 있음. 
      - 수평 확장은 상대적으로 저렴하지만 애플리케이션 코드의 큰 변경이 필요할 수 있으며 항상 가능한 것도 아님(관계형 데이터 베이스) 
    - 마이크로서비스로 애플리케이션 분할 
      - 마이크로서비스는 일반적으로 RESTful(Repressentaional State Transfer) API를 제공하는 HTTP와 같은 동기 프로토콜과 AMQP(Advanced Message Queuing Protocl)와 같은 비동기 프로토콜로 통신함 
  - 애플리케이션에 일관된 환경 제공 
  - 지속적인 배포로 전환: 데브옵스와 노옵스
    - 데브옵스 : 개발자, 품질 보증(QA), 운영 팀이 전체 프로세스에서 협업해야 함
    - 노옵스 : 자동화로 운영 팀의 손이 거의 필요 없는 환경
      - 하드웨어 인프라를 전혀 알지 못하더라도 운영 팀을 거치지 않고 개발자가 애플리케이션을 직접 배포하는 방식이 가장 이상적 
    - 쿠버네티스를 사용하면 하드웨어를 추상화하고 이를 애플리케이션에 배포, 실행을 위한 플랫폼으로 제공함으로써 개발자는 시스템 관리자의 도움없이도 애플리케이션을 구성, 배포할 수 있으며 시스템 관리자는 실제 실행되는 애플리케이션을 알 필요 없이 인프라를 유지하고 운영하는 데 집중할 수 있음 
- 컨테이너 기술 소개 
  - 베어메탈 머신 : 어떤 소프트웨어도 설치돼 있지 않은 하드웨어 자체를 의미하지만, 클라우드가 보편화되면서 가상화되지 않은 머신을 의미함 
  - 컨테이너 격리를 가능하게 하는 메커니즘
    - 리눅스 네임스페이스로 각 프로세스가 시스템(파일, 프로세스, 네트워크 인터페이스, 호스트 이름 등)에 대한 독립된 뷰만 볼 수 있도록 함.
    - 리눅스 컨트롤 그룹(cgroups)으로, 프로세스가 사용할 수 있는 리소스(CPU, 메모리, 네트워크 대역폭 등)의 양을 제한함 
  - 리눅스 네임스페이스로 프로세스 격리
    - 네임스페이스의 종류
      - 마운트(mnt)
      - 프로세스 ID(pid)
      - 네트워크(net)
      - 프로세스 간 통신(ipc) 
      - 호스트와 도메인 이름(uts)
      - 사용자 ID(user)
  - 도커 컨테이너 플랫폼
    - 도커 기반 컨테이너 이미지와 가상머신 이미지의 큰 차이점은 컨테이너 이미지가 여러 이미지에서 공유되고 재사용될 수 있는 레이어로 구성돼 있다는 것 
    - 동일한 레이어를 포함하는 다른 컨테이너 이미지를 실행할 때 다른 레이어가 이미 다운로드 된 경우 이미지의 특정 레이어만 다운로드 하면 됨 
    - 도커 개념 
      - 도커는 애플리케이션을 패키징, 배포, 실행하기 위한 플랫폼 
      - 애플리케이션을 전체 환경과 함께 패키지화할 수 있음. 애플리케이션에서 필요한 몇 가지 라이브러리나 운영체제의 파일시스템에 설치되는 모든 파일을 포함시킬 수 있음 
      - 이미지 : 애플리케이션과 해당 환경을 패키지화한 것 
      - 레지스트리 : 도커 이미지를 저장하고 다른 사람이나 컴퓨터 간에 해당 이미지를 쉽게 공유할 수 있는 저장소 
      - 컨테이너 : 도커 기반 컨테이너 이미지에서 생성된 일반적인 리눅스 컨테이너 
  - 쿠버네티스 
    - 쿠버네티스 클러스터 아키텍처
      - 마스터 노드는 전체 쿠버네티스 시스템을 제어하고 관리하는 쿠버네티스 컨트롤 플레인을 실행함 
      - 워커 노드는 실제 배포되는 컨테이너 애플리케이션을 실행함 
      - 컨트롤 플레인(Control Plane)
        - 클러스터를 제어하고 작동시킴
        - 구성 요소 
          - 쿠버네티스 API 서버는 사용자, 컨트롤 플레인 구성 요소와 통신함. 
          - 스케줄러는 애플리케이션의 배포를 담당함(애플리케이션의 배포 가능한 각 구성 요소를 워크 노드에 할당) 
          - 컨트롤러 매니저는 구성 요소 복제본, 워커 노드 추적, 노드 장애 처리 등과 같은 클러스터단의 기능을 수행함 
          - Etcd는 클러스터 구성을 지속적으로 저장하는 신뢰할 수 있는 분산 데이터 저장소 
      - 노드 
        - 워커 노드는 컨테이너화된 애플리케이션을 실행하는 시스템 
        - 구성 요소 
          - 컨테이너를 실행하는 도커, rkt 또는 다른 컨테이너 런타임
          - API 서버와 통신하고 노드의 컨테이너를 관리하는 Kubelet 
          - 애플리케이션 구성 요소 간에 네트워크 트래픽을 로드밸런싱하는 쿠버네티스 서비스  프록시(kube-proxy) 
    - 쿠버네티스 사용의 장점
      - 애플리케이션 배포의 단순화
      - 하드웨어 활용도 높이기 
      - 상태 확인과 자가 치유 
      - 오토스케일링
      - 애플리케이션 개발 단순화 

# 도커와 쿠버네티스 첫걸음 
- Minikube는 로컬에서 쿠버네티스를 테스트하고 애플리케이션을 개발하는 목적으로 단일 노드 클러스터를 설치하는 도구 
- 구글 쿠버네티스 엔진을 활용한 관리형 쿠버네티스 클러스터 사용하기 
  - > kubectl describe node gke-kubia-85f6-node-0rrx
    - 오브젝트 세부 정보 가져오기, 출력 결과는 CPU와 메모리, 시스템 정보, 노드에 실행 중인 컨테이너 등을 포함한 노드 상태를 보여줌 
  - kubectl의 alias와 명령줄 자동완성 설정하기
    - ~/.bashrc나 이에 준하는 파일에 추가함. 
    - > alias k=kubectl
- 쿠버네티스 첫 번째 애플리케이션 실행하기 
  - 파드 소개 
    - 파드는 하나 이상의 밀접하게 연관된 컨테이너의 그룹으로 같은 워커 노드에서 같은 리눅스 네임스페이스로 함께 실행됨 
    - 각 파드는 자체 IP, 호스트 이름, 프로세스 등이 있는 논리적으로 분리된 머신 
    - > kubectl get pods : 파드 조회하기 
    - 스케줄링(Scheduling)이라는 용어는 파드가 특정 노드에 할당됨을 의미함. 파드는 즉시 실행됨. 스케줄링이라는 용어의 의미처럼 미래의 특정 시간에 실행됨을 의미하는 것이 아님 
  - 서비스 오브젝트 생성하기 
    - 쿠버네티스에게 앞서 생성한 레플리케이션컨트롤러를 노출하도록 명령함 
    - > kubectl expose rc kubia --type=LoadBalancer --name kubia-http 
- 시스템의 논리적인 부분
  - 레플리케이션컨트롤러의 역할 이해 
    - kubia 레플리케이션 컨트롤러는 항상 정확히 하나의 파드 인스턴스를 실행하도록 지정함 
    - 레플리케이션컨트롤러는 파드를 복제(즉, 여러 개의 파드 복제본을 생성)하고 항상 실행 상태로 만듬 
  - 서비스가 필요한 이유 
    - 파드는 일시적(ephemeral)임. 파드는 언제든 사라질 수 있음. 파드가 실행 중인 노드가 실패할 수도 있고 누군가 파드를 삭제할 수도 있고, 비정상 노드에서 파드가 제거될 수도 있음 
    - 사라진 파드는 레플리케이션컨트롤러에 의해 생성된 파드로 대체됨. 새로운 파드는 다른 IP 주소를 할당받음. 이것이 서비스가 필요한 이유 
- 애플리케이션 수평 확장
  - > kubectl scale rc kubia --replicas=3 
  - 파드의 레플리카 수를 늘리려면 레플리카 컨트롤러에서 의도하는 Desired 레프리카 수를 변경해야 함 
- 애플리케이션이 실행 중인 노드 검사하기 
  - -o wide 옵션을 사용하면 추가 열을 요청할 수 있음. 파드를 조회할 때 이 옵션은 파드 IP와 파드가 실행중인 노드를 표시함 
  - > kubectl get pods -o wide 
- 쿠버네티스 대시보드 소개 
  - 대시보드에서 파드, 레플리케이션컨트롤러, 서비스 같은 클러스터의 많은 오브젝트를 생성할 수 있고 생성, 수정, 삭제 또한 가능함 
  - > kubectl cluster-info | grep dashboard : 대시보드 URL을 찾을 수 있음 
  - > gcloud container clusters describe kubia | grep -E "(username|password):" : 사용자 이름과 암호를 알 수 있음 
  - > minikube dashboard : Minikube를 사용할 때 대시보드 접근하기 

# 파드: 쿠버네티스에서 컨테이너 실행 
- 파드
  - 파드는 함께 배치된 컨테이너 그룹이며 쿠버네티스의 기본 빌딩 블록임 
  - 파드 안에 있는 모든 컨테이너는 같은 노드에서 실행됨 
  - 파드 이해하기
    - 여러 프로세스를 단일 컨테이너로 묶지 않기 때문에, 컨테이너를 함께 묶고 하나의 단위로 관리할 수 있는 또 다른 상위 구조가 필요함 
    - 컨테이너 모음을 사용해 밀접하게 연관된 프로세스를 함께 실행하고 단일 컨테이너 안에서 모두 함께 실행되는 것처럼(거의) 동일한 환경을 제공할 수 있으면서도 이들을 격리된 상태로 유지할 수 있음 
    - 파드 안의 컨테이너가 동일한 네트워크 네임스페이스에서 실행되기 때문에, 동일한 IP주소와 포트 공간을 공유한다는 것 
  - 파드 간 플랫 네트워크 
    - 쿠버네티스 클러스터의 모든 파드는 하나의 플랫(flat)한 공유 네트워크 주소 공간에 상주하므로 모든 파드는 다른 파드의 IP 주소를 사용해 접근하는 것이 가능함 
  - 파드에서 컨테이너의 적절한 구성
    - 다계층 애플리케이션을 여러 파드로 분할
    - 개별 확장이 가능하도록 여러 파드로 분할 
    - 파드에서 여러 컨테이너를 사용하는 경우 
- YAML 또는 JSON 디스크크립터로 파드 생성 
  - 파드를 포함한 다른 쿠버네티스 리소스는 일반적으로 쿠버네티스 REST API 엔드포인트에 JSON 혹은 YAML 매니페스트를 전송해 생성함 
  - 배포된 파드의 전체 YAML
    - YAML이 디스크립터에서 사용한 쿠버네티스 API버전
    - 쿠버네티스 오브젝트/리소스 유형 
    - 파드 메타데이터(이름, 레이블, 어노테이션 등) 
    - 파드 정의/내용(파드 컨테이너 목록, 볼륨 등) 
    - 파드와 그 안의 여러 컨테이너의 상세한 상태 
  - 파드를 정의하는 주요 부분 소개 
    - Metadata: 이름, 네임스페이스, 레이블 및 파드에 관한 기타 정보를 포함함 
    - Spec : 파드 컨테이너, 볼륨, 기타 데이터 등 파드 자체에 관한 실제 명세를 가짐 
    - Status: 파드 상태, 각 컨테이너 설명과 상태, 파드 내부 IP, 기타 기본 정보 등 현재 실행 중인 파드에 관한 현재 정보를 포함함 
  - status 부분에는 특정 시간의 리소스 상태를 보여주는 읽기 전용의 런타임 데이터가 포함돼 있음. 새 파드를 만들 때 status 부분은 작성할 필요가 없음 
  - > kubectl create -f : YAML 또는 JSON 파일로 (파드뿐만 아니라) 리소스를 만드는데 사용함 
  - > kubectl got pod {} -o yaml : 파드의 전체 정의를 볼 수 있음 
  - > kubectl get pods : 파드를 조회해 상태를 확인함 
  - > kubectl logs {} : 애플리케이션 로그 보기(마지막으로 순환된 로그 항목만 보여줌) 
  - 컨테이너 이름을 지정해 다중 컨테이너 파드에서 로그 가져오기 
    - 여러 컨테이너를 포함한 파드인 경우에는, 컨테이너 이름을 kubectl logs 명령에 -c <컨테이너 이름> 옵션과 함께 명시적으로 포함함 
    - > kubectl logs kubia-manual -c kubia 
- 레이블을 이용한 파드 구성 
  - 마이크로서비스 아키텍처의 경우 배포된 마이크로서비스의 수는 매우 쉽게 20개를 초과함. 이러한 구성 요소는 복제돼(동일한 구성 요소의 여러 복사본이 배포됨) 여러 버전 혹은 릴리스(안정,베타,카나리 등)가 동시에 실행됨 
  - 레이블을 통해 파드와 기타 다른 쿠버네티스 오브젝트의 조직화가 이뤄짐 
  - 레이블 소개 
    - 레이블은 파드와 모든 다른 쿠버네티스 리소스를 조직화할 수 있는 단순하면서 강력한 쿠버네티스 기능
    - 레이블은 리소스에 첨부하는 키-값 쌍으로, 이 쌍은 레이블 셀럭터를 사용해 리소스를 선택할 때 활용됨
      - app : 파드가 속한 애플리케이션, 구성 요소 혹은 마이크로서비스를 지정함 
      - rel : 파드에서 실행 중인 애플리케이션이 안정, 베타 혹은 카나리 릴리스인지 보여줌(카나리 릴리스는 안정 버전 옆에 새 버전을 배포하고, 모든 사용자에게 배포하기 전에 소수의 사용자만이 새로운 버전을 사용할 수 있도록 해서 어떻게 동작하는지 볼 수 있게 하는 것)
- 파드에 어노테이션 달기 
  - 파드 및 다른 오브젝트는 레이블 외에 어노테이션(annotations)을 가질 수 있음. 어노테이션은 키-값 쌍으로 레이블과 거의 비슷하지만 식별 정보를 갖지 않음 
  - 어노테이션이 유용하게 사용되는 경우는 파드나 다른 API 오브젝트에 설명을 추가해 두는 것. 클러스터를 사용하는 모든 사람이 개별 오브젝트에 관한 정보를 신속하게 찾아볼 수 있음 
- 네임스페이스를 사용한 리소스 그룹화 
  - 오브젝트를 겹치지 않는 그룹으로 분할하고자 할 때 한 번에 하나의 그룹 안에서만 작업하고 싶을 것. 쿠버네티스는 오브젝트를 네임스페이스로 그룹화함 
  - 쿠버네티스 네임스페이스는 오브젝트 이름의 범위를 제공함. 모든 리소스를 하나의 단일 네임스페이스에 두는 대신에 여러 네임스페이스로 분할할 수 있으며, 분리된 네임스페이스는 같은 리소스 이름을 다른 네임스페이스에 걸쳐 여러 번 사용할 수 있게 해줌 
  - 네임스페이스의 필요성 
    - 여러 네임스페이스를 사용하면 많은 구성 요소를 가진 복잡한 시스템을 좀 더 작은 개별 그룹으로 분리할 수 있음 
    - 멀티테넌트(multi-tenant)환경처럼 리소스를 분리하는 데 사용됨
  - 네임스페이스가 제공하는 격리 
    - 네임스페이스를 사용하면 오브젝트를 별도 그룹으로 분리해 특정한 네임스페이스 안에 속한 리소스를 대상으로 작업할 수 있게 해주지만, 실행 중인 오브젝트에 대한 격리는 제공하지 않음 

# 레플리케이션과 그 밖의 컨트롤러: 관리되는 파드 배포 
- 레플리케이션컨트롤러 또는 디플로이먼트와 같은 유형의 리소스를 생성해 실제 파드를 생성하고 관리함 
- 파드를 안정적으로 유지하기 
  - 쿠버네티스에 컨테이너 목록을 제공하면 해당 컨테이너를 클러스터 어딘가에서 계속 실행되도록 할 수 있다는 것 
  - 파드 리소스를 생성하고 쿠버네티스가 이 파드를 실행할 워커 노드를 지정하며 해당 노드에서 파드의 컨테이너가 실행되도록 함으로써 이 작업을 수행함 
  - 컨테이너의 주 프로세스에 크래시(Crash)가 발생하면 Kubelet이 컨테이너를 다시 시작함 
  - 라이브니스 프로브 소개 
    - 라이브니스 프로브(liveness probe)를 통해 컨테이너가 살아 있는지 확인할 수 있음. 파드의 스펙(specification)에 각 컨테이너의 라이브니스 프로브를 지정할 수 있음. 쿠버네티스는 주기적으로 프로브를 실행하고 프로브가 실패할 경우 컨테이너를 다시 시작함 
      - HTTP GET 프로브는 지정한 IP 주소, 포트, 경로에 HTTP GET 요청을 수행함 
      - TCP 소켓 프로브는 컨테이너의 지정된 포트에 TCP 연결을 시도함 
      - Exec 프로브는 컨테이너 내의 임의의 명령을 실행하고 명령의 종료 상태 코드를 확인함 
    - > kubectl logs mypod --previous : 이전 컨테이너가 종료된 이유를 파악하려는 경우 이전 컨테이너의 로그를 확인
    - 컨테이너가 종료되면 완전히 새로운 컨테이너가 생성됨. 동일한 컨테이너가 다시 시작되는 것이 아님 
  - 라이브니스 프로브의 추가 속성 설정 
    - kubectl describe 
      - Liveness: http-get gttp://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3 
      - 명시적으로 지정한 라이브니스 프로브 옵션 외에도 지연(delay), 제한 시간(timeout), 기간(period) 등과 같은 추가 속성을 볼 수도 있음 
      - delay=0s 부분은 컨테이너가 시작된 후 바로 프로브가 시작된다는 것을 나타남. 제한 시간이 1초로 설정돼 있으므로(timeout=1s) 컨테이너가 1초 안에 응답해야 함 
      - 10초마다 프로브를 수행하며(period=10s) 프로브가 3번 연속 실패하면(#failure=3) 컨테이너가 다시 시작됨 
    - initialDelaySeconds 속성
      - initialDelaySeconds: 15 - 쿠버네티스는 첫 번째 프로브 실행까지 15초를 대기함 
      - 초기 지연을 설정하지 않으면 프로브는 컨테이너가 시작되마자 프로브를 시작함. 이 경우 대부분 애프리케이션이 요청을 받을 준비가 돼 있지 않기 때문에 프로브가 실패함 
  - 효과적인 라이브니스 프로브 생성 
    - 운영 환경에서 실행 중인 파드는 반드시 라이브니스 프로브를 정의해야 함. 정의하지 않으면 쿠버네티스가 애플리케이션이 살아 있는지를 알 수 있는 방법이 없음 
    - 라이브니스 프로브는 애플리케이션의 내부만 체크하고, 외부 요인의 영향을 받지 않도록 해야 함 
- 레플리케이션 컨트롤러 소개 
  - 레플리케이션 컨트롤러는 쿠버네티스 리소스로서 파드가 항상 실행되도록 보장함. 클러스터에서 노드가 사라지거나 노드에서 파드가 제거된 경우, 레플리케이션 컨트롤는 사라진 파드를 감지해 교체 파드를 생성함 
  - 동작 
    - 레플리케이션컨트롤러는 실행 중인 파드 목록을 지속적으로 모니터링하고, 특정 유형의 실제 파드 수가 의도하는 수와 일치하는지 항상 확인함. 이런 파드가 너무 적게 실행 중인 경우 파드 템플릿에서 새 복제본을 만듬. 너무 많은 파드가 실행 중이면 초과 복제본이 제거됨 
    - 레플리케이션컨트롤러의 세 가지 요소 이해 
      - 레이블 셀렉터(label selector) : 레플리케이션컨트롤러의 범위에 있는 파드를 결정함 
      - 레플리카 수(replica count) : 실행할 파드의 의도하는(desired) 수를 지정함 
      - 파드 템플릿(pod template) : 새로운 파드 레플리카를 만들 때 사용됨 
    - 레플리케이션컨트롤러 사용 시 이점 
      - 기존 파드가 사라지면 새 파드를 시작해 파드(또는 여러 파드의 복제본)가 항상 실행되도록 함 
      - 클러스터 노드에 장애가 발생하면 장애가 발생한 노드에서 실행 중인 모든 파드(레플리케이션컨트롤러의 제어하에 있는 파드)에 관한 교체 복제본이 생성됨 
      - 수동 또는 자동으로 파드를 쉽게 수평으로 확장할 수 있게 함 
    - > kubectl get rc - terminating(종료 중) 
      - 의도하는(desired) 파드 수, 실제 파드(current) 수, 준비된(ready) 파드 수를 표시하는 세 개의 열이 표시됨 
  - 쿠버네티스를 사용하지 않는 환경에서 노드에 장애가 발생하면 운영 팀은 해당 노드에서 실행 중인 애플리케이션을 수동으로 다른 시스템에 마이그레이션해야 할 것. 쿠버네티스는 이를 자동으로 수행함 
  - 레플리케이션컨트롤러가 생성한 파드는 어떤 식으로든 이 레플리케이션컨트롤러와 묶이지 않음. 레플리케이션컨트롤러는 레이블 셀렉터와 일치하는 파드만을 관리함 
  - 수평 파드 스케일링 
    - 레플리케이션컨트롤러 스케일 업(확장)하기 
      - > kubectl scale rc kubia --replicas=10 
  - 레플리케이션 컨트롤러 삭제 
    - kubectl delete를 사용해 레플리케이션컨트롤러를 삭제할 때, 명령에 --cascade=false 옵션을 추가해 해당 파드를 계속 실행시킬 수 있음 
    - > kubectl delete rc kubia --cascade=false 
    - 레플리케이션컨트롤러를 삭제해서 파드가 어디에도 속해 있지 않음. 더 이상 관리되지 않음. 그러나 언제든 적절한 레이블 셀렉터를 사용하는 새 레플리케이션컨트롤러를 작성해 다시 관리할 수 있음 
- 레플리케이션컨트롤러 대신 레플리카세 사용하기 
  - 초기에는 레플리케이션컨트롤러가 파드를 복제하고 노드 장애가 발생했을 때 재스케줄링하는 유일한 쿠버네티스 구성 요소였음. 후에 레플리카셋이라는 유사한 리소스가 도입되어 차세대 레플리케이션컨트롤러이며, 레플리케이션컨트롤러를 완전히 대체할 것(레플리케이션컨트롤러는 결국 사용되지 않게 될 것) 
  - 레플리카셋과 레플리케이션컨트롤러 비교 
    - 레플리카셋은 레플리케이션컨트롤러와 똑같이 동작하지만 좀 더 풍부한 표현식을 사용하는 파드 셀렉터를 갖고 있음. 
    - 레플리케이션컨트롤러의 레이블 셀렉터는 특정 레이블이 있는 파드만을 매칭시킬 수 있는 반면, 레플리카셋의 셀렉터는 특정 레이블이 없는 파드나 레이블의 값과 상관없이 특정 레이블의 키를 갖는 파드를 매칭시킬 수 있음 
    - 레플리케이션컨트롤러는 값에 상관없이 레이블 키의 존재만으로 파드를 매칭시킬 수 없지만, 레플리카셋은 가능함 
  - 레플리카셋 정의하기 
    - 파드가 가져야 하는 레이블은 selector 속성 바로 아래 나열하는 대신 SElector.matchLables 아래에 지정함 
  - 레플리카 생성 및 검사 
    - > kubectl get rs 
  - 레플리카셋의 더욱 표현적인 레이블 셀렉터 사용하기 
    - 셀렉터에 표현식을 추가할 수 있음 
    - In은 레이블의 값이 지정된 값 중 하나와 일치해야 함 
    - NotIn은 레이블의 값이 지정된 값과 일치하지 않아야 함 
    - Exists 파드는 지정된 키를 가진 레이블이 포함돼야 함(값은 중요하지 않음). 이 연산자를 사용할 때는 값 필드를 지정하지 않아야 함 
    - DoesNotExist는 파드에 지정된 키를 가진 레이블이 포함돼 있지 않아야 함. 값 필드를 지정하지 않아야 함 
- 데몬셋을 사용해 각 노드에 정확히 한 개의 파드 실행하기 
  - 클러스터의 모든 노드에, 노드당 하나의 파드만 실행되길 원하는 경우
  - 모든 노드에서 로그 수집기와 리소스 모니터를 실행하려는 경우. 좋은 예는 쿠버네티스의 kube-proxy 프로세스이며, 서비스를 작동시키기 위해 모든 노드에서 실행돼야 함 
  - 데몬셋(각 노드에 정확히 하나의 복제본만 실행)은 각 노드에서 하나의 파드 복제본만 실행하지만 레플리카셋은 클러스터 전체에서 무작위로 파드를 분산시킴 
  - 데몬셋으로 모든 노드에 파드 실행하기 
    - 모든 클러스터 노드마다 파드를 하나만 실행하려면 데몬셋(DaemonSet) 오브젝트를 생성해야 함. 데몬셋에 의해 생성되는 파드는 타깃 노드가 이미 지정돼 있고 쿠버네티스 스케줄러를 건너뛰는 것을 제외하면 이 오브젝트는 레플리케이션컨트롤러 또는 레플리카셋과 매우 유사함. 파드가 클러스터 내에 무작위로 흩어져 배포되지 않음 
    - 레플리카셋(또는 레플리케이션컨트롤러)이 클러스터에 원하는 수의 파드 복제본이 존재하는지 확인하는 반면, 데몬셋에는 원하는 복제본 수라는 개념이 없음. 파드 셀렉터와 일치하는 파드 하나가 각 노드에서 실행 중인지 확인하는 것이 데몬셋이 수행해야 하는 역할이기 때문에 복제본 개념이 필요하지 않음 
    - 노드가 다운되면 데몬셋은 다른 곳에서 파드를 생성하지 않음. 그러나 새 노드가 클러스터에 추가되면 데몬셋은 즉시 새 파드 인스턴스를 새 노드에 배포함. 실수로 파드 중 하나를 삭제해 노드에 데몬셋의 파드가 없는 경우에도 마찬가지 
- 완료 가능한 단일 태스크를 수행하는 파드 실행 
  - 잡 리소스 
    - 잡은 파드의 컨테이너 내부에서 실행 중인 프로세스가 성공적으로 완료되면 컨테이너를 다시 시작하지 않는 파드를 실행할 수 있음 
    - 노드에 장애가 발생한 경우 해당 노드에 있던 잡이 관리하는 파드는 레플리카셋 파드와 같은 방식으로 다른 노드로 다시 스케줄링됨. 프로세스 자체에 장애가 발생한 경우(프로세스가 에러 종료 코드를 리턴할 때), 잡에서 컨테이너를 다시 시작할 것인지 설정할 수 있음 

# 서비스: 클라이언트가 파드를 검색하고 통신을 가능하게 함 
- 서비스 
  - 쿠버네티스의 서비스는 동일한 서비스를 제공하는 파드 그룹에 지속적인 단일 접점을 만들려고 할 때 생성하는 리소스. 각 서비스는 서비스가 존재하는 동안 절대 바뀌지 않는 IP 주소와 포트가 있음 
  - 클라이언트는 해당 IP와 포트로 접속한 다음 해당 서비스를 지원하는 파드 중 하나로 연결됨 
  - 서비스 생성
    - kubectl expose로 서비스 생성 
      - expose 명령어는 레플리케이션컨트롤러에서 사용된 것과 동일한 파드 셀렉터를 사용해 서비스 리소스를 생성하고 모든 파드를 단일 IP 주소와 포트로 노출함 
    - 실행 중인 컨테이너에 원격으로 명령어 실행 
      - kubectl exec 명령어를 사용하면 기존 파드의 컨테이너 내에서 원격으로 임의의 명령어를 실행할 수 있음. 컨테이너의 내용, 상태, 환경을 검사할 때 유용함 
      - kubectl exec {k get pods의 파드} -- curl -s {http://10.111.249.153 - k get svc의 클러스터 IP}
        - 명령어의 더블 대시(--)는 kubectl 명령줄 옵션의 끝을 의미함. 더블 대시 뒤의 모든 것은 파드 내에서 실행돼야 하는 명령 
- 클러스터 외부에 있는 서비스 연결 
  - 서비스 엔드포인트 
    - 서비스는 파드에 직접 연결(link)되지 않음. 대신 엔드포인트 리소스가 그 사이에 있음
    - 엔드포인트 리소스는 서비스로 노출되는 파드의 IP 주소와 포트 목록 
  - 외부 서비스를 위한 별칭 생성  
    - ExternalName 서비스 생성 
      - 외부 서비스의 별칭으로 사용되는 서비스를 만들려면 유형(type) 필드를 ExternalName으로 설정해 서비스 리소스를 만듬 
- 외부 클라이언트에 서비스 노출 
  - 외부에서 서비스를 액세스할 수 있는 방법
    - 노드포트로 서비스 유형 설정 : 노드포트 서비스의 경우 각 클러스터 노드는 노드 자체에서 포트를 결고 해당 포트로 수신된 트래픽을 서비스로 전달함. 이 서비스는 내부 클러스터 IP와 포트로 액세스할 수 있을 뿐만 아니라 모든 노드의 전용 포트로도 액세스할 수 있음 
    - 서비스 유형을 노드포트 유형의 확장인 로드밸런서로 설정 : 쿠버네티스가 실행 중인 클라우드 인프라에서 프로비저닝된 전용 로드밸런서로 서비스에 액세스할 수 있음. 로드밸런서는 트래픽을 모든 노도의 노드포트로 전달함. 클라이언트는 로드밸런서의 IP로 서비스에 액세스함 
    - 단일 IP 주소로 여러 서비스를 노출하는 인그레스 리소스 만들기 : HTTP 레벨에서 작동하므로 4계층 서비스보다 더 많은 기능을 제공할 수 있음 
  - 노드포트 서비스 사용 
    - 노드포트 서비스를 만들면 쿠버네티스는 모든 노드에 특정 포트를 할당하고(모든 노드에서 동일한 포트 번호가 사용됨) 서비스를 구성하는 파드로 들어오는 연결을 전달함 
    - 일반 서비스(실제 유형은 ClusterIP)와 유사하지만 서비스의 내부 클러스터 IP뿐만 아니라 모든 노드의 IP와 할당된 노드포트로 서비스에 액세스할 수 있음 
    - 노드포트 서비스 확인 
      - > kubectl get svc kubia-nodeport 
      - EXTERNAL-IP에 <nodes>라고 표시돼 있고 클러스터 노드의 IP 주소로 서비스에 액세스할 수 있음을 나타냄 
  - 외부 로드밸런서로 서비스 노출
    - 클라우드 공급자에서 실행되는 쿠버네티스 크러스터는 일반적으로 클라우드 인프라에서 로드밸런서를 자동으로 프로비저닝하는 기능을 제공함 
    - 로드밸런서는 공개적으로 액세스 가능한 고유한 IP주소를 가지며 모든 연결을 서비스로 전달함. 로드밸런서의 IP 주소로 서비스에 액세스할 수 있음 
- 인그레스 리소스로 서비스 외부 노출 
  - 인그레스가 필요한 이유
    - 로드밸런서 서비스는 자신의 공용 IP 주소를 가진 로드밸런서가 필요하지만, 인그레스는 한 IP 주소로 수십 개의 서비스에 접근이 가능하도록 지원해줌
    - 클라이언트가 HTTP 요청을 인그레스에 보낼 때, 요청한 호스트와 경로에 따라 요청을 전달할 서비스가 결정됨 
    - 인그레스는 네트워크 스택의 애플리케이션 계층(HTTP)에서 작동하며 서비스가 할 수 없는 쿠키 기반 세션 어피니티 등과 같은 기능을 제공할 수 있음 
- 파드가 연결을 수락할 준비가 됐을 때 신호 보내기 
  - 레디니스 프로브 소개
    - 레디니스 프로브는 주기적으로 호출되며 특정 파드가 클라이언트 요청을 수신할 수 있는지를 결정함 
    - 컨테이너의 레디니스 프로브가 성공을 반환하면 컨테이너가 요청을 수락할 준비가 됐다는 신호 
  - 레디니스 프로브 유형 
    - 프로세스를 실행하는 Exec 프로브는 컨테이너의 상태를 프로세스의 종료 상태 코드로 결정함 
    - HTTP GET 프로브는 HTTP GET 요청을 컨테이너로 보내고 응답의 HTTP 상태 코드를 보고 컨테이너가 준비됐는지 여부를 결정함 
    - TCP 소켓 프로브는 컨테이너의 지정된 포트로 TCP 연결을 염. 소켓이 연결되면 컨테이너가 준비된 것으로 간주함 
  - 레디스 프로브의 동작 
    - 라이브니스 프로브와 달리 컨테이너가 준비 상태 점검에 실패하더라도 컨테이너가 종료되거나 다시 시작되지 않음 
    - 라이브니스 프로브는 상태가 좋지 않은 컨테이너를 제거하고 새롭고 건강한 컨테이너로 교체해 파드의 상태를 정상으로 유지하는 반면, 레디니스 프로브는 요청을 처리할 준비가 된 파드의 컨테이너만 요청을 수신하도록 함 
- 서비스 문제 해결
  - 먼저 외부가 아닌 클러스터 내에서 서비스의 클러스터 IP에 연결되는지 확인함 
  - 서비스에 액세스할 수 있는지 확인하려고 서비스 IP로 핑을 할 필요 없음(서비스의 클러스터 IP는 가상 IP이므로 평되지 않음) 
  - 레디니스 프로브를 정의했다면 성공했는지 확인하라. 그렇지 않으면 파드는 서비스에 포함되지 않음 
  - 파드가 서비스의 일부인지 확인하려면 kubectl get endpoints를 사용해 해당 엔드포인트 오브젝트를 확인함 
  - FQDN이나 그 일부(myservice.mynamespace.svc.cluster.local 또는 myservice.mynamespace)로 서비스에 액세스하려고 하는데 작동하지 않는 경우, FQDN 대신 클러스터 IP를 사용해 액세스할 수 있는지 확인함 
  - 대상 포트가 아닌 서비스로 노출된 포트에 연결하고 있는지 확인함 
  - 파드 IP에 직접 연결해 파드가 올바른 포트에 연결돼 있는지 확인함 
  - 파드 IP로 애플리케이션에 액세스할 수 없는 경우 애플리케이션이 로컬호스트에만 바인딩하고 있는지 확인함 

# 볼륨: 컨테이너에 디스크 스토리지 연결 
- 스토리지 볼륨은 파드와 같은 최상위 리소스는 아니지만 파드의 일부분으로 정의되며 파드와 동일한 라이프사이클을 가짐. 파드가 시작되면 볼륨이 생성되고, 파드가 삭제되면 볼륨이 삭제된다는 것을 의미함 
- 볼륨 소개 
  - 쿠버네티스 볼륨은 파드의 구성 요소로 컨테이너와 동일하게 파드 스펙에서 정의됨. 볼륨은 독립적인 쿠버네티스 오브젝트가 아니므로 자체적으로 생성, 삭제될 수 없음 
  - 사용 가능한 볼륨 유형 소개 
    - emptyDir : 일시적인 데이터를 저장하는 데 사용되는 간단한 빈 디렉터리 
    - hostPath : 워커 노드의 파일시스템을 파드의 디렉터리로 마운트하는 데 사용함 
    - gitRepo : 깃 리포지터리의 콘텐츠를 체크아웃해 초기화환 볼륨 
    - nfs :NFS 공유를 파드에 마운트함 
    - gcePersistentDisk(Google Compute Engine Persistent Disk), awsElasticBlock Store(Amazon Web Services Elastic Block Store Volume), azureDisk(Microsoft Azure Disk Volume): 클라우드 제공자의 전용 스토리지를 마운트하는 데 사용함 
    - cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rdb, flexVolume, vsphere Volume, photonPersistentDisk, ScaleIO : 다른 유형의 네트워크 스토리지를 마운트하는 데 사용함 
    - configMap, secret, downwardAPI : 쿠버네티스 리소스나 클러스터 정보를 파드에 노출하는 데 사용되는 특별한 유형의 볼륨 
    - persistentVolumeClaim : 사전에 혹은 동적으로 프로비저닝된 퍼시스턴트 스토리지를 사용하는 방법 
- 볼륨을 사용한 컨테이너 간 데이터 공유
  - emptyDir 볼륨 사용 
    - 볼륨이 빈 디렉터리로 시작됨. 파드에 실행중인 애플리케이션은 어떤 파일이든 볼륨에 쓸 수 있음. 볼륨의 라이플사이클이 파드에 묶여 있으므로 파드가 삭제되면 볼륨의 콘텐츠는 사라짐 
    - emptyDir 볼륨은 동일 파드에서 실행 중인 컨테이너 간 파일을 공유할 때 유용함 
    - 사이드카 컨테이너 소개
      - 사이드카 컨테이너는 파드의 주 컨테이너의 동작을 보완함 
      - 새로운 로직을 메인 애플리케이션 코드에 밀어 넣어 복잡성을 더하고 재사용성을 떨어뜨리는 대신에 파드에 사이드카를 추가하면 기존 컨테이너 이미지를 사용할 수 있음   
- 워커 노드 파일 시스템의 파일 접근
  - hostPath 볼륨
    - hostPath 볼륨은 노드 파일시스템의 특정 파일이나 디렉터리를 가리킴. 동일 노드에 실행 중인 파드가 hostPath 볼륨의 동일 경로를 사용 중이며 동일한 파일이 표시됨 
    - gitRepo나 emptyDir 볼륨의 콘텐츠는 파드가 종료되면 삭제되는 반면, hostPath 볼륨의 콘텐츠는 삭제되지 않음. 파드가 삭제되면 다음 파드가 호스트의 동일 경로를 가리키는 hostPath 볼륨을 사용하고, 이전 파드와 동일한 노드에 스케줄링된다는 조건에서 새로운 파드는 이전 파드가 남긴 모든 항목을 볼 수 있음 
- 기반 스토리지 기술과 파드 분리 
  - 퍼시스턴트볼륨과 퍼시스턴트볼륨클레임 
    - 인프라스트럭처의 세부 사항을 처리하지 않고 애플리케이션이 쿠버네티스 클러스터에 스토리지를 요청할 수 있도록 하기 위해 새로운 리소스 두 개가 도입됌. 퍼시스턴트볼륨(PV, PersistentVolume)과 퍼시스턴트볼륨클레임(PVC, PersistentVolumeClaim)
  - 퍼시스턴트볼륨 생성 
    - 퍼시스턴트볼륨을 생성할 때 관리자는 쿠버네티스에게 용량이 얼마가 되는지 단일 노드나 동시에 다수 노드에 읽기나 쓰기가 가능한지 여부를 알려야 함. 또한 쿠버네티스에게 퍼시스턴트볼륨이 해제되면 어떤 동작을 해야 할지 알려야 함(바인딩된 퍼시스턴트볼륨클레임이 삭제되는 경우)
    - 퍼시스턴트볼륨을 지원하는 실제 스토리지의 유형, 위치, 그 밖의 속성 정보를 지정해야 함 
  - 퍼시스턴트볼륨클레임 생성을 통한 퍼시스턴트볼륨 요청 
    - 퍼시스턴트볼륨클레임이 생성되자마자 쿠버네티스는 적절한 퍼시스턴트볼륨을 찾고 클레임에 바인딩함. 퍼시스턴트볼륨의 용량은 퍼시스턴트볼륨클레임의 요청을 수용할만큼 충분히 커야 함 
      - RWO(ReadWriteOnce) : 단일 노드만이 읽기/쓰기용으로 볼륨을 마운트할 수 있음 
      - ROX(ReadOnlyMany) : 다수 노드가 읽기용으로 볼륨을 마운트할 수 있음 
      - RWX(ReadWriteMany) : 다수 노드가 읽기/쓰기용으로 볼륨을 마운트할 수 있음 
- 퍼시스턴트볼륨의 동적 프로비저닝 
  - 스토리지클래스 리소스를 통한 사용 가능한 스토리지 유형 정의하기
    - 스토리지클래스 리소스는 퍼시스턴트볼륨클레임이 스토리지클래스에 요청할 때 어떤 프로비저너가 퍼시스턴트볼륨을 프로비저닝하는 데 사용돼야 할지를 지정함. 스토리지 클래스에 정의된 파라미터들은 프로비저너에 전달되며, 파라미터는 각 프로비저너 플러그인마다 다름

# 컨피그맵과 시크릿: 애플리케이션 설정 
- 컨테이너화된 애플리케이션 설정
  - 설정 데이터를 저장하는 쿠버네티스 리소스를 컨피그맵(ConfigMap). 컨피그맵을 사용해 설정 데이터를 저장할지 여부에 관계없이 다음 방법을 통해 애플리케이션을 구성할 수 있음 
    - 컨테이너에 명령줄 인수 전달 
    - 각 컨테이너를 위한 사용자 정의 환경변수 지정 
    - 특수한 유형의 볼륨을 통해 설정 파일을 컨테이너에 마운트 
- 컨테이너에 명령줄 인자 전달 
  - 도커에서 명령어와 인자 정의 
    - Dockerfile에서 두 개의 지침
      - ENTRYPOINT는 컨테이너가 시작될 때 호출될 명령어를 정의함
      - CMD는 ENTRYPOINT에 전달되는 인자를 정의함 
      - ENTRYPOINT 명령어로 실행하고 기본 인자를 정의하려는 경우에만 CMD를 지정하는 것 
- 컨피그맵으로 설정 분리
  - 컨피그맵 
    - 쿠버네티스에서는 설정 옵션을 컨피그맵이라 부르는 별도 오브젝트로 분리할 수 있음. 컨피그맵은 짧은 문자열에서 전체 설정 파일에 이르는 값을 가지는 키/값 쌍으로 구성된 맵 
- 시크릿으로 민감한 데이터를 컨테이너에 전달
  - 민감하지 않고, 일반 설정 데이터는 컨피그맵을 사용함
  - 본질적으로 민감한 데이터는 시크릿을 사용해 키 아래에 보관하는 것이 필요함. 만약 설정 파일이 민감한 데이터와 그렇지 않은 데이터를 모두 가지고 있다면 해당 파일을 시크릿 안에 저장해야 함 
  - 컨피그맵과 시크릿 비교
    - 시크릿 항목의 내용은 Base64 인코딩 문자열로 표시되고, 컨피그맵의 내용은 일반 텍스트로 표시됨. 처음에는 시크릿 안에 있는 YAML과 JSON 매니페스트를 다루는 것이 고통스러울 것. 각 항목을 설정하고 읽을 때마다 인코딩과 디코딩을 해야 하기 때문 
    - 바이너리 데이터 시크릿 사용
      - Base64 인코딩을 사용하는 까닭은 시크릿 항목에 일반 텍스트뿐만 아니라 바이너리 값도 담을 수 있기 때문. Base64 인코딩은 바이너리 데이터를 일반 텍스트 형식인 YAML이나 JSON 안에 넣을 수 있음 
    - StringData 필드는 쓰기 전용(읽기 전용이 아니라 쓰기 전용). 값을 설정할 때만 사용할 수 있음. kubectl get -o yaml 명령으로 시크릿의 YAML 정의를 가져올 때, StringData 필드는 표시되지 않음. StringData 필드로 지정한 모든 항목은 data 항목 아래에 다른 모든 항목처럼 Base64로 인코딩돼 표시됨 
     
# 애플리케이션에서 파드 메타데이터와 그 외의 리소스에 액세스하기 
- Downward API로 메타데이터 전달
  - Downward API로 환경변수 또는 (downwardAPI 볼륨 내에 있는) 파일로 파드와 해당 환경의 메타데이터를 전달할 수 있음 
  - 사용 가능한 메타데이터 
    - 파드의 이름
    - 파드의 IP 주소
    - 파드가 속한 네임스페이스
    - 파드가 실행 중인 노드의 이름
    - 파드가 실행 중인 서비스 어카운트 이름 
    - 각 컨테이너의 CPU와 메모리 요청
    - 각 컨테이너의 CPU와 메모리 제한
    - 파드의 레이블
    - 파드의 어노테이션 
  - 서비스 어카운트란 파드가 API 서버와 통신할 때 인증하는 계정 
- 쿠버네티스 API 서버와 통신하기
  - kubectl 프록시로 API 서버 액세스하기
    - kubectl proxy 명령은 프록시 서버를 실행해 로컬 컴퓨터에서 HTTP 연결을 수신하고, 이 연결을 인증을 관리하면서 API 서버로 전달하기 때문에, 요청할 때마다 인증 토큰을 전달할 필요가 없음 
  - 파드가 쿠버네티스와 통신하는 방법
    - 파드 내에서 실행 중인 애플리케이션이 쿠버네티스 API에 적절히 액세스할 수 있는 방법
      - 애플리케이션은 API 서버의 인증서가 인증 기관으로부터 서명됐는지를 검증해야하며, 인증 기관의 인증서는 ca.crt 파일에 있음 
      - 애플리케이션은 token 파일의 내용을 Authorization HTTP 헤더에 Bearer 토큰으로 넣어 전송해서 자신을 인증해야 함 
      - namespace 파일은 파드의 네임스페이스 안에 있는 API 오브젝트의 CRUD 작업을 수행할 때 네임스페이스를 API 서버로 전달하는 데 사용해야 함 
        - CRUD은 Create, Read, Update, Delete를 나타냄. 해당 HTTP 메서드는 각각 POST, GET, PATCH/PUT, DELETE 

# 디플로이먼트: 선언적 애프리케이션 업데이트 
- 파드에서 실행 중인 애플리케이션 업데이트 
  - 모든 파드를 업데이트 하는 방법
    - 기존 파드를 모두 삭제한 다음 새 파드를 시작함
      - 짧은 시간 동안 애플리케이션을 사용할 수 없음 
    - 새로운 파드를 시작하고, 기동하면 기존 파드를 삭제함. 새 파드를 모두 추가한 다음 한꺼번에 기존 파드를 삭제하거나 순차적으로 새 파드를 추가하고 기존 파드를 점진적으로 제거해 이 작업을 수행할 수 있음   
      - 애플리케이션이 동시에 두 가지 버전을 실행해야 함. 애플리케이션이 데이터 저장소에 데이터를 저장하는 경우 새 버전이 이전 버전을 손상시킬 수 있는 데이터 스키마나 데이터의 수정을 해서는 안됨 
  - 새 파드 기동과 이전 파드 삭제
    - 한 번에 이전 버전에서 새 버전으로 전환 
      - 파드의 앞쪽에는 일반적으로 서비스를 배치함. 새 버전을 실행하는 파드를 불러오는 동안 서비스는 파드의 이전 버전에 연결됨. 
      - 블루 그린 디플로이먼트 : 새 파드가 모두 실행되면 서비스의 레이블 셀렉터를 변경하고 서비스를 새 파드로 전환할 수 있음. 전환한 후 새 버전이 올바르게 작동하면 이전 레플리케이션컨트롤러를 삭제해 이전 파드를 삭제할 수 있음 
    - 롤링 업데이트
      - 새 파드가 모두 실행된 후 이전 파드를 한 번에 삭제하는 방법 대신 파드를 단계별로 교체하는 롤링 업데이트. 이전 레플리케이션컨트롤러를 천천히 스케일 다운하고 새 파드를 스케일 업해 이를 수행할 수 있음 
- 애플리케이션을 선언적으로 업데이트하기 위한 디플로이먼트 사용하기 
  - 디플로이먼트는 낮은 수준(lower-level)의 개념으로 간주되는 레플리케이션컨트롤러 또는 레플리카셋을 통해 수행하는 대신 애플리케이션을 배포하고 선언적(declarative) 으로 업데이트하기 위한 높은 수준(high-level)의 리소스 
  - 디플로이먼트를 생성하면 레플리카셋 리소스가 그 아래에 생성됨(결과적으로 더 많은 리소스가 생성됨)
  - 레플리카셋도 파드를 복제하고 관리함. 디플로이먼트를 사용하는 경우 실제 파드는 디플로이먼트가 아닌 디플로이먼트의 레플리카셋에 의해 생성되고 관리됨 
  - 파드를 감시하는 레플리카셋이 디플로이먼트를 지원함(디플로이먼트 -> 레플리카셋 -> 파드) 
  - 디플로이먼트 생성 
    - 디플로이먼트는 레이블 셀렉터, 원하는 레플리카 수, 파드 템플릿으로 구성됨. 디플로이 먼트 리소스가 수정될 때 업데이트 수행 방법을 정의하는 디플로이먼트 전략을 지정하는 필드도 있음 
  - 디플로이먼트 롤아웃 상태 출력
    - 디플로이먼트 상태를 확인하기 위해 특별히 만들어진 다른 명령어 
    - > kubectl rollout status deployment kubia 
  - 디플로이먼트 업데이트 
    - 사용 가능한 디플로이먼트 전략 
      - Recreate 전략
        - 새 파드를 만들기 전에 이전 파드를 모두 삭제함. 애플리케이션이 여러 버전을 병렬로 실행하는 것을 지원하지 않고 새 버전을 시작하기 전에 이전 버전을 완전히 중지해야 하는 경우 이 전략을 사용함 
        - 애플리케이션을 완전히 사용할 수 없는 짧은 서비스 다운타임이 발생함 
      - RollingUpdate 전략
        - 이전 파드를 하나씩 제거하고 동시에 새 파드를 추가해 전체 프로세스에서 애플리케이션을 계속 사용할 수 있도록 하고 서비스 다운 타임이 없도록 함. 기본 전략 
        - 의도하는 레플리카 수보다 많거나 적은 파드 수에 관한 상한과 하한을 설정할 수 있음. 애플리케이션에서 이전 버전과 새 버전을 동시에 실행할 수 있는 경우에만 이 전략을 사용해야 함 
  - 쿠버네티스의 기존 리소스 수정하기
    - kubectl edit : 기본 편집기로 오브젝트의 매니페스트를 오픈함. 변경 후 파일을 저장하고 편집기를 종료하면 오브젝트가 업데이트됨. kubectl edit deployment kubia
    - kubectl patch : 오브젝트의 개별 속성을 수정함. kubectl patch deployment kubia -p '{"spec": {"template": {"spec": {"containers": [{"name":"nodejs","image": "luksa/kubia:v2"}]}}}}'
    - kubectl apply : 전체 YAML/JSON 파일의 속성 값을 적용해 오브젝트를 수정함. YAML/JSON에 지정된 오브젝트가 아직 없으면 생성됨. 파일에는 리소스의 전체 정의가 포함돼야 함. kubectl apply -f kubia-deployment-v2.yaml 
    - kubectl replace : YAML/JSON 파일로 오브젝트를 새 것으로 교체함. apply 명령어와 달리 이 명령은 오브젝트가 있어야 함. 그렇지 않으면 오류를 출력함. kubectl replace -f kubia-deployment-v2.yaml 
    - kubectl set image : 파드, 레플리케이션컨트롤러 템플릿, 디플로이먼트, 데몬셋, 잡 또는 레플리카셋에 정의된 컨테이너 이미지를 변경함. kubectl set image deployment kubia nodejs=luksa/kubia:v2 
    - 이 명령들이 하는 역할은 디플로이먼트 스펙을 변경하는 것으로 이 변경으로 롤아웃 프로세스가 시작됨 
  - 디플로이먼트 롤백 
    - 디플로이먼트 롤아웃 이력 표시 
      - 디플로이먼트는 개정 이력(revision history)을 유지하므로 롤아웃의 롤백이 가능함. 롤아웃이 완료되면 이전 레플리카셋은 삭제되지 않으므로 이전 버전뿐만 아니라 모든 버전으로 롤백할 수 있음. kubectl rollout history 명령어로 개정 이력을 표시할 수 있음 
    - 특정 디플로이먼트 개정으로 롤백
      - undo 명령어에서 개정(revison) 번호를 지정해 특정 개정으로 롤백할 수 있음. 
      - kubectl rollout undo deployment kubia --to-revision=1 : 첫 번째 버전으로 롤백
    - 롤아웃 일시 정지
      - 카나리 릴리스는 잘못된 버전의 애플리케이션이 롤아웃돼 모든 사용자에게 영향을 주는 위험을 최소화하는 기술. 새 버전을 모든 사람에게 롤아웃하는 대신 하나 또는 적은 수의 이전 파드만 새 버전으로 바꿈. 이렇게 하면 소수의 사용자만 초기에 새 버전을 사용하게 됨 
- 하나의 YAML 파일에 여러 리소스를 정의하려면 대시 세 개를 구분 기호로 사용함 
- 보이지 않는 곳에서 정확히 무엇을 하는지 확인하려면 kubectl의 자세한 로깅 옵션(--v 옵션)을 켬 

# 스테이트풀셋: 복제된 스테이트풀 애플리케이션 배포하기 
- 스테이트풀 파드 복제하기
  - 동일 레플리카셋의 모든 파드는 항상 같은 퍼시스턴볼륨클레임과 퍼시스턴트볼륨을 사용함
    - 레플리카셋 - 파드3 - 퍼시스턴트볼륨클레임 - 퍼시스턴트볼륨
- 스테이트풀셋 이해하기 
  - 스테이트풀셋은 애플리케이션의 인스턴스가 각각 안정적인 이름과 상태를 가지며 개별적으로 취급돼야 하는 애플리케이션에 알맞게 만들어졌음 
  - 스테이트풀셋과 레플리카셋 비교
    - 스테이트리스 애플리케이션의 인스턴스는 가축과 같이 동작함. 인스턴스가 죽더라도 새로운 인스턴스를 만들 수 있고 사람들은 그 차이를 알아차리지 못할 것이기 때문에 인스턴스가 죽는 것은 아무런 문제가 되지 않음 
    - 스테이트풀 애플리케이션은 애완동물과 같음. 애완동물이 죽었을 때 새 애완동물을 바로 살 수 없고, 사람들도 금방 알아차릴 것. 잃어버린 애완동물을 대체하려 이전 애완동물과 생김새나 행동이 완전히 똑같은 새로운 애완동물을 찾아야 함. 애플리케이션의 경우 새 인스턴스가 이전 인스턴스와 완전히 같은 상태와 아이덴티티를 가져야 함을 의미함 
  - 스테이트풀셋을 레플리카셋 혹은 레플리케이션컨트롤러와 비교하기 
    - 레플리카셋이나 레플리케이션컨트롤러로 관리되는 파드는 가축과 같음. 이들은 대부분 스테이트리스로 언제든지 완전히 새로운 파드로 교체될 수 있음.
    - 스테이트풀 파드가 종료되면 (혹은 실행 중인 노드가 실패하면) 새로운 파드 인스턴스는 교체되는 파드와 동일한 이름, 네트워크 아이덴티티, 상태 그대로 다른 노드에서 되살아나야 함. 스테이트풀셋은 파드가 아이덴티티와 상태를 유지하면서 다시 스케줄링되게 함 
  - 안정적인 네트워크 아이덴티티 제공하기 
    - 스테이트풀셋으로 생성된 파드는 서수 인덱스(0부터 시작)가 할당되고 파드의 이름과 호스트 이름, 안정적인 스토리지를 붙이는 데 사용됨. 스테이트풀셋의 이름과 인스턴스의 서수 인덱스로부터 파생되므로 파드의 이름을 예측할 수 있음. 파드는 임의의 이름이 아닌 잘 정리된 이름을 갖음 
  - 스테이트풀셋은 스케일 다운할 때 퍼시스턴트볼륨클레임을 삭제하지 않음. 그 다음 스케일을 다시 확장할 때 다시 연결됨 
- 스테이트풀셋의 피어 디스커버리
  - 피어 디스커버리(클러스터의 다른 멤버를 찾는 기능). 스테이트풀셋의 각 멤버는 모든 다른 멤버를 쉽게 찾을 수 있어야 함 
  - SRV 레코드 
    - SRV 레코드는 특정 서비스를 제공하는 서버의 호스트 이름과 포트를 가리키는 데 사용됨. 쿠버네티스는 헤드리스 서비스를 뒷받침하는 파드의 호스트 이름을 가리키도록 SRV 레코드를 생성함 

# 쿠버네티스 내부 이해하기
- 아키텍처 이해
  - 컨트롤 플레인 구성 요소 
    - 컨트롤 플레인(Control Plane)은 클러스터 기능을 제어하고 전체 클러스터가 동작하게 만드는 역할을 함 
    - 구성 요소
      - etcd 분산 저장 스토리지 
      - API 서버 
      - 스케줄러
      - 컨트롤러 매니저 
  - 워커 노드에서 실행하는 구성 요소 
    - 컨테이너를 실행하는 작업은 각 워커 노드에서 실행되는 구성 요소가 담당함 
      - Kubelet
      - 쿠버네티스 서비스 프록시(kube-proxy)
      - 컨테이너 런타임(Docker, rkt 외 기타) 
  - 애드온 구성 요소 
    - 컨트롤 플레인과 노드에서 실행되는 구성 요소 외에도 클러스터에서 추가 구성 소가 필요함 
      - 쿠버네티스 DNS 서버
      - 대시보드
      - 인그레스 컨트롤러(Ingress Controller)
      - 힙스터
      - 컨테이너 네트워크 인터페이스(Container Network Interface) 플러그인 
  - 쿠버네티스 구성 요소의 분산 특성
    - 구성 요소가 서로 통신하는 방법
      - 쿠버네티스 시스템 구성 요소는 오직 API 서버하고만 통신함. 서로 직접 통신하지 않음. API 서버는 etcd와 통신하는 유일한 구성 요소 
      - kubectl을 이용해 로그를 가져오거나 kubectl attach 명령으로 실행 중인 컨테이너에 연결할 때 kubectl port-forwar 명령을 실행할 때는 API 서버가 Kubelet에 접속함 
      - kubectl attach 명령은 kubectl exec와 비슷하지만 별도 프로세스를 실행하는 대신 컨테이너에서 실행 중인 메인 프로세스에 연결함 
  - 쿠버네티스가 etcd를 사용하는 방법
    - 모든 오브젝트(파드, 레플리케이션컨트롤러, 서비스, 시크릿 등)는 API 서버가 다시 시작되거나 실패하더라도 유지하기 위해 매니페스트가 영구적으로 저장될 필요가 있음. 쿠버네티스는 빠르고, 분산해서 저장되며, 일관된 키-값 저장소를 제공하는 etcd를 사용함. 분산돼 있기 때문에 둘 이상의 etcd 인스턴스를 실행해 고가용성과 우수한 성능을 제공할 수 있음 
    - 쿠버네티스 API 서버만이 etcd와 직접적으로 통신하는 유일한 구성 요소. 다른 구성요소는 API 서버로 간접적으로 데이터를 읽거나 쓸 수 있음 
    - 강력한 낙관적 잠금 시스템뿐만 아니라 유효성을 검사하는 등의 이점을 얻을 수 있음. 실제 저장소 메커니즘을 추상화해 다른 모든 구성 요소에 제공해 나중에 교체하기 훨씬 쉽도록 함. 쿠버네티스가 클러스터 상태와 메타데이터를 저장하는 유일한 장소가 etcd 
      - 낙관성 동시성 제어(낙관적 잠금)는 데이터 조각에 잠금을 설정해 그동안 데이터를 읽거나 업데이트 하지 못하도록 하는 대신, 데이터에 버전 번호를 포함하는 방법. 데이터가 업데이트될 때마다 버전 번호는 증가됨. 데이터를 업데이트 할 때, 클라이언트가 데이터를 읽은 시간과 업데이트를 제출하는 시간 사이에 버전 번호가 증가했는지 여부를 체크함. 만약에 버전 번호가 증가했다면 수정된 내용은 거부되고 클라이언트는 다시 새 데이터를 읽고, 다시 업데이트를 시도해야 함 
    - 클러스터링된 etcd의 일관성 보장 
      - 고가용성을 보장하기 위해 두 개 이상의 etcd 인스턴스를 실행하는 것이 일반적 
      - etcd는 RAFT 합의 알고리즘을 사용해 어느 순간이든 각 노드 상태가 대다수의 노드가 동의하는 현재 상태이거나 이전에 동의된 상태 중에 하나임을 보장함 
    - etcd 인스턴스 수가 홀수인 이유
      - etcd는 인스턴스를 일반적으로 홀수로 배포함. 두 개의 인스턴스가 있으면 두 인스턴스 모두에 과반이 필요함. 둘 중 하나라도 실패하면 과반이 존재하지 않기 때문에 상태를 변경할 수 없음 
      - 대규모 etcd 클러스터에서는 일반적으로 5대 혹은 7대 노드면 충분함 
  - API 서버의 기능
    - 쿠버네티스 API 서버는 다른 모든 구성 요소와 kubectl 같은 클라이언트에서 사용하는 중심 구성 요소. 클러스터 상태를 조회하고 변경하기 위해 RESTful API로 CRUD(쓰기, 읽기, 갱신, 삭제) 인터페이스를 제공함. 상태는 etcd 안에 저장함 
    - 오브젝트 유효성 검사 작업도 수행하기 때문에 잘못 설정된 오브젝트를 저장할 수 없음(만약 저장소에 직접 저장하는 경우에는 가능함). 유효성 검사와 함께 낙관점 잠금도 처리하기 때문에 동시에 업데이터가 발생하더라도 다른 클라이언트에 의해 오브젝트의 변경 사항이 재정의(override)되지 않음 
    - > kubectl get pods -o yaml --watch
      - --watch 옵션을 이용해 파드의 생성,수정, 삭제 통보를 받을 수 있음
      - 감시(watch) 메커니즘을 계속해서 살펴볼 컨트롤 플레인 구성 요소인 스케줄러에서도 사용됨 
  - 스케줄러 이해 
    - API 서버의 감시 메커니즘을 통해 새로 생성될 파드를 기다리고 있다가 할당된 노드가 없는 새로운 파드를 노드에 할당하기만 함 
    - 스케줄러는 선택된 노드(또는 해당 노드에서 실행 중인 Kubelet)에 파드를 실행하도록 지시하지 않음. 단지 스케줄러는 API 서버로 파드 정의를 갱신함. API 서버는 Kubelet에 파드가 스케줄링된 것을 통보함 
    - 대상 노드의 Kubelet은 파드가 해당 노드에 스케줄링된 것을 확인하자마자 파드의 커테이너를 생성하고 실행함 
    - 기본 스케줄링 알고리즘
      - 노드의 선택
        - 모든 노드 중에서 파드를 스케줄링할 수 있는 노드 목록을 필터링함 
        - 수용가능한 노드의 우선순위를 정하고 점수가 높은 노드를 선택함. 만약 여러 노드가 같은 최상위 점수를 가지고 있다면, 파드가 모든 노드에 고르게 배포되도록 라운드-로빈을 사용함 
      - 스케줄러는 파드를 수용할 수 있는 노드를 찾아 그중에서 가장 적합한 노드를 선택함 
    - 수용 가능한 노드 찾기
      - 파드를 수용할 수 있는 노드를 찾기 위해, 스케줄러는 미리 설정된 조건 함수(predicate function) 목록에 각 노드를 전달함
        - 노드가 하드웨어 리소스에 대한 파드 요청을 충족시킬 수 있는가? 
        - 노드에 리소스가 부족한가(메모리 혹은 디스크 부족 상태를 보고하는가)?
        - 파드를 특정 노드(이름)로 스케줄링 하도록 요청한 경우에, 해당 노드인가? 
        - 노드가 파드 정의 안에 있는 노드 셀렉터와 일치하는 레이블을 가지고 있는가(정의돼 있는 경우)? 
        - 파드가 특정 호스트 포트에 할당되도록 요청한 경우 해당 포트가 이 노드에서 이미 사용 중인가?
        - 파드 요청이 특정한 유형의 볼륨을 요청하는 경우 이 노드에서 해당 볼륨을 파드에 마운트할 수 있는가, 아니면 이 노드에 있는 다른 파드가 이미 같은 볼륨을 사용하고 있는가?
        - 파드가 노드의 테인트(taints)를 허용하는가? 테인트와 톨러레이션(tolerations)
        - 파드가 노드와 파드의 어피니티(affinity), 안티-어피티니(anti-affinity)규칙을 지정했는가? 만약에 그렇다면 이 노드에 파드를 스케줄링하면 이런 규칙을 어기게 되는가?
      - 이 모든 검사를 통과해야 노드가 파드를 수용할 수 있음 
    - 다중 스케줄러 사용
      - 클러스터에서 스케줄러를 하나만 실행하는 대신 여러 개의 스케줄러를 실행할 수 있음. 파드 정의 안에 schedulerName 속성에 파드를 스케줄링할 때 사용할 스케줄러를 지정함 
  - 컨트롤러 매니저에서 실행되는 컨트롤러 소개
    - API 서버로 배포된 리소스에 지정된 대로 시스템을 원하는 상태로 수렴되도록 하는 다른 활성 구성 요소가 필요함. 컨트롤러 매니저 안에서 실행되는 컨트롤러에 의해 수행됨 
    - 컨트롤러 목록
      - 레플리케이션 매니저(레플리케이션컨트롤러 리소스의 컨트롤러)
      - 레플리카셋, 데몬셋, 잡 컨트롤러
      - 디플로이먼트 컨트롤러
      - 스테이트풀셋 컨트롤러
      - 노드 컨트롤러
      - 서비스 컨트로러 
      - 엔드포인트 컨트롤러 
      - 네임스페이스 컨트롤러
      - 퍼시스턴트볼륨 컨트롤러
      - 그 밖의 컨트롤러 
    - 리소스는 클러스터에 어떤 것을 실행해야 하는지 기술하는 반면, 컨트롤러는 리소스를 배포함에 따라 실제 작업을 수행하는 활성화된 쿠버네티스 구성 요소 
    - 컨트롤러의 역할과 동작 방식 이해 
      - 컨트롤러는 다양한 작업을 수행하지만 모두 API 서버에서 리소스(디플로이먼트, 서비스 등)가 변경되는 것을 감시하고 각 변경 작업(새로운 오브젝트를 생성하거나 이미 있는 오브젝트의 갱신 혹은 삭제)을 수행함.대부분 이러한 작업은 다른 리소스 생성, (오브젝트의 status 등) 감시 중인 리소스 자체를 갱신하는 것이 포함됨 
    - 레플리케이션 매니저
      - 레플리케이션 매니저 : 레플리케이션컨트롤러 리소스를 활성화하는 컨트롤러
      - 실제 작업을 수행하는 것은 레플리케이션컨트롤러가 아닌 레플리케이션 매니저 
      - 레플리케이션컨트롤러의 동작을 무한 루프로 생각할 수 있다고 얘기했는데, 매번 루프를 돌 때마다 컨트롤러는 파드 셀렉터와 일하는 파드의 수를 찾고 이를 원하는 레플리카 수와 비교함 
    - 레플리카셋, 데몬셋, 잡 컨트롤러
      - 레플라카셋 컨트롤러는 레플리케이션 매니저와 거의 동일한 기능을 수행함
      - 데몬셋과 잡 컨트롤러는 비슷함. 이 둘은 리소스에 정의된 파드 템플릿에서 파드 리소스를 생성함. 
      - 레플리케이션 매니저와 마찬가지로 이 컨트롤러들은 파드를 실행하지 않고, 파드 정의 API 서버에 게시해 Kubelet이 컨테이너를 생성하고 실행하도록 함 
    - 디플로이먼트 컨트롤러
      - 디플로이먼트 컨트롤러는 실제 배포된 상태와 디플로이먼트 API 오브젝트에 기록된 원화는 상태가 동기화되록 관리함
      - 디플로이먼트 컨트롤러는 디플로이먼트 오브젝트가 수정(수정이 배포된 파드에 영향을 준다면)될 때마다 새로운 버전을 롤아웃함 
    - 스테이트풀셋 컨트롤러
      - 스테이트풀셋 컨트롤러는 레플라카셋 컨트롤러와 기타 관련된 컨트롤러와 비슷하게 스테이트풀셋 리소스 정의에 따라 파드를 생성, 관리하고 삭제함. 다른 컨트롤러가 파드만을 관리하는 반면, 스테이트풀 컨트롤러는 각 파드 인스턴스를 위한 퍼시스턴트볼륨클레임하고 인스턴스화하고 관리함 
    - 노드 컨트롤러
      - 노드 컨트롤러는 클러스터의 워커 노드를 기술하는 노드 리소스를 관리함. 무엇보다도 클러스터에서 실행 중인 실제 머신 목록과 노드 오브젝트 목록을 동기화하는 데 중점을 둠 
      - 각 노드의 상태를 모니터링하고 연결이 끊어진 노드에서 파드를 제거함 
    - 서비스 컨트롤러
      - LoadBalancer 유형의 서비스, 인프라스터럭처에 로드 밸런서를 요청해 외부에서 서비스를 사용할 수 있게 만듬
      - 서비스 컨트롤러는 LoadBalancer 유형의 서비스가 생성되거나 삭제될 때 인프라스트럭처에 로드 밸런서를 요청하고 해제하는 역할을 수행함 
    - 엔드포인트 컨트롤러 
      - 서비스는 파드에 직접 연결돼 있지 않지만, 서비스에 정의된 파드 셀렉터에 따라 수동 혹은 자동으로 생성되고 갱신되는 엔드포인트(IP와 포트) 목록을 포함한다는 것을 기억할 것 
      - 레이블 셀렉터와 일치하는 파드의 IP와 포트로 엔드포인트 리스트를 계속 갱신하는 활성 구성 요소 
      - 서비스가 추가 또는 갱신되거나 파드가 추가, 갱신, 삭제될 경우 서비스의 파드 셀렉터와 일치하는 파드를 선택해 IP와 포트를 엔드포인트 리소스에 추가함 
    - 네임스페이스 컨트롤러
      - 네임스페이스 리소스가 삭제되면, 해당 네임스페이스에 속해 있는 모든 리소스도 같이 삭제돼야 함. 네임스페이스 컨트롤러가 하는 일 
      - 네임스페이스 오브젝트의 삭제 통보를 받으면 API 서버로 네임스페이스에 속해 있는 모든 리소스를 삭제함 
    - 퍼시스턴트볼륨 컨트롤러
      - 사용자가 퍼시스턴트볼륨클레임을 생성하면, 쿠버네티스는 적절한 퍼시스턴트볼륨을 찾아 해당 클레임에 연결해줘야 함. 퍼시스턴트볼륨 컨트롤러에 의해 수행됨 
      - 퍼시스턴트볼륨클레임이 나타나면, 컨트롤러는 요청한 접근 모드와 일치하는 퍼시스턴트볼륨을 먼저 찾음. 그리고 그 가운데 요청한 용량보다 크지만 선언된 용량이 가장 작은 퍼시스턴트볼륨을 선택함 
    - 컨트롤러 정리
      - 모든 컨트롤러는 API 서버로 API 오브젝트를 제어함. 어떤 컨트롤러도 Kubelet과 직접 통신하거나 어떠한 명령도 내리지 않음 
  - Kubelet이 하는 일 
    - 쿠버네티스 컨트롤 플레인이 일부로써 마스터 노드에서 실행되는 다른 컨트롤러와 달리, Kubelet과 서비스 프록시는 실제 파드 컨테이너가 실행되고 있는 워커 노드에서 실행됨 
    - Kubelet의 작업 이해 
      - Kubelet은 워커 노드에서 실행하는 모든 것을 담당하는 구성 요소 
      - Kubelet이 실행 중인 노드를 노드 리소스로 만들어서 API 서버에 등록하는 것 
      - API 서버를 지속적으로 모니터링해 해당 노드에 파드가 스케줄링 되면, 파드의 컨테이너를 시작함. 설정된 컨테이너 런타임(도커, CoreOS의 rkt 등)에 지정된 컨테이너 이미지로 컨테이너를 실행하도록 지시함으로써 이 작업을 수행함 
      - 실행 중인 컨테이너를 계속 모니터링하면서 상태, 이벤트, 리소스 사용량 API 서버에서 보고함 
      - Kubelet은 컨테이너 라이브니스 프로브(liveness probes)를 실행하는 구성 요소이기도 하며, 프로브가 실패할 경우 컨테이너를 다시 시작함. API 서버에서 파드가 삭제되면 컨테이너를 정지하고 파드가 종료된 것을 서버에 통보함 
  - 쿠버네티스 서비스 프록시의 역할
    - 모든 워커 노드는 클라이언트가 쿠버네티스 API로 정의한 서비스에 연결할 수 있도록 해주는 kube-proxy도 같이 실행함. kube-proxy는 서비스 IP와 포트로 들어온 접속을 서비스(혹은 파드가 없는 서비스 엔드포인트)를 지원하는 파드 중 하나와 연결시켜줌. 
    - 서비스가 둘 이상의 파드에서 지원되는 경우 프록시는 파드 간에 로드밸런싱을 수행함 
    - 프록시라고 부르는 이유 
      - 서비스 IP로 향하는 연결을 가로채기 위해 프록시는 iptables 규칙(iptables 도구는 리눅스 커널의 패킷 필터링 기능을 관리함)을 설정해 이를 프록시 서버로 전송했음 
  - 쿠버네티스 애드온
    - DNS 서버 동작 방식
      - 클러스터의 모든 파드는 기본적으로 클러스터의 내부 DNS 서버를 사용하도록 설정돼 있음. 파드는 서비스를 이름으로 쉽게 찾을 수 있고 헤드리스(headless) 서비스 파드인 경우에는 해당 의 IP 주소를 조회할 수 있음 
      - DNS 서버 파드는 kube-dns 서비스로 노출되므로, 해당 파드를 다른 파드와 마찬가지로 클러스터 안에서 이동할 수 있음 
    - 인그레스 컨트롤러 동작 방식
      - 인그레스 컨트롤러는 리버스 프록시 서버(Nginx)를 실행하고 클러스터에 정의된 인그레스, 서비스, 엔드포인 리소스 설정을 유지함. 컨트롤러는 이러한 리소스를 감시하고 변경이 일어날 때마다 프록시 서버 설정을 변경함 
      - 인그레스 리소스의 정의는 서비스를 가리키지만, 인그레스 컨트롤러는 트래픽을 서비스의 IP로 보내지 않고 서비스의 파드로 직접 전달함. 외부에서 접속한 클라이언트가 인그레스 컨트롤러로 연결할 때 IP를 보존하는 데 영향을 주기 때문에, 특정 사용 사례에서는 서비스를 선호하도록 만듬 
- 컨트롤러가 협업하는 방법
  - 이벤트 체인
    - 디플로이먼트 리소스가 API 서버에 제출될 때 일어나는 이벤트 체인
    - 디플로이먼트 컨트롤러가 레플리카셋 생성
    - 레플리카셋 컨틀롤러가 파드 리소스 생성
    - 스케줄러가 새로 생성한 파드에 노드 할당
    - Kubelet은 파드의 컨테이너를 실행함 
- 실행 중인 파드에 관한 이해 
  - 퍼즈(pause) 컨테이너는 파드의 모든 컨테이너를 함께 담고 있는 컨테이너, 퍼즈 컨테이너는 이러한 네임스페이스를 모두 보유하는 게 유일한 목적인 인프라스트럭처 컨테이너 
- 고가용성 클러스터 실행
  - 쿠버네티스 안에서 애플리케이션을 실행하는 이유 가운데 하나는 인프라스트럭처 장애가 발생하는 경우에도 사용자의 개입 없이 혹은 제한적인 수동 개입만으로 중단 없이 계속 실행할 수 있게 해주기 때문 
  - 서비스를 중단 없이 계속 실행하기 위해서는 애플리케이션뿐만 아니라 쿠버네티스 컨트롤 플레인 구성 요소도 항상 동작하고 있어야 함 
  - 애프리케이션 가용성 높이기 
    - 쿠버네티스에서 애플리케이션을 실행할 때, 다양한 컨트롤러는 노드 장애가 발생해도 애플리케이션이 특정 규모로 원활하게 동작할 수 있게 해줌. 애플리케이션의 가용성을 높이려면 디플로이먼트 리소스로 애플리케이션을 실행하고 적절한 수의 레플리카를 설정하기만 하면 되며, 나머지는 쿠버네티스가 처리함 
    - 가동 중단 시간을 줄이기 위한 다중 인스턴스 실행
    - 수평 스케일링이 불가능한 애플리케이션을 위한 리더 선출 메커니즘 사용
      - 중단 시간이 발생하는 것을 피하려면, 활성 복제본과 함께 비활성 복제본을 실행해두고 빠른 임대(fast-acting lease) 혹은 리더 선출 메커니즘(leader-election mechanism)을 이용해 단 하나만 활성화 상태로 만들어야 함 
      - 리더 서눌에 익숙하지 않다면 이는 여러 애플리케이션 인스턴스가 분산 환경에서 실행 중인 경우에 누가 리더가 될지 합의하는 방법 

# 쿠버네티스 API 서버 보안 
- 인증 이해 
  - 인증 플러그인은 다음 방법을 사용해 클라이언트의 아이덴티티를 얻음 
    - 클라이언트의 인증서
    - HTTP 헤더로 전달된 인증 토큰
    - 기본 HTTP 인증
    - 기타 
  - 사용자와 그룹 
    - 인증 플러그인은 인증된 사용자의 사용자 이름과 그룹을 반환함 
    - 사용자
      - 실제 사람(사용자)
      - 파드(더 구체적으로, 파드 내부에서 실행되는 애플리케이션)
      - 사용자는 싱글 사인 온(SSO, Single Sin On)과 같은 외부 시스템에 의해 관리돼야 하지만 파드는 서비스 어카운트(service account)라는 매커니즘을 사용하며, 클러스터에 서비스어카운트(Service Account) 리소스로 생성되고 저장됨 
  - 서비스어카운트
    - 서비스어카운트는 파드 내부에서 실행되는 애플리케이션이 API 서버에 자신을 인증하는 방법
    - 서비스어카운트 리소스
      - 서비스어카운트는 파드, 시크릿, 컨피그맵 등과 같은 리소스이며 개별 네임스페이스로 범위가 지정됨. 각 네임스페이스마다 default 서비스어카운트가 자동으로 생성됨 
- 역할 기반 액세스 제어로 클러스터 보안
  - RBAC 인가 플러그인 
    - 액션 이해하기
      - REST 클라이언트는 GET, POST, PUT, DELETE 및 기타 유형의 HTTP 요청을 특정 REST 리소스를 나타내는 특정 URL 경로로 보냄 
        - 파드 가져오기(GET)
        - 서비스 생성하기(CREATE)
        - 시크릿 업데이트(Update)
        - 기타 등등
  - RBAC 리소스
    - RBAC 인가 규칙은네 개의 리소스로 구성되며 두 개의 그룹으로 분류할 수 있음 
      - 롤(Role)과 클러스터롤(ClusterRole) : 리소스에 수행할 수 있는 동사를 지정함
      - 롤바인딩(RoleBinding)과 클러스터롤바인딩(ClusterRoleBinding) : 위의 롤을 특정 사용자, 그룹 또는 서비스어카운트에 바인딩함 
    - 롤은 수행할 수 있는 작업을 정의하고, 바인딩은 누가 이를 수행할 수 있는지를 정의함 
    - 롤은 권한을 부여하며 롤바인딩은 롤을 주체에 바인딩함 
    - 롤과 롤바인딩은 네임스페이스가 지정된 리소스이고 클러스터롤과 클러스터롤바인딩은 네임스페이스를 지정하지 않는 클러스터 수준의 리소스라는 것 
  - 특정 롤과 바인딩의 조합을 사용해야 하는 경우
  - |접근|롤 타입|사용할 바인딩 타입| 
    |:---:|:---:|:---:|
    |클러스터 수준 리소스(노드, 퍼시스턴트볼륨)|클러스터롤|클러스터롤바인딩|
    |리소스가 아닌 URL(/api, /healthz,...)|클러스터롤|클러스터롤바인딩|
    |모든 네임스페이스의 네임스페이스로 지정된 리소스|클러스터롤|클러스터롤바인딩| 
    |특정 네임스페이스의 네임스페이스로 지정된 리소스(여러 네임스페이스에 동일한 클러스터롤 재사용)|클러스터롤|롤바인딩|
    |특정 네임스페이스의 네임스페이스로 지정된 리소스(각 네임스페이스에 롤을 정의해야 함)|롤|롤바인딩|

# 클러스터 노드와 네트워크 보안 
- 파드에서 호스트 노드의 네임스페이스 사용
  - 각 파드는 고유한 PID 네임스페이스가 있기 때문에 고유한 프로세스 트리가 있으며 고유한 IPC 네임스페이스도 사용하므로 동일한 파드의 프로세스 간 통신 메커니즘(IPC)으로 서로 통신할 수 있음 
- 컨테이너 보안 컨텍스트 구성
  - 보안 컨텍스트에서 설저알 수 있는 사항
    - 컨테이너의 프로세스를 실행할 사용자(사용자 ID) 지정하기
    - 컨테이너가 루트로 실행되는 것 방지하기
    - 컨테이너를 특권 모드(privileged mode)에서 실행해 노드의 커널에 관한 모든 접근 권한을 가짐
    - 특권 모드에서 컨테이너를 실행해 컨테이너에 가능한 모든 권한을 부여하는 것과 달리 기능을 추가하거나 삭제해 세분화된 권환 구성하기
    - 컨테이너의 권한 확인을 강력하게 하기 위해 SELinux(Security Enhanced Linux) 옵셜 설정하기
    - 프로세스가 컨테이너의 파일시스템에 쓰기 방지하기 
- 파드의 보안 관련 기능 사용 제한 
  - PodSecurityPolicy 리소스 
    - PodSecurityPolicy는 클러스터 수준 리소스로 사용자가 파드에서 사용할 수 있거나 사용할 수 없는 보안 관련 기능을 정의함
    - PodSecurityPolicy가 할 수 있는 작업
      - 파드가 호스트의 IPC, PID 또는 네트워크 네임스페이스를 사용할 수 있는지 여부
      - 파드가 바인딩할 수 있는 호스트 포트
      - 컨테이너가 실행할 수 있는 사용자 ID
      - 특권을 갖는 컨테이너(privileged containers)가 있는 파드를 만들 수 있는지 여부
      - 어떤 커널 기능이 허용되는지, 어떤 기능이 기본으로 추가되거나 혹은 항상 삭제되는지 여부
      - 컨테이너가 사용할 수 있는 SELinux 레이블
      - 컨테이너가 쓰기 가능한 루트 파일시스템을 사용할 수 있는지 여부
      - 컨테이너가 실행할 수 있는 파일시스템 그룹
      - 파드가 사용할 수 있는 볼륨 유형 
- 파드 네트워크 격리
  - 네트워킹 플로그인이 지원하는 경우 NetworkPolicy 리소스를 만들어 네트워크 격리를 구성할 수 있음
  - NetworkPolicy는 해당 레이블 셀렉터와 일치하는 파드에 적용되며 일치하는 파드에 액세스할 수 있는 소스나 파드에서 액세스할 수 있는 대상을 지정함. 각각 인그레스(ingress)와 이그레서(egress) 규칙으로 구성됨. 두 유형의 규칙은 파드 셀렉터와 일치하는 파드나 레이블이 네임스페이스 셀렉터와 일치하는 네임스페이스의 모든 파드, 또는 CIDR(Classless Inter-Domain Routing) 표기법(ex-192.168.1.0/24)을 사용해 지정된 네트워크 IP 대역과 일치하는 파드에 대해 적용됨 
  - 네임스페이스에서 네트워크 격리 사용
    - 기본적으로 지정된 네임스페이스의 파드는 누구나 액세스할 수 있음 
  - CIDR 표기법으로 격리
    - NetworkPolicy에서 대상으로 지정된 파드에 액세스할 수 있는 대상을 정의하려면 파드 또는 네임스페이스 셀렉터를 지정하는 대신 CIDR 표기법으로 IP 블록을 지정할 수도 있음 

# 파드의 컴퓨팅 리소스 관리
- 파드 컨테이너의 리소스 요청
  - 파드를 생성할 때 컨테이너가 필요로 하는 CPU와 메모리 양(requests라고 부름)과 사용할 수 있는(limits로 알려진) 엄격한 제한을 지정할 수 있음 
  - 파드의 리소스 요청(requests)과 제한(limits)은 모든 컨테이너의 리소스 요청과 제한의 합
- 컨테이너에 사용 가능한 리소스 제한
  - CPU는 압축 가능한 리소스. 즉, 컨테이너에서 실행 중인 프로세스에 부정적인 영향을 주지 않고 컨테이너가 사용하는 CPU 양을 조절할 수 있음.
  - 메모리는 분명 다름. 압축이 불가능함. 프로세스에 메모리가 주어지면 프로세스가 메모리를 해제하지 않는 한 가져갈 수 없음. 그것이 컨테이너에 할당되는 메모리의 최대량을 제한해야 하는 이유 
  - 메모리를 제한하지 않으면 워커 노드에 실행 중인 컨테이너(혹은 파드)는 사용 가능한 모든 메모리를 사용해서 노드에 있는 다른 모든 파드와 노드에 스케줄링되는 새 파드에 영향을 미칠 수 있음(새 파드는 실제 메모리 사용량이 아닌 메모리 요청에 기반으로 노드에 스케줄링됨) 오작동하거나 악의적인 파드 하나가 실제 전체 노드를 사용할 수 없게 만들 수 있음 
  - 리소스 제한 초과
    - CrashLoopBackOff는 각 크래시 후 Kublet이 컨테이너를 다시 시작하기 전에 간격을 늘리는 것을 의미함. 첫번째 크래시 후에 Kubelet은 컨테이너를 즉시 다시 시작하고 다시 크래시가 발생하면 다시 시작하기 전에 10초를 기다림. 이후 크래시가 발생하면 지연 시간이 20초, 40초, 80초, 160초로 지수로 증가하고 마지막으로 300초로 제한됨 
    - 간격이 300초 제한에 도달하면 Kublet은 파드가 크래시를 멈추거나 삭제될 때까지 5분마다 컨테이너를 계속 재시작함 
- 파드 QoS 클래스 
  - 쿠버네티스는 파드를 세 가지 서비스 품질(QoS,Quality of Service) 클래스로 분류함
    - BestEffort(최하위 우선순위)
    - Burstable
    - Guaranteed(최상위 우선순위)
  - 파드의 QoS 클래스 정의
    - QoS 클래스는 파드 컨테이너의 리소스 요청과 제한의 조합에서 파생됨
    - BestEffort 클래스에 파드를 할당하기
      - 우선순위가 가장 낮은클래스. 아무런 리소스 요청과 제한이 없는(파드의 컨테이너 중 하나도 없는) 파드에 할당되. 파드에 실행 중인 컨테이너는 리소스 보장을 받지 못함
      - 최악의 경우 CPU 시간을 전혀 받지 못할 수 있고 다른 파드를 위해 메모리가 해제돼야 할 때 가장 먼저 종료됨
      - BestEffort 파드는 설정된 메모리 제한이 없으므로 메모리가 충분하다면 컨테이너는 원하는 만큼 메모리를 사용할 수 있음 
    - Guaranteed 클래스에 파드를 할당하기
      - 모든 리소스를 컨테이너의 리소스 요청이 리소스 제한과 동일한 파드에게 주어짐
      - CPU와 메모리에 리소스 요청과 제한이 모두 설정돼야 함 
      - 각 컨테이너에 설정돼야 함 
      - 리소스 요청과 제한이 동일해야 함 
    - Burstable QoS 클래스에 파드를 할당하기
      - BestEffrot와 Guranteed 사이가 Burstable QoS 클래스
      - 컨테이너 하나의 리소스 요청과 제한은 일치하지만 다른 컨테이너의 리소스 요청과 제한을 지정하지 않는 파드도 포함됨. 요청한 양 만큼의 리소스를 얻지만 필요하면 추가 리소스(리소스 제한까지)를 사용할 수 있음 
  - 컨테이너 QoS 클래스 파악
    - 리소스 요청과 제한에 기반으로 한 컨테이너가 하나인 파드의 QoS 클래스
    - |CPU 요청 대 제한|메모리 요청 대 제한|컨테이너 QoS 클래스|
      |:---:|:---:|:---:|
      |미설정|미설정|BestEffort
      |미설정|요청 < 제한|Burstable
      |미설정|요청 = 제한|Burstable
      |요청 < 제한|미설정|Burstable
      |요청 < 제한|요청 < 제한|Burstable
      |요청 < 제한|요청 = 제한|Burstable
      |요청 = 제한|요청 = 제한|Guranteed
  - 메모리가 부족할 때 어떤 프로세스가 종료되는지 이해
    - 시스템이 오버커밋되면 QoS 클래스는 어떤 컨테이너를 먼저 종료할지 결정하고 해제된 리소스를 높은 우선순위 파드에 줄 수 있음 
    - BestEffort 클래스가 가장 먼저 종료되고 다음은 Burstable 파드가 종료되며, 마지막으로 Guranteed 파드는 시스템 프로세스가 메모리를 필요로 하는 경우에만 종료됨 
- 파드 리소스 사용량 모니터링
  - 인플럭스DB는 애플리케이션 메트릭과 기타 모니터링 데이터를 저장하는 데 이상적인 오픈소스 시계열 데이터베이스 
  - 오픈소스인 그라파나는 인플럭스DB에 저장된 데이터를 시각화하고 시간이 지남에 따라 애플리케이션의 리소스 사용량이 어떻게 변하는지 확인할 수 있는 멋진 웹 콘솔을 갖춘 분석 및 

                 # 파드의 컴퓨팅 리소스 관리
- 파드 컨테이너의 리소스 요청
  - 파드를 생성할 때 컨테이너가 필요로 하는 CPU와 메모리 양(requests라고 부름)과 사용할 수 있는(limits로 알려진) 엄격한 제한을 지정할 수 있음 
  - 파드의 리소스 요청(requests)과 제한(limits)은 모든 컨테이너의 리소스 요청과 제한의 합
- 컨테이너에 사용 가능한 리소스 제한
  - CPU는 압축 가능한 리소스. 즉, 컨테이너에서 실행 중인 프로세스에 부정적인 영향을 주지 않고 컨테이너가 사용하는 CPU 양을 조절할 수 있음.
  - 메모리는 분명 다름. 압축이 불가능함. 프로세스에 메모리가 주어지면 프로세스가 메모리를 해제하지 않는 한 가져갈 수 없음. 그것이 컨테이너에 할당되는 메모리의 최대량을 제한해야 하는 이유 
  - 메모리를 제한하지 않으면 워커 노드에 실행 중인 컨테이너(혹은 파드)는 사용 가능한 모든 메모리를 사용해서 노드에 있는 다른 모든 파드와 노드에 스케줄링되는 새 파드에 영향을 미칠 수 있음(새 파드는 실제 메모리 사용량이 아닌 메모리 요청에 기반으로 노드에 스케줄링됨) 오작동하거나 악의적인 파드 하나가 실제 전체 노드를 사용할 수 없게 만들 수 있음 
  - 리소스 제한 초과
    - CrashLoopBackOff는 각 크래시 후 Kublet이 컨테이너를 다시 시작하기 전에 간격을 늘리는 것을 의미함. 첫번째 크래시 후에 Kubelet은 컨테이너를 즉시 다시 시작하고 다시 크래시가 발생하면 다시 시작하기 전에 10초를 기다림. 이후 크래시가 발생하면 지연 시간이 20초, 40초, 80초, 160초로 지수로 증가하고 마지막으로 300초로 제한됨 
    - 간격이 300초 제한에 도달하면 Kublet은 파드가 크래시를 멈추거나 삭제될 때까지 5분마다 컨테이너를 계속 재시작함 
- 파드 QoS 클래스 
  - 쿠버네티스는 파드를 세 가지 서비스 품질(QoS,Quality of Service) 클래스로 분류함
    - BestEffort(최하위 우선순위)
    - Burstable
    - Guaranteed(최상위 우선순위)
  - 파드의 QoS 클래스 정의
    - QoS 클래스는 파드 컨테이너의 리소스 요청과 제한의 조합에서 파생됨
    - BestEffort 클래스에 파드를 할당하기
      - 우선순위가 가장 낮은클래스. 아무런 리소스 요청과 제한이 없는(파드의 컨테이너 중 하나도 없는) 파드에 할당되. 파드에 실행 중인 컨테이너는 리소스 보장을 받지 못함
      - 최악의 경우 CPU 시간을 전혀 받지 못할 수 있고 다른 파드를 위해 메모리가 해제돼야 할 때 가장 먼저 종료됨
      - BestEffort 파드는 설정된 메모리 제한이 없으므로 메모리가 충분하다면 컨테이너는 원하는 만큼 메모리를 사용할 수 있음 
    - Guaranteed 클래스에 파드를 할당하기
      - 모든 리소스를 컨테이너의 리소스 요청이 리소스 제한과 동일한 파드에게 주어짐
      - CPU와 메모리에 리소스 요청과 제한이 모두 설정돼야 함 
      - 각 컨테이너에 설정돼야 함 
      - 리소스 요청과 제한이 동일해야 함 
    - Burstable QoS 클래스에 파드를 할당하기
      - BestEffrot와 Guranteed 사이가 Burstable QoS 클래스
      - 컨테이너 하나의 리소스 요청과 제한은 일치하지만 다른 컨테이너의 리소스 요청과 제한을 지정하지 않는 파드도 포함됨. 요청한 양 만큼의 리소스를 얻지만 필요하면 추가 리소스(리소스 제한까지)를 사용할 수 있음 
  - 컨테이너 QoS 클래스 파악
    - 리소스 요청과 제한에 기반으로 한 컨테이너가 하나인 파드의 QoS 클래스
    - |CPU 요청 대 제한|메모리 요청 대 제한|컨테이너 QoS 클래스|
      |:---:|:---:|:---:|
      |미설정|미설정|BestEffort
      |미설정|요청 < 제한|Burstable
      |미설정|요청 = 제한|Burstable
      |요청 < 제한|미설정|Burstable
      |요청 < 제한|요청 < 제한|Burstable
      |요청 < 제한|요청 = 제한|Burstable
      |요청 = 제한|요청 = 제한|Guranteed
  - 메모리가 부족할 때 어떤 프로세스가 종료되는지 이해
    - 시스템이 오버커밋되면 QoS 클래스는 어떤 컨테이너를 먼저 종료할지 결정하고 해제된 리소스를 높은 우선순위 파드에 줄 수 있음 
    - BestEffort 클래스가 가장 먼저 종료되고 다음은 Burstable 파드가 종료되며, 마지막으로 Guranteed 파드는 시스템 프로세스가 메모리를 필요로 하는 경우에만 종료됨 
- 파드 리소스 사용량 모니터링
  - 인플럭스DB는 애플리케이션 메트릭과 기타 모니터링 데이터를 저장하는 데 이상적인 오픈소스 시계열 데이터베이스 
  - 오픈소스인 그라파나는 인플럭스DB에 저장된 데이터를 시각화하고 시간이 지남에 따라 애플리케이션의 리소스 사용량이 어떻게 변하는지 확인할 수 있는 멋진 웹 콘솔을 갖춘 분석 및 
