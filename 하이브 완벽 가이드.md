# 서평
- 이 책은 데이터 모델링부터 쿼리, 색인, 튜닝, 함수, 스토리지 핸들러, HCatalog 등의 고급 기능까지 총망라하여 제공하고 있다.
- 이 책의 목적은 개발자,데이터베이스 관리자, 아키텍트는 물로인고 비즈니스 분석가처럼 기술 수준이 낮은 사용자에게 이르는 모든 이에게 HiveQL을 예제 중심으로 설명하는 것, 개발자나 하이브 쿼리의 성능 튜닝 및 사용자 정의 함수, 커스텀 데이터 포맷 정의를 사용할 하둡 관리자에게 필요한 기술적 내용을 자세히 제공하는 것 

# 소개
- 맵리듀스 프로그래밍은 큰 연산 작업을 다수의 값싼 서버로 구성된 클러스터에 고르게 나누어 처리하는 프로그래밍 모델로, 비용효율성과 수평확장성을 제공함, 이 연산 모델 아래에는 하둡 분산 파일시스템(HDFS, Hadoop Distributed Filesystem)이라는 분산 파일 시스템이 존재함 
- 하이브는 하둡 클러스터에 있는 데이터를 검색하기 위해 하이브 쿼리 언어(HiveQL) 혹은 HQL이라 부르는 SQL 호환 언어를 제공함 
- 하이브에서 레코드 단위 갱신(record-level update), 삽입, 삭제를 할 수 없긴 하지만 쿼리로 새 테이블을 만들 수 있고 쿼리 결과를 파일로 남길 수도 있음.
- 하둡은 배치처리 기반 시스템(batch-oriented system)이기 때문에 하이브 쿼리 응답 시간이 매우 길음, 맵리듀스의 기동 부하(start-up overhead)가 있기 때문
- 하이브는 트랜잭션을 제공하지 않음
- 큰 규모의 데이터를 다룰 수 있는 OLTP가 필요하면 NoSQL 데이터베이스를 사용하는 것이 더 좋음. NoSQL에는 하둡과 연동된 NoSQL인 HBase, 카산드라, 만약 아마존의 Elastic MapReduce 또는 Elastic Compute Cloud을 이용한다면 다이나모디비와 같은 것도 있음 
- 하이브는 아주 큰 데이터셋을 다루는 곳에서 데이터를 분석하여 정보를 찾아내고 보고서를 만드는 데이터 웨어하우스 애플리케이션에 적합함
- 하둡과 맵리듀스 개요
  - 맵리듀스
    - 맵리듀스는 큰 데이터를 다루는 잡(job)을 여러 태스크(task)로 나누어 다수의 서버로 구성된 클러스터에서 병렬 처리하는 연산 모델. 태스크의 결과를 하나로 합쳐 최종 결과물을 만들 수 있음 
    - 맵리듀스라는 용어는 맵과 리듀스라는 두 개의 기본 데이터 변환연산에서 나왔음. 맵연산은 컬렉션(collection)의 모든 원소를 한 형태에서 다른 형태로 변환함. 이때 입력 키-값(key-value) 쌍은 0개에서 다수의(zero-to-many) 출력 키-값 쌍으로 변환됨. 입력키와 결과키, 입력값과 결과값은 완전히 달라질 수 있음 
    - 맵리듀스에서 같은 키를 가진 모든 키-값 쌍은 컬렉션 형태로 동일한 리듀서 함수에 전달됨. 리듀서 함수의 목적은 값 컬렉션을 합계, 평균과 같은 값으로 변경하거나 다른 컬렉션으로 변경하는 것. 최종 키-값 쌍은 리듀서가 내보냄 
    - 맵리듀스의 입력과 출력에 사용되는 자료구조는 키-값 쌍. 맵퍼가 시작되면 문서의 줄마다 반복 호출됨. 호출할 때 전달되는 키는 문서 시작 부분의 문자 오프셋임
    - 맵퍼는 한 줄의 단어를 키-값 쌍으로 출력함. 정렬과 셔플 단계를 자동으로 처리함. 하둡은 키 순서대로 키-값 쌍을 정렬함. 
    - 셔플 : 같은 키를 가진 키를 모두 모아서 같은 리듀서에게 보냄.
    - 리듀서의 입력 역시 키-값 쌍. 키는 맵퍼에서 나온 단어 중 하나이고 값은 그 단어 발생 횟수의 컬렉션
    - 키 데이터형은 문자열이고 값 데이터형은 정수형 컬렉션
    - 모든 리듀서는 각 단어의 값 컬렉션별로 값을 모두 더하고 단어-총 발생 수 키-값 쌍을 출력해야 함 
- 하둡 생태계에서 하이브
  - SQL 사용자를 위한 친숙한 프로그래밍 모델을 제공할 뿐만 아니라 자바에서 해야 하는 반복적이고 까다로운 코딩을 하지 않도록 해줌 
  - 하이브와 상호동작하는 몇 가지 인터페이스가 있음. CLI, 그래픽 사용자 인터페이스가 편한 사용자는 상용인 Karmasphere, 오픈소스인 클라우데라(Cloudera)의 Hue, Qubole의 새로운 Hive-as-service 등 
  - CLI와 간단한 하이브 웹 인터페이스(HWI), JDBC, ODBC, 쓰리프트(Thrift)서버 
  - 모든 명령어와 쿼리는 입력을 해석하고 필요한 연산을 최적화한 후 보통은 맵리듀스 잡으로 되어있는 단계를 실행하는 드라이버(Driver)로 보내짐 
  - 하이브는 맵리듀스 잡을 시작하기 위해 잡트래커(JobTracker)와 통신함
  - 하이브는 테이블 스키마와 시스템 메타데이터를 보관하기 위한 메타스토어(MetaStore)를 저장하기 위해 별도의 관계형 데이터베이스 (보통 MySQL)을 이용함
  - 하이브는 실시간 응답을 요하는 쿼리나 레코드 수준의 삽입, 갱신, 삭제를 필요로 하지 않는 데이터 웨어하우스 애플리케이션에 적합함
  - 피그 
    - 야후!는 피그를 개발함.
    - 피그는 쿼리 언어가 아닌 데이터 흐름 언어(data flow language), 피그에서는 다른 관계로부터 관계를 정의하기 위해 일련의 선언문을 작성함. 각각의 새로운 관계별로 새로운 데이터 변환을 실행함 
  - HBase
    - HBase는 분산 및 확장 가능한 데이터 저장소. 로우 단위 갱신, 빠른 응답 시간과 로우 단위 트랜잭션을 제공함(복수의 로우 트랜잭션은 제공하지 않음)
    - 칼럼 지향 저장소를 지원하는 것 
  - 캐스케이딩, 크런치, 그 외 도구
    - 모두 JVM 라이브러리
    - 개발자에게 이러한 도구는 튜링 완전체(Turing complete) 프로그래밍 언어의 강력함을 제공함 \
    - 고수준 하둡을 위한 대체 라이브러리
      - 캐스케이딩(Cascading) - 데이터 처리 추상화를 위한 자바 API. 캐스케이딩을 위한 도메인 특화 언어(DSLs)가 있음. 스칼라,그루비,JRuby, Jython
      - 캐스캐로그(Cascalog) - 캐스케이딩용 클로저(Clojure) DSL. 데이터 처리 및 쿼리 추상화를 위한 Datalog로부터 영감을 받은 추가 기능을 제공함
      - 크런치(Crunch) - 데이터 플로우 파이프라인을 위한 자바,스칼라 API
    - 맵리듀스를 사용하지 않는 분산 데이터 처리 도구
      - 스파크(Spark) - 분산된 데이터를 스칼라 API로 처리하는 분산 연산 프레임워크, HDFS와 동작함. 많은 맵리듀스 처리에서 눈에 띄는 성능 향상을 제공함. 하이브를 스파크에 포팅하는 프로젝트도 있으며 샤크라고 불림 
      - 스톰(Storm) - 실시간 이벤트 스트림 처리 시스템
      - 카프카(Kafka) - 분산 메시지 시스템
    - 기타 데이터 처리 언어와 도구
      - R - 통계 분석, 데이터 도식활르 위한 오픈소스 언어로 통계학자, 경제학자 등이 주로 사용함. 분산 시스템이 아니기 때문에 처리할 수 있는 데이터 크기는 제한되어 있음 
      - 매트랩 - 데이터 분석, 수치 연산을 위한 상용 시스템, 엔지니어와 과확자가 주로 사용함
      - 옥타브(Octave) - 매트랩의 오픈소스 버전
      - 메스메티카(Mathmatica) - 상용 데이터 분석, 기호 처리, 수치 계산 시스템. 과학자와 엔지니어가 주로 사용함
      - 사이파이(SciPy), 넘피(NumPy) - 과학 프로그램을 위한 파이썬 확장 소프트웨어 패키지. 데이터 과학자 사이에 널리 사용됨 

# 시작하기
- VMware용 하둡 가상 머신 
  - 클라우데라(Cloudera) - 클라우데라가 자체적으로 배포하는 하둡 버전을 사용하고 있음
  - 맵알(MapR) - HDFS를 MapR 파일시스템으로 대체하여 자체적으로 배포하는 하둡 버전을 사용하고 있음 
  - 홀튼웍스(Hortonworks) - 가장 최신의 안정된 아파치 배포 하둡을 사용함. 현재는 배포 중지한 상태
  - 씽크 빅 아날리틱스(Think Big Analytics) - 가장 최신의 안정된 아파치 배포 하둡을 사용함 
- 로컬 모드, 의사 분산 모드, 분산 모드 
  - 런타임 모드, 기본 모드는 로컬 파일시스템을 사용하는 로컬 모드. 하이브 쿼리를 포함하는 하둡 잡(Job)이 실행될 때 맵 태스크와 리듀스 태스크가 동일한 프로세스에서 동작함 
  - 실제 클러스터는 분산 모드로 설정함. 잡은 이 클러스터 상의 잡트래커(JobTracker)에 의해서 관리되고 각각의 태스크는 각기 다른 프로세스에서 동작함 
  - 하나의 머신에서 의사 분산 (pseudodistributed) 모드로 동작하게 설정할 수 있음 
  - 의사 분산 모드는 분산 모드와 같게 동작함. 파일 시스템은 분산 파일시스템을 기본으로 참조하고 잡은 잡트래커 서비스로 관리함. 다른 점은 하나의 컴퓨터라는 점 
  - HDFS 블록의 복사 개수를 하나로 제한함. 노드가 하나인 클러스털처럼 동작함 
- 하이브 구성 
  - 쓰리프트 서비스는 다른 프로세스에서 원격으로 접근할 수 있도록 해줌. JDBC와 ODBC로 접근할 수 있는 방법을 제공하는데, 이는 쓰리프트 위에서 동작함
  - 하이브를 설치할 대 테이블 스키마와 같은 메타데이터를 저장하기 위해서 메타스토어(metastore)서비스가 필요함 , 메타스토어는 관계형 데이터베이스의 테이블로 구현함 
  - 하이브는 기본 데이터베이스로 프로세스에 내장되어 있고 제한된 기능을 제공하는 더비(Derby) SQL 서버를 사용함 
  - 원격으로 하이브에 접근할 수 있는 웹 인터페이스인 HWI(Hive Web Interface) 를 제공함
- 하둡 환경 설정하기
  - 분산과 의사 분산 모드 설정
    - 분산 모드에서는 여러 서비스가 클러스터 내에서 동작함. 잡트래커는 잡을 관리하고 네임노드는 HDFS 마스터.
    - 작업 노드는 태스크트래커(TaskTracker)와 데이터노드(DataNode)로 구성됨 
    - 태스크트래커는 잡태스크를 구동하고 관리하며 데이터노드는 분산 파일시스템에 저장된 파일을 구성하는 블록을 관리함 
  - JDBC를 사용하는 메타스토어
    - 메타스토어는 create table x... 또는 alter table y... 등과 같은 명령을 수행할 때 명시되는 테이블 스키마와 파티션 정보와 같은 메타데이터를 저장하고 있음 
    - 메타스토어가 단일 고장점(SPOF : Single Point Of Failure)이기 때문에 안정된 서비스를 보장하기 위해서 메타데이터를 복제하고 백업하기 위한 방법을 강구해야 함 
- 하이브 명령
  - 하이브 서비스
    - CLI(명령행 인터페이스) - 테이블을 정의하거나 쿼리 등을 수행하기 위해 사용됨. 다른 서비스가 명시되어 있지 않으면 기본 서비스로 동작함 
    - hiveserver(하이브 서버) - 별도의 프로세스에서 쓰리프트 연결을 위해 대기하고 있는 데몬 
    - hwi(하이브 웹 인터페이스) - 클러스터로 로그인하거나 CLI를 사용하지 않고 쿼리나 다른 명령을 수행하기 위한 간단한 웹 인터페이스
    - jar - 하이브 환경에서 애플리케이션을 수행하기 위한 것으로 hadoop jar 명령의 확장이라고 이해하면 됨
    - metastore - 다중 클라이언트를 지원하기 위해서 하이브 외부에 메타스토어를 구동하는 서비스 
    - rcfilecat - RCFile의 내용을 출력하기 위해서 사용되는 도구 
  
  # 데이터형과 파일 포맷
  - 원시 데이터형
    - 하이브는 여러 크기의 정수형과 부동소수점 및 하나의 불린형(Boolean)과 임의의 길이를 가지는 문자열을 제공함 
    - 원시 데이터형
      - TINYINT - 1바이트 크기의 정수 데이터형 (20)
      - SMALLINT - 2바이트 크기의 정수 데이터형 (20)
      - INT - 4바이트 크기의 정수 데이터형 (20)
      - BIGINT - 8바이트 크기의 정수 데이터형 (20)
      - BOOLEAN - TRUE 또는 FALSE - TRUE
      - FLOAT - 단정도 부동소수점 - 3.14159
      - DOUBLE - 배정도 부동소수점 - 3.14159
      - STRING - 문자의 시퀀스, 문자열 셋도 설정 가능작은 따옴표 혹은 큰 따옴표 생성 가능  
      - TIMESTAMP - 정수형, 부동소수점형, 문자열형
      - BINARY - 바이트 배열 형태 지원
    - 하이브는 다른 SQL에서 일반적으로 지원하는 최대 허용 길이를 갖는 문자 배열(character array)을 지원하지 않음. 관계형 데이터베이스에서 성능 최적화 기능으로 색인이나 스캔 등이 용의한 고정된 길이의 레코드를 지원함 
    - 하이브가 속한 느슨한 세계(looser world)에서는 자체 데이터 저장소를 가지지 않고 다양한 파일 포맷을 지원해야 하기 때문에 필드를 구분할 수 있는 구분 기호에 의존함 
    - 하둡 및 하이브는 디스크의 읽기, 쓰기 성능 최적화를 강조하기 때문에 컬럼값의 길이를 고정하는 것은 상대적으로 중요하지 않음 
- 컬렉션 데이터형
  - STRUCT - C 언어의 구조와 객체와 유사함 - struct('John','Doe')
  - MAP - ['key']처럼 필드를 배열 표기법으로 접근할 수 있는 키-값 형태의 컬렉션 데이터형, map('first','John','last','Doe')
  - ARRAY - 0으로 시작하는 정수로 색인할 수 있는 동일한 데이터형의 순차 시퀀스 - array('John','Doe')
  - 컬렉션 데이터형은 정규화를 깰 수 있기 때문에 대부분의 관계형 데이터베이스에서는 컬렉션 데이터형을 지원하지 않음
  - 정규화를 깸으로써 발생하는 더 큰 문제는 데이터 중복, 저장 공간 낭비, 잠재적 데이터의 불일치가 있음. 중복된 데이터는 데이터에 어떤 변경이 발생할 때 동기화되는 데이터를 증가시킬 수 있음 
  - 테라바이트에서 페타바이트 급의 데이터를 검색할 때 최소한의 디스크 탐색은 필수. 컬렉션 데이터형을 이용하면 최소한의 디스크 탐색으로 레코드를 빠르게 검색할 수 있음. 그러나 외래 키 관계의 데이터를 탐색하는 것은 상당한 성능 오버 헤드와 함께 여러 디스크를 걸치는 탐색을 요구함 
- 데이터값의 텍스트 파일 인코딩
  - 쉼표로 필드를 구분하는 텍스트 파일(CSVs:Comma-separated values) 또는 탭으로 필드를 구분하는 텍스트 파일(TSVs:tab-separated values)
  - 하이브의 레코드와 필드 기본 구분 기호
    - \n : 텍스트 파일에서 각 줄은 하나의 레코드가 됨. 그러므로 줄 바꿈 문자는 레코드를 분리함
    - ^A : 모든 컬럼을 분리함. CREATE TABLE문에서 명시적으로 지정할 때는 8진수 코드 '\001'을 사용함
    - ^B : ARRAY, STRUCT 또는 MAP 키-값 쌍의 요소를 분리함
    - ^C : MAP의 키-값 쌍에서 키를 관련된 값과 분리함 
- Schema on Read
  - schema on write 
    - 전통 데이터베이스에서 외부 데이터 로딩, 쿼리 결과물 혹은 UPDATE 문 수행을 통하여 데이터를 쓸 때 데이터베이스는 데이터 저장소 모두를 제어함
    - 데이터베이스는 문지기(gatekeeper)가 되는 것
    - 데이터베이스가 데이터를 쓸 때 스키마를 강제로 맞추려 하기 때문
  - schema on read
    - 하이브는 저장소에 대해 쓰기 제어를 가지고 있지 않음. 생성, 수정, 그리고 심지어 하이브가 쿼리할 때 데이터를 손상시키는 여러 방법이 있음. 하이브는 단지 쿼리 결과를 읽어들일 때에만 스키마를 적용함 

# HiveQL : 데이터 정의
- 하이브는 로우 레벨의 삽입과 변경 그리고 삭제를 지원하지 않음. 트랜잭션 또한 지원하지 않음. 하둡이 지원하는 범위 안에서 보다 나은 성능을 위해 확장할 수 있는 기능을 제공하며 사용자가 정의한 확장과 외부 프로그램을 하이브와 연동할 수 있음 
- 데이터 정의 언어는 데이터베이스, 테이블, 뷰, 함수, 색인을 생성하고 변경하고 삭제하는 데 사용함 
- 하이브에서의 데이터 베이스
  - 하이브에서 데이터베이스 개념은 단지 테이블의 카탈로그 또는 네임스페이스. 여러 팀과 사용자가 어우러져 큰 규모로 작업할 때 테이블명의 충돌을 막는 매우 유용한 방법
  - 데이터베이스 테이블을 논리 그룹으로 구성하는 데 사용
  - IF NOT EXISTS 문으로 필요할 때만 데이터베이스를 즉시 생성 
  - USE 명령어는 파일시스템에서 작업(working) 디렉터리를 변경하는 것과 비슷하게 작업 데이터베이스를 설정하는 데 사용함 
