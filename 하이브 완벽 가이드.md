# 서평
- 이 책은 데이터 모델링부터 쿼리, 색인, 튜닝, 함수, 스토리지 핸들러, HCatalog 등의 고급 기능까지 총망라하여 제공하고 있다.
- 이 책의 목적은 개발자,데이터베이스 관리자, 아키텍트는 물로인고 비즈니스 분석가처럼 기술 수준이 낮은 사용자에게 이르는 모든 이에게 HiveQL을 예제 중심으로 설명하는 것, 개발자나 하이브 쿼리의 성능 튜닝 및 사용자 정의 함수, 커스텀 데이터 포맷 정의를 사용할 하둡 관리자에게 필요한 기술적 내용을 자세히 제공하는 것 

# 소개
- 맵리듀스 프로그래밍은 큰 연산 작업을 다수의 값싼 서버로 구성된 클러스터에 고르게 나누어 처리하는 프로그래밍 모델로, 비용효율성과 수평확장성을 제공함, 이 연산 모델 아래에는 하둡 분산 파일시스템(HDFS, Hadoop Distributed Filesystem)이라는 분산 파일 시스템이 존재함 
- 하이브는 하둡 클러스터에 있는 데이터를 검색하기 위해 하이브 쿼리 언어(HiveQL) 혹은 HQL이라 부르는 SQL 호환 언어를 제공함 
- 하이브에서 레코드 단위 갱신(record-level update), 삽입, 삭제를 할 수 없긴 하지만 쿼리로 새 테이블을 만들 수 있고 쿼리 결과를 파일로 남길 수도 있음.
- 하둡은 배치처리 기반 시스템(batch-oriented system)이기 때문에 하이브 쿼리 응답 시간이 매우 길음, 맵리듀스의 기동 부하(start-up overhead)가 있기 때문
- 하이브는 트랜잭션을 제공하지 않음
- 큰 규모의 데이터를 다룰 수 있는 OLTP가 필요하면 NoSQL 데이터베이스를 사용하는 것이 더 좋음. NoSQL에는 하둡과 연동된 NoSQL인 HBase, 카산드라, 만약 아마존의 Elastic MapReduce 또는 Elastic Compute Cloud을 이용한다면 다이나모디비와 같은 것도 있음 
- 하이브는 아주 큰 데이터셋을 다루는 곳에서 데이터를 분석하여 정보를 찾아내고 보고서를 만드는 데이터 웨어하우스 애플리케이션에 적합함
- 하둡과 맵리듀스 개요
  - 맵리듀스
    - 맵리듀스는 큰 데이터를 다루는 잡(job)을 여러 태스크(task)로 나누어 다수의 서버로 구성된 클러스터에서 병렬 처리하는 연산 모델. 태스크의 결과를 하나로 합쳐 최종 결과물을 만들 수 있음 
    - 맵리듀스라는 용어는 맵과 리듀스라는 두 개의 기본 데이터 변환연산에서 나왔음. 맵연산은 컬렉션(collection)의 모든 원소를 한 형태에서 다른 형태로 변환함. 이때 입력 키-값(key-value) 쌍은 0개에서 다수의(zero-to-many) 출력 키-값 쌍으로 변환됨. 입력키와 결과키, 입력값과 결과값은 완전히 달라질 수 있음 
    - 맵리듀스에서 같은 키를 가진 모든 키-값 쌍은 컬렉션 형태로 동일한 리듀서 함수에 전달됨. 리듀서 함수의 목적은 값 컬렉션을 합계, 평균과 같은 값으로 변경하거나 다른 컬렉션으로 변경하는 것. 최종 키-값 쌍은 리듀서가 내보냄 
    - 맵리듀스의 입력과 출력에 사용되는 자료구조는 키-값 쌍. 맵퍼가 시작되면 문서의 줄마다 반복 호출됨. 호출할 때 전달되는 키는 문서 시작 부분의 문자 오프셋임
    - 맵퍼는 한 줄의 단어를 키-값 쌍으로 출력함. 정렬과 셔플 단계를 자동으로 처리함. 하둡은 키 순서대로 키-값 쌍을 정렬함. 
    - 셔플 : 같은 키를 가진 키를 모두 모아서 같은 리듀서에게 보냄.
    - 리듀서의 입력 역시 키-값 쌍. 키는 맵퍼에서 나온 단어 중 하나이고 값은 그 단어 발생 횟수의 컬렉션
    - 키 데이터형은 문자열이고 값 데이터형은 정수형 컬렉션
    - 모든 리듀서는 각 단어의 값 컬렉션별로 값을 모두 더하고 단어-총 발생 수 키-값 쌍을 출력해야 함 
- 하둡 생태계에서 하이브
  - SQL 사용자를 위한 친숙한 프로그래밍 모델을 제공할 뿐만 아니라 자바에서 해야 하는 반복적이고 까다로운 코딩을 하지 않도록 해줌 
  - 하이브와 상호동작하는 몇 가지 인터페이스가 있음. CLI, 그래픽 사용자 인터페이스가 편한 사용자는 상용인 Karmasphere, 오픈소스인 클라우데라(Cloudera)의 Hue, Qubole의 새로운 Hive-as-service 등 
  - CLI와 간단한 하이브 웹 인터페이스(HWI), JDBC, ODBC, 쓰리프트(Thrift)서버 
  - 모든 명령어와 쿼리는 입력을 해석하고 필요한 연산을 최적화한 후 보통은 맵리듀스 잡으로 되어있는 단계를 실행하는 드라이버(Driver)로 보내짐 
  - 하이브는 맵리듀스 잡을 시작하기 위해 잡트래커(JobTracker)와 통신함
  - 하이브는 테이블 스키마와 시스템 메타데이터를 보관하기 위한 메타스토어(MetaStore)를 저장하기 위해 별도의 관계형 데이터베이스 (보통 MySQL)을 이용함
  - 하이브는 실시간 응답을 요하는 쿼리나 레코드 수준의 삽입, 갱신, 삭제를 필요로 하지 않는 데이터 웨어하우스 애플리케이션에 적합함
  - 피그 
    - 야후!는 피그를 개발함.
    - 피그는 쿼리 언어가 아닌 데이터 흐름 언어(data flow language), 피그에서는 다른 관계로부터 관계를 정의하기 위해 일련의 선언문을 작성함. 각각의 새로운 관계별로 새로운 데이터 변환을 실행함 
  - HBase
    - HBase는 분산 및 확장 가능한 데이터 저장소. 로우 단위 갱신, 빠른 응답 시간과 로우 단위 트랜잭션을 제공함(복수의 로우 트랜잭션은 제공하지 않음)
    - 칼럼 지향 저장소를 지원하는 것 
  - 캐스케이딩, 크런치, 그 외 도구
    - 모두 JVM 라이브러리
    - 개발자에게 이러한 도구는 튜링 완전체(Turing complete) 프로그래밍 언어의 강력함을 제공함 \
    - 고수준 하둡을 위한 대체 라이브러리
      - 캐스케이딩(Cascading) - 데이터 처리 추상화를 위한 자바 API. 캐스케이딩을 위한 도메인 특화 언어(DSLs)가 있음. 스칼라,그루비,JRuby, Jython
      - 캐스캐로그(Cascalog) - 캐스케이딩용 클로저(Clojure) DSL. 데이터 처리 및 쿼리 추상화를 위한 Datalog로부터 영감을 받은 추가 기능을 제공함
      - 크런치(Crunch) - 데이터 플로우 파이프라인을 위한 자바,스칼라 API
    - 맵리듀스를 사용하지 않는 분산 데이터 처리 도구
      - 스파크(Spark) - 분산된 데이터를 스칼라 API로 처리하는 분산 연산 프레임워크, HDFS와 동작함. 많은 맵리듀스 처리에서 눈에 띄는 성능 향상을 제공함. 하이브를 스파크에 포팅하는 프로젝트도 있으며 샤크라고 불림 
      - 스톰(Storm) - 실시간 이벤트 스트림 처리 시스템
      - 카프카(Kafka) - 분산 메시지 시스템
    - 기타 데이터 처리 언어와 도구
      - R - 통계 분석, 데이터 도식활르 위한 오픈소스 언어로 통계학자, 경제학자 등이 주로 사용함. 분산 시스템이 아니기 때문에 처리할 수 있는 데이터 크기는 제한되어 있음 
      - 매트랩 - 데이터 분석, 수치 연산을 위한 상용 시스템, 엔지니어와 과확자가 주로 사용함
      - 옥타브(Octave) - 매트랩의 오픈소스 버전
      - 메스메티카(Mathmatica) - 상용 데이터 분석, 기호 처리, 수치 계산 시스템. 과학자와 엔지니어가 주로 사용함
      - 사이파이(SciPy), 넘피(NumPy) - 과학 프로그램을 위한 파이썬 확장 소프트웨어 패키지. 데이터 과학자 사이에 널리 사용됨 

# 시작하기
- VMware용 하둡 가상 머신 
  - 클라우데라(Cloudera) - 클라우데라가 자체적으로 배포하는 하둡 버전을 사용하고 있음
  - 맵알(MapR) - HDFS를 MapR 파일시스템으로 대체하여 자체적으로 배포하는 하둡 버전을 사용하고 있음 
  - 홀튼웍스(Hortonworks) - 가장 최신의 안정된 아파치 배포 하둡을 사용함. 현재는 배포 중지한 상태
  - 씽크 빅 아날리틱스(Think Big Analytics) - 가장 최신의 안정된 아파치 배포 하둡을 사용함 
- 로컬 모드, 의사 분산 모드, 분산 모드 
  - 런타임 모드, 기본 모드는 로컬 파일시스템을 사용하는 로컬 모드. 하이브 쿼리를 포함하는 하둡 잡(Job)이 실행될 때 맵 태스크와 리듀스 태스크가 동일한 프로세스에서 동작함 
  - 실제 클러스터는 분산 모드로 설정함. 잡은 이 클러스터 상의 잡트래커(JobTracker)에 의해서 관리되고 각각의 태스크는 각기 다른 프로세스에서 동작함 
  - 하나의 머신에서 의사 분산 (pseudodistributed) 모드로 동작하게 설정할 수 있음 
  - 의사 분산 모드는 분산 모드와 같게 동작함. 파일 시스템은 분산 파일시스템을 기본으로 참조하고 잡은 잡트래커 서비스로 관리함. 다른 점은 하나의 컴퓨터라는 점 
  - HDFS 블록의 복사 개수를 하나로 제한함. 노드가 하나인 클러스털처럼 동작함 
- 하이브 구성 
  - 쓰리프트 서비스는 다른 프로세스에서 원격으로 접근할 수 있도록 해줌. JDBC와 ODBC로 접근할 수 있는 방법을 제공하는데, 이는 쓰리프트 위에서 동작함
  - 하이브를 설치할 대 테이블 스키마와 같은 메타데이터를 저장하기 위해서 메타스토어(metastore)서비스가 필요함 , 메타스토어는 관계형 데이터베이스의 테이블로 구현함 
  - 하이브는 기본 데이터베이스로 프로세스에 내장되어 있고 제한된 기능을 제공하는 더비(Derby) SQL 서버를 사용함 
  - 원격으로 하이브에 접근할 수 있는 웹 인터페이스인 HWI(Hive Web Interface) 를 제공함
- 하둡 환경 설정하기
  - 분산과 의사 분산 모드 설정
    - 분산 모드에서는 여러 서비스가 클러스터 내에서 동작함. 잡트래커는 잡을 관리하고 네임노드는 HDFS 마스터.
    - 작업 노드는 태스크트래커(TaskTracker)와 데이터노드(DataNode)로 구성됨 
    - 태스크트래커는 잡태스크를 구동하고 관리하며 데이터노드는 분산 파일시스템에 저장된 파일을 구성하는 블록을 관리함 
  - JDBC를 사용하는 메타스토어
    - 메타스토어는 create table x... 또는 alter table y... 등과 같은 명령을 수행할 때 명시되는 테이블 스키마와 파티션 정보와 같은 메타데이터를 저장하고 있음 
    - 메타스토어가 단일 고장점(SPOF : Single Point Of Failure)이기 때문에 안정된 서비스를 보장하기 위해서 메타데이터를 복제하고 백업하기 위한 방법을 강구해야 함 
- 하이브 명령
  - 하이브 서비스
    - CLI(명령행 인터페이스) - 테이블을 정의하거나 쿼리 등을 수행하기 위해 사용됨. 다른 서비스가 명시되어 있지 않으면 기본 서비스로 동작함 
    - hiveserver(하이브 서버) - 별도의 프로세스에서 쓰리프트 연결을 위해 대기하고 있는 데몬 
    - hwi(하이브 웹 인터페이스) - 클러스터로 로그인하거나 CLI를 사용하지 않고 쿼리나 다른 명령을 수행하기 위한 간단한 웹 인터페이스
    - jar - 하이브 환경에서 애플리케이션을 수행하기 위한 것으로 hadoop jar 명령의 확장이라고 이해하면 됨
    - metastore - 다중 클라이언트를 지원하기 위해서 하이브 외부에 메타스토어를 구동하는 서비스 
    - rcfilecat - RCFile의 내용을 출력하기 위해서 사용되는 도구 
  
  # 데이터형과 파일 포맷
  - 원시 데이터형
    - 하이브는 여러 크기의 정수형과 부동소수점 및 하나의 불린형(Boolean)과 임의의 길이를 가지는 문자열을 제공함 
    - 원시 데이터형
      - TINYINT - 1바이트 크기의 정수 데이터형 (20)
      - SMALLINT - 2바이트 크기의 정수 데이터형 (20)
      - INT - 4바이트 크기의 정수 데이터형 (20)
      - BIGINT - 8바이트 크기의 정수 데이터형 (20)
      - BOOLEAN - TRUE 또는 FALSE - TRUE
      - FLOAT - 단정도 부동소수점 - 3.14159
      - DOUBLE - 배정도 부동소수점 - 3.14159
      - STRING - 문자의 시퀀스, 문자열 셋도 설정 가능작은 따옴표 혹은 큰 따옴표 생성 가능  
      - TIMESTAMP - 정수형, 부동소수점형, 문자열형
      - BINARY - 바이트 배열 형태 지원
    - 하이브는 다른 SQL에서 일반적으로 지원하는 최대 허용 길이를 갖는 문자 배열(character array)을 지원하지 않음. 관계형 데이터베이스에서 성능 최적화 기능으로 색인이나 스캔 등이 용의한 고정된 길이의 레코드를 지원함 
    - 하이브가 속한 느슨한 세계(looser world)에서는 자체 데이터 저장소를 가지지 않고 다양한 파일 포맷을 지원해야 하기 때문에 필드를 구분할 수 있는 구분 기호에 의존함 
    - 하둡 및 하이브는 디스크의 읽기, 쓰기 성능 최적화를 강조하기 때문에 컬럼값의 길이를 고정하는 것은 상대적으로 중요하지 않음 
- 컬렉션 데이터형
  - STRUCT - C 언어의 구조와 객체와 유사함 - struct('John','Doe')
  - MAP - ['key']처럼 필드를 배열 표기법으로 접근할 수 있는 키-값 형태의 컬렉션 데이터형, map('first','John','last','Doe')
  - ARRAY - 0으로 시작하는 정수로 색인할 수 있는 동일한 데이터형의 순차 시퀀스 - array('John','Doe')
  - 컬렉션 데이터형은 정규화를 깰 수 있기 때문에 대부분의 관계형 데이터베이스에서는 컬렉션 데이터형을 지원하지 않음
  - 정규화를 깸으로써 발생하는 더 큰 문제는 데이터 중복, 저장 공간 낭비, 잠재적 데이터의 불일치가 있음. 중복된 데이터는 데이터에 어떤 변경이 발생할 때 동기화되는 데이터를 증가시킬 수 있음 
  - 테라바이트에서 페타바이트 급의 데이터를 검색할 때 최소한의 디스크 탐색은 필수. 컬렉션 데이터형을 이용하면 최소한의 디스크 탐색으로 레코드를 빠르게 검색할 수 있음. 그러나 외래 키 관계의 데이터를 탐색하는 것은 상당한 성능 오버 헤드와 함께 여러 디스크를 걸치는 탐색을 요구함 
- 데이터값의 텍스트 파일 인코딩
  - 쉼표로 필드를 구분하는 텍스트 파일(CSVs:Comma-separated values) 또는 탭으로 필드를 구분하는 텍스트 파일(TSVs:tab-separated values)
  - 하이브의 레코드와 필드 기본 구분 기호
    - \n : 텍스트 파일에서 각 줄은 하나의 레코드가 됨. 그러므로 줄 바꿈 문자는 레코드를 분리함
    - ^A : 모든 컬럼을 분리함. CREATE TABLE문에서 명시적으로 지정할 때는 8진수 코드 '\001'을 사용함
    - ^B : ARRAY, STRUCT 또는 MAP 키-값 쌍의 요소를 분리함
    - ^C : MAP의 키-값 쌍에서 키를 관련된 값과 분리함 
- Schema on Read
  - schema on write 
    - 전통 데이터베이스에서 외부 데이터 로딩, 쿼리 결과물 혹은 UPDATE 문 수행을 통하여 데이터를 쓸 때 데이터베이스는 데이터 저장소 모두를 제어함
    - 데이터베이스는 문지기(gatekeeper)가 되는 것
    - 데이터베이스가 데이터를 쓸 때 스키마를 강제로 맞추려 하기 때문
  - schema on read
    - 하이브는 저장소에 대해 쓰기 제어를 가지고 있지 않음. 생성, 수정, 그리고 심지어 하이브가 쿼리할 때 데이터를 손상시키는 여러 방법이 있음. 하이브는 단지 쿼리 결과를 읽어들일 때에만 스키마를 적용함 

# HiveQL : 데이터 정의
- 하이브는 로우 레벨의 삽입과 변경 그리고 삭제를 지원하지 않음. 트랜잭션 또한 지원하지 않음. 하둡이 지원하는 범위 안에서 보다 나은 성능을 위해 확장할 수 있는 기능을 제공하며 사용자가 정의한 확장과 외부 프로그램을 하이브와 연동할 수 있음 
- 데이터 정의 언어는 데이터베이스, 테이블, 뷰, 함수, 색인을 생성하고 변경하고 삭제하는 데 사용함 
- 하이브에서의 데이터 베이스
  - 하이브에서 데이터베이스 개념은 단지 테이블의 카탈로그 또는 네임스페이스. 여러 팀과 사용자가 어우러져 큰 규모로 작업할 때 테이블명의 충돌을 막는 매우 유용한 방법
  - 데이터베이스 테이블을 논리 그룹으로 구성하는 데 사용
  - IF NOT EXISTS 문으로 필요할 때만 데이터베이스를 즉시 생성 
  - USE 명령어는 파일시스템에서 작업(working) 디렉터리를 변경하는 것과 비슷하게 작업 데이터베이스를 설정하는 데 사용함 
- 테이블 생성
  - 하이브는 자동으로 두 개의 테이블 속성을 넣음. last_modified_by 속성은 테이블을 수정한 마지막 사용자의 이름을 저장하고 last_modified_time 속성은 수정한 유닉스 표준 시간을 담음 
  - 매니지드 테이블
    - 생성한 테이블은 하이브가 데이터의 생명 주기를 제어하기 때문에 매니지드 테이블(Managed Table) 또는 내부 테이블로 부름 
    - 매니지드 테이블 대신 외부 테이블(External Table)을 정의하여 하이브가 해당 데이터를 소유하지 않도록 함 
  - 외부 테이블
    - EXTERNAL 예약어는 해당 테이블이 외부에 있으면 LOCATION ... 절엣허 지정한 위치에 존재한다는 것을 하이브에게 알려줌 
    - 외부에 존재하기 때문에 하이브는 해당 데이터를 소유하지 않음. 테이블을 삭제할 때 테이블의 메타데이터는 지워지지만 해당 데이터를 지우지는 않음 
- 파티셔닝된 매니지드 테이블
  - 많은 형태를 가질 수 있지만 수평적으로 부하를 분산하기 위해 자주 사용하는 데이터를 사용자와 물리적으로 가까운 위치에 두는 등의 목적으로 사용함 
  - 데이터를 파티셔닝하는 가장 중요한 이유는 빠른 쿼리를 위해서
  - 특정값을 걸러내는 WHERE 조건절을 포함할 때 이러한 조건을 파티션 필터라 부름 
  - > SHOW ParTITIONS {table} : 파티셔닝된 정보를 확인
  - > DESCRIBE EXTENDED {table} : 파티션 키를 보여줌 
  - 파티셔닝된 외부 테이블
    - 로그 파일 분석, 대부분의 기관은 시간, 심각도(severity, ex - ERROR, WARNING, INFO), 서버 이름, 프로세스 아이디, 그리고 임의의 텍스트 메신저로 이루어진 표준 포맷의 로그 메시지를 사용함 
    - 한 달 이상된 오래된 데이터를 S3로 이동
      - > hadoop distcp /data/log_messages/2011/12/02 s3n://ourbucket/logs/2011/12/02
      - 해당 파티셔닝 내 데이터를 S3로 복사함. 
      - > ALTER TABLE log_messages PARTITION(year=2011, month=12, day=2) SEL LOCATION 's3n://ourbucket/logs/2011/01/02';
      - 테이블의 파티셔닝이 S3 위치를 가리키도록 변경함 
      - > hadoop fs -rmr 명령어를 이용하여 HDFS 파티셔닝을 삭제함 
      - hdaddop fs -rmr 명령어를 이용하여 HDFS 파티셔닝을 삭제함
    - 새로운 데이터는 다른 디렉터리 내에 존재하는 오래된 데이터로부터 명확히 구분된 특정 디렉터리에 쓰여짐. 오래된 데이터를 아카이브 위치로 옮기거나 명확히 삭제하더라도 별도의 디렉터리에 데이터가 존재하기 때문에 새로운 데이터에 영향을 미칠 위험은 줄어듬 
  - 테이블 저장 포맷 사용자화
    - 하이브는 레코드들이 파일 내에서 인코딩되는 방법과 컬럼들이 레코드 내에서 인코딩되는 방법을 구분함 
    - 레코드 인코딩은 TEXTFILE 내부에 조재하는 자바 클래스(org.apache.hadoop.mapred.TextInputForamt)와 같은 입력 포맷 객체(Input Format Object)에 의해 다루어짐 
    - 레코드 분석은 serializer/deserializer 혹은 줄여서 SerDE(쎄데,쎄르데)에 의해서 행해짐 
- 테이블 삭제
  - > DROP TABLE IF EXISTS {table};
  - IF EXISTS 예약어는 옵션. 예약어가 없고 테이블이 조재하지 않을 때 하이브는 에러를 돌려줌 

# HiveQL : 데이터 조작
- 데이터 조작 언어(Data Manipulation Language) : 테이블에 데이터를 넣거나 테이블에서 원하는 데이터를 꺼내어 파일 시스템에 옮김 
- 매니지드 테이블에 데이터 로딩하기
  - 하이브는 로우 레벨에서 삽입,갱신,삭제를 지원하지 않음. 
  - 벌크(bulk) : 데이터를 테이블에 넣는 유일한 방법은 벌크로 로딩하는 것뿐, 아니면 테이블 디렉터리에 직접 파일을 복사하는 방법 
  - ```sql
    LOAD DATA LOCAL INPATH '{$env:HOME}/california-employees'
    OVERWRITE INTO TABLE employees
    PARTITON (country = 'US', state = 'CA');
    ```
  - 해당 파티션을 위한 디렉터리가 존재하지 않을 때 디렉터리를 생성하고 여기에 데이터를 복사함
  - LOCAL 예약어를 사용하면 파일 위치는 로컬 파일시스템에 존재하는 것으로 간주하며 데이터는 테이블 디렉터리로 복사됨 
    - LOAD DATA LOCAL...은 로컬 파일시스템의 파일을 분산 파일시스템으로 복사하며 LOAD DATA ...는 해당 데이터를 최종 위치로 이동시킴
    - 보통 뿐산 파일시스템에 데이터 파일의 사본을 갖는 것을 원하지 않는다는 가정에 기반함 

# HiveQL : 쿼리
- SELECT ... FROM 절
  - SQL에서 SELECT 프로젝션(projection) 연산자. FROM 절은 레코드를 선택하기 위해 필요한 테이블. 뷰 또는 중첩 쿼리(nested query)를 식별함 
  - 컬렉션 데이터형의 컬럼을 선택하면 하이브는 출력을 위해 JSON(Java Script Object Notation)문법을 사용함. 
  - ARRAY 데이터형은 쉼표로 구분된 목록이 [...]로 둘려싸여 있음 
  - MAP의 경우 쉼표로 구분된 키:값 쌍의 목록을 {...}로 둘러싸는 JSON 표현을 사용함 
  - STRUCT로 JSON 맵 형식을 사용함 
  - 하이브는 오버 플로우나 언더플로우가 발생할 때 더 넓은 범위의 데이터형이 존재하더라도 결과를 자동으로 변환하지 않는 자바 데이터형 규칙을 따름 
- 기타 내장 함수
  - > parse_url(url,partname,key) : HOST, PATH, QUERY, REF,PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY:<key>. 옵션 키는 마지막에 QUERY:<key>를 요청함 
  - > find_in_set(s, 쉼표로 구분된 String) : 쉼표로 구분된 문자열에서 s의 색인을 반환함. 찾지 못하면 NULL이 반환됨
  - > locate(substr,str,pos) : str의 post 위치로부터 substr이 있는 색인을 반환함 
  - > instr(str,substr) : str에서 substr의 색인을 반환함
  - > str_to_map(s,delim1,delim2) : delim1을 키-값 쌍의 구분자로 사용하고 delim2를 키와 값의 구분자로 사용하여 문자열 s를 파싱한 후 맵을 생성함 
  - > sentences(s,lang,locale) : 문자열 s를 단어의 배열로 이루어진 문장의 배열로 반환함 
  - > ngrams(array<array<string>>, N, K, pf) : 텍스트에서 top-K n-gram을 반환함. pf는 정밀도
  - > context_ngrams(array<array<string>>,array<string>, int K, int pf> : ngrams와 같지만 출력 배열에서 두 번째 단어 배열로 시작하는 n-gram을 찾음
  - > in_file(s, filename) : filenmae 파일에서 문자열 s가 나타나면 true를 반환함 
- WHERE 절
  - WHERE 절에서 컬럼 별칭을 사용할 수 없음. 하지만 중첩 SELECT 문은 사용할 수 있음 
  - ```sql
    SELECT e.* FROM 
    (SELECT name, salary, deductions['Federal Taxes"] as ded,
    salary * (1 - deductions['Federal Taxes"]) as salary_minus_fed_taxes
    FROM employees) e
    WHERE round(e.salary_minus_fed_taxes) > 70000;
    ```
  - LIKE와 RLIKE
    - 하이브는 LIKE 절을 자바 정규표현식을 사용할 수 있는 RLIKE절로 확장 
    - 마침표(.)는 어떠 한 문자와 일치하고 별(*)은 왼쪽에 있는 것이 0번에서 여러 번 반복되는 것을 의미함 
    - (x|y) 표현식은 x 혹은 y가 일치하는 것을 의미함 
- 조인 문
  - 하이브는 고전적인 SQL 조인 문을 제공함. 동등 조인(EQUAL-JOIN)만 제공함 
  - 조인 최적화
    - 하이브는 쿼리의 마지막 테이블이 가장 크다고 가정함. 다른 테이블을 버퍼링하려고 시도하고 각 레코드에 대해서 조인을 수행하면서 마지막 테이블을 흘려보냄. 조인 쿼리를 구성할 때는 가장 큰 테이블이 가장 마지막에 오도록 함 
    - 하이브는 쿼리 최적화(optimizer)이기에 어떤 테이블을 마지막으로 흘려보내야 하는지 지정하는 힌트(hint) 메카니즘을 제공함
    - ```sql
      SELECT /** STREAMTABLE(s) */ s.ymd, s.symbol, s.price_close, d.dividend
      FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol
      WHERE s.symbol = 'AAPL';
      ```
    - 중첩 SELECT 문
    - ```sql
      SELECT s.ymd, s.symbol, s.price_close, d.dividend FROM
      (SELECT * FROM stocks WHERE symbol = 'AAPL' AND exchange = 'NASDAQ') s
      LEFT OUTER JOIN
      (SELECT * FROM dividends WHERE symbol = 'AAPL' AND exchange = 'NASDAQ') d
      ON s.ymd = d.ymd;
      ```
    - 중첩 SELECT 문은 데이터 조인 전에 파티션 필터를 적용하는 데 필요한 푸시다운(push down)을 수행함
      - 푸시다운은 WHERE 절의 술어 중 일부를 떼어내어 미리 실행하는 것을 말함. 리지주얼(residual)은 푸시ㅏ운 후에 남은 술어를 일컫음 
      - 하이브는 조인을 수행한 후에 WHERE 절을 평가함. WHERE 절은 NULL이 되지 않는 컬럼값에 대해서만 필터를 적용하는 술어를 사용해야함. 하이브 문서와는 달리 파티션 필터는 외부 조인의 ON 절에서 동작하지 않음 
  - 왼쪽 세미 조인
    - 왼쪽 세미 조인(LEFT SEMI-JOIN)은 오른쪽 테이블에서 ON의 술어를 만족하는 레코드를 찾을 경우 왼쪽 테이블의 레코드를 반환함, 하이브는 오른쪽 세미 조인을 지원하지 않음
  - 맵 사이드 조인(Map-side Join)
    - 만약 한 테이블만 빼고 모두 작다면 작은 테이블은 메모리에 캐시하고 가장 큰 테이블은 맵퍼로 흘려보낼 수 있음. 하이브는 메모리에 캐시한 작은 테이블로부터 일치하는 모든 것을 찾아 낼 수 있기 때문에 맵에서 모든 조인을 할 수 있음. 이렇게 하면 일반 조인 시나리오에서 필요한 리듀스 단계를 제거할 수 있음 
    - 더 작은 데이터셋에 대해서도 이 최적화는 일반 조인보다 눈에 띌 만큼 빠름. 리듀스 단계를 제거할 뿐만 아니라 맵 단계 역시 줄어들기 때문
    - ```sql
      SELECT /*+ MAPJOIN(d) */ s.ymd, s.symbol, s.price_close, d.dividend 
      FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol
      WHERE s.symbol = 'AAPL'
      ```
    - 하이브는 오른쪽 외부 조인과 완전 외부 조인에 대해서 최적화를 지원하지 않음
- ORDER BY와 SORT BY
  - 하이브는 ORDER BY 대신 데이터를 각 리듀서에 정렬하는 SORT BY를 추가함. 각 리듀스의 출력이 정렬되도록 지역 정렬(local ordering)을 수행하는 것을 의미
- SORT BY와 함께 사용하는 DISTRIBUTE BY
  - DISTRIBUTE BY는 맵의 출력을 리듀서로 어떻게 나누어 보내는지를 제어함 
  - 기본적으로 맵리듀스는 맵퍼가 출력하는 키에 대해서 해시값을 계산하고 해시값을 이용하여 키-값 쌍을 가용한 리듀서로 균등하게 분산하려고 노력함 
  - 맵퍼에서 출력한 키-값 쌍의 값을 계산하여 리듀서를 선택하는 것은 파티셔너의 역할 
  - SORT BY는 리듀서 안에서 데이터 정렬을 제어하는 반면 DISTRIBUTE BY는 리듀서가 처리할 로우를 어떻게 받는지 제어한다는 점에서 GROUP BY처럼 동작함
  - 하이브는 SORT BY 절 전에 DISTRIBUTE BY 절을 사용할 것을 요구하므로 주의해야 함 
- CLUSTER BY
  - DISTRIBUTE BY ... SORT BY 또는 간단히 사용하는 CLUSTER BY 절은 출력 파일 간에 전체 정렬을 이루면서 SORT BY의 병렬 처리를 사용하도록 하는 방법 
- 데이터 표본을 만드는 쿼리
  - 매우 큰 데이터셋에 대해서 전체를 사용하는 것이 아니라 어떤 쿼리를 수행하여 나온 결과를 대표 표본으로 하여 작업하고자 하는 경우가 종종 있음
  - 하이브는 테이블을 버킷으로 구성하여 표본을 만든느 쿼리로 이를 지원함 
  - 버킷 테이블에 대한 입력 푸루닝
    - pruning - 푸루닝은 데이터 분석 작업에 불필요한 데이터를 미리 잘라내는 작업을 말함. 가지치기, 추리기 정도로 번역할 수 있으나 데이터베이스나 데이터 분석에 많이 사용하는 용어이므로 원어 그대로 사용 

# HiveQL : 뷰
- 쿼리는 뷰를 테이블처럼 저장하거나 다룸. 하지만 테이블처럼 데이터르리 저장하지는 않기 때문에 논리적인 구조물 
- 하이브는 매티리얼라이즈드 뷰(materialized view)를 지원하지 않음
- 쿼리의 복잡합을 줄여주는 뷰
  - 캡슐화(encapsulation)화는 최종 사용자로 하여금 재사용 가능한 부분을 이용하여 복잡한 쿼리를 더 이해하기 쉽게 만듬, 하이브 쿼리는 일반적으로 다단계의 중첩 구조로 되어있음
- 동적 테이블을 위한 뷰와 MAP 데이터형
  - 하이브는 ARRAY, MAP, STRUCT 데이텨형을 지원함. 전통적인 데이터베이스는 제 1 정규형(first normal form)을 깬다는 이유로 이런 데이터형을 지원하지 않음
  - 하이브는 한 줄의 텍스트를 고정된 개수의 컬럼만으로 처리하는 전통적인 데이터베이스와는 달리 MAP으로도 처리할 수 있음. 이 기능을 활용하면 뷰와 결합하여 한 물리 테이블에서 여러 논리 테이블을 정의할 수 있음 
- 기타
  - 하이브는 뷰를 먼저 수행한 후 쿼리를 수행함. 그러나 하이브는 쿼리 최적화 차원에서 쿼리와 뷰 두 절을 하나의 실제 쿼리로 결합할 수 있음 
  - 뷰와 뷰를 사용하는 쿼리 둘 다 ORDER BY 절이나 LIMIT 절을 가지고 있다면 뷰의 개념이 여전히 적용됨. 쿼리 절을 사용하기 전에 뷰 절을 먼저 수행하는 것 
  - 테이블과 마찬가지로 DESCRIBE shipments와 DESCRIBE EXTENDED shipments는 shipments 뷰에 대한 유용한 정보를 보여줌. 후자를 사용하면 Detailed Table Information의 tableType 속성은 VIRTUAL_VIEW 값을 가지고 있을 것
  - 뷰를 INSERT나 LOAD 명령의 대상(traget)으로 사용할수 없음. 뷰는 읽기 전용. 뷰의 TRLPROPERTIES 메타데이터만 변경할 수 있음 

# HiveQL : 색인
- 색인은 또한 논리적 파티션이 크기는 작고 개수가 많은 경우 파티셔닝을 대신해서 사용하기에도 좋음. 색인은 맵리듀스 잡의 입력으로 사용할 테이블 블록을 푸루닝(데이터 분석에 불필요한 데이터를 미리 잘라내는 작업을 말함)하는데 도움을 줄 수 있음
- 색인 생성
  - ```sql
    CREATE INDEX employees_index
    ON TABLE employees (country)
    AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'
    WITH DEFERED REBUILD
    IDXPROPERTIES ('creator = 'me', 'created_at' = 'some_time')
    IN TABLE employees_index_table
    PARTITIONED BY (country, name)
    COMMENT 'Employees indexed by country and name.';
    ```
  - 그래뉴얼리티(granularity) : 어떤 대상을 잘게 쪼개는 정도를 나타내는 용어
  - AS ... 절은 색인을 구현한 자바 클래스를 색인 핸들러(index handler)로 지정함
  - 비트맵 색인
    - 비트맵 색인은 컬럼에 고유값이 많지 않은 경우 사용함 
- 색인 재구축
  - 만약 WITH DEFERRED REBUILD를 지정했다면 새로운 색인은 비어 있는 상태에서 시작함, ALTER INDEX 문을 이용하여 언제든지 재구축할 수 있음
  - ```sql
    ALTER INDEX employees_index
    ON TABLE employees
    PARTITION (country = 'US')
    REBUILD;
    ```
- 색인 보기
  - > SHOW FORMATTED INDEX ON employees;
  - 테이블의 어떤 컬럼에 대해서든 정의되어 있는 모든 색인을 보여줌 
- 색인 삭제
  - 색인을 사젝하면 색인 테이블도 삭제됨 

# 스키마 설계
- 파티션 설계 시 고려사항
  - 네임노드는 파일 시스템에 대한 모든 메타데이터를 메모리에 유지하고 있어야함. 각 파일의 메타데이터를 저장하기 위해서 대략 150바이트 정도의 작은 메모리가 필요한데 이것이 결국 HDFS가 관리할 수 있는 파일의 최대 개술르 제한함 
  - MapR이나 아마존 S3와 같은 다른 파일시스템은 이런 제한이 없음 
- 고유 키와 정규화
  - 관계형 데이터베이스는 데이터베이스 메모리 구조에 맞게 데이터를 저장하기 위해 일반적으로 고유 키(unique key), 색인, 정규ㅜ화(normalization)을 사용함 
  - 하이브에는 기본 키와 자동으로 순차적인 키를 생성하는 개념이 없음. 비정구화된 데이터에 대한 조인은 가능하면 피해야 함 
  - 하나의 로우에서 데이터를 일대다(one-to-many) 형태로 저장하기 위해서는 ARRAY,MAP,STRUCT와 같은 컬렉션 데이터형을 사용함. 정규화를 절대 사용하지 말라는 것이 아니라 스타 스키마(star-schema)를 사용하여 설계하라는 뜻
  - 정규화를 피해야하 하는 주요 이유는 외래 키 관계를 찾아다니면서 생기는 디스크 탐색을 최소화하기 위해서. 데이터를 비정규화하면 디스크 상의 인접한 대규모의 데이터를 찾아보거나 쓰기 작업을 할 때에 최적의 I/O 성능을 낼 수 있음. 비정규화로 발생하는 데이터 중복과 데이터 불일초로 인한 더 큰 위험을 감수해야 함 
- 동일 데이터에 대한 다중 패스 만들기
  - 하이브에는 한 번의 소스 데이터 통과로 반복 스캔 없이 여러 번 집계를 할 수 있는 특별한 문법이 있음 
  - 다중 패스(multiple passes) 기능은 대규모 입력 데이터를 처리할 때 상당한 시간을 절약해줌 
  - ```sql
    INSERT OVERWRITE TABLE sales
    SELECT * FROM history WHERE action='purchased';
    INSERT OVERWRITE TABLE credits
    SELECT * FROM history WHERE action='returned';
    -- 문제없이 정확하지만 비효율적이라 아래처럼 변경 
    FROM history
    INSERT OVERWRITE sales SELECT * WHERE action='purchased'
    INSERT OVERWRITE credits SELECT * WHERE action='returned';
    ```
- 테이블에 컬럼 추가하기
  - SerDe는 일반적으로 왼쪽에서 오른쪽으로 파싱해가면서 특정 컬럼 구분자를 사용하여 로우를 나눔
  - SerDe를 ETL에서 쿼리를 통해서 데이터를 가져와 적합한 포맷으로 변환하기 위한 필터 정도로 이해하면 됨 
- 컬럼 기반 테이블 사용하기
  - 하이브는 전형적으로 로우 지향(row oriented) 저장소를 사용하지만 로우와 컬럼을 둘 다 지향하는 하이브리드 형태로 정보를 저장하는 컬럼 기반의 SerDe 또한 가지고 있음 

# 튜닝
- HiveQL은 선언형 언어(declarative language), 사용자가 선언적 쿼리를 내보내면 하이브는 쿼리를 어떻게 맵리듀스 잡으로 변환할지 알아냄 
  - 문제를 해결하는 절차(control flow)를 기술하지 않고 문제를 설명하는(logic of computation) 고급 언어. 어떻게 할지를 기술하는 것이 아니라 무엇을 할지를 기술함. 반대 개념의 언어는 어떻게 할지를 기술하는 명령형 언어(imperative language). 선언형 언어의 예로는 SQL, 정규식, 논리 프로그래밍 언어, 함수 프로그래밍 언어 등이 있음
- EXPLAIN 사용하기
  - EXPLAIN 기능을 사용하여 하이브가 쿼리를 어떻게 맵리듀스 잡으로 변환하는지를 살펴보는 것 
  - 추상 문법 트리(abstract syntax tree)가 출력됨. 하이브가 어떻게 쿼리를 토큰(token)과 리터럴(literals)로 분해하는지 보여줌
  - 파서(parser)와 토크나이저(tokenizer)에 익숙하지 않은 사용자는 내용을 이해하기 어려울 것(일단 TOK_ 접두사는 무시해라)
  - 하이브 잡은 하나 이상의 스테이지(stage)로 구성됨. 스테이지 사이에는 의존관계가 있음. 
  - 스테이지는 맵리듀스 잡, 샘플링(sampling) 스테이지, 병합 스테이지, 결과 제한(limit) 스테이지를 비롯해 그 외 하이브가 필요한 일을 하는 스테이지로 구성될 수 있음. 하이브는 이러한 스테이지를 한 번에 하나씩 실행함 
- EXPLAIN EXTENDED
  - EXPLAIN과의 차별점은 Reduce Operator Tree 부분 
- LIMIT 튜닝
  - LIMIT 절은 여전히 데이터 전체에 대해 쿼리를 실행하고 일부 결과만 반환함, 이는 낭비이기 때문에 가능한 피해야 함 
- 최적화된 조인
  - 가장 큰 테이블을 조인 절 마지막에 두거나 /*streamtable(table_name) */ 지시문(directive)을 사용하는 것은 꼭 기억하기 
  - 나머지 테이블은 모두 크지만 한 테이블만 메모리에 올릴 수 있을 정도로 충분히 작다면 하이브는 맵 사이드 조인을 수행할 수 있음. 리듀스 태스크가 필요 없을 뿐만 아니라 일부 맵 태스크도 없앨 수 있음 
- 로컬 모드
  - 많은 하둡 잡이 대규모 데이터셋을 처리하기 위해 하둡의 규모 가변성(scalability)을 활용함 
- 병렬 수행
  - 하이브는 쿼리를 하나 이상의 스테이지(stage)로 변환함. 스테이지는 맵리듀스 스테이지, 샘플링, 병합, 결과 개수 제한 등 하이브가 필요로 하는 모든 것이 될 수 있음 
  - 하이브는 기본적으로 한 번에 한 스테이지만 실행함. 하지만 종속 관계가 없는 스테이지가 포함된 잡은 더 일찍 수행을 마칠 수 있게 병렬로 실행할 수 있음 
- strict모드
  - 하이브를 strict 모드로 설정하면 사용자가 의도하지 않거나 원하지 않는 효과를 발생시키는 쿼리 입력을 막아줌 
  - 파티션이 있는 테이블은 WHERE 절에 범위를 제한하는 파티션 필터를 포함하지 않으면 쿼리를 실행할 수 없음. 전체 파티션을 검색하는 쿼리를 막음 
  - LIMIT 절 없이 ORDER BY 절을 사용하는 것. ORDER BY는 정렬을 수행하기 위해 모든 결과를 단일 리듀서에 보내기 때문. LIMIT 절을 명시하도록 강제하는 것은 리듀서가 예상보다 긴 시간 동안 수행하는 것을 막아줌 
  - 카타시안 프로덕트(cartesian product)
- 맵퍼와 리듀서 수의 최적화
  - 하이브는 쿼리를 한 개 이상의 맵리듀스 잡으로 나누어 처리하여 병렬로 수행함. 맵리듀스 각각은 다수의 맵퍼와 리듀서 태스크로 되어 있는데, 그중 적어도 몇몇은 병렬로 수행될 수 있음
  - 적절한 수의 맵퍼와 리듀서 수를 결정하는 일은 입력의 크기, 데이터에 수행하는 연산의 종류 등과 같은 다양한 변수에 의존적
  - 너무 많은 맵퍼와 리듀서 태스크는 잡을 초기화하고 스케줄링하고 실행하기 위해 많은 부하를 주는 반면, 너무 적은 태스크는 클러스터가 가진 병렬성을 제대로 활용하지 못하기 때문 
- 자바 가상 머신 재사용
  - 자바 가상 머신(JVM)의 재사용은 하이브 성능과 매우 밀접함
  - 특히 작은 파일을 처리해야 하는 경우와 대부분의 태스크가 수행 시간이 짧은 상황을 처리해야만 할 때 유용함
  - 하둡의 기본 설정은 일반적으로 포크된(forked) 자바 가상 머신에서 맵 또는 리듀서 태스크를 실행함 
- 색인
  - 색인은 GROUP BY 쿼리의 계산 속도를 높이는 데 사용됨
  - 비트맵 색인은 (도시명 같이 한정된 수의 값이 있는 컬럼) 주어진 컬럼에 고유값이 비교적 적을 때 주로 사용됨
  - 동적 파티셔닝 튜닝
    - 동적 파티션 INSERT 문은 파티션된 테이블에 많은 새 파티션을 생성하여 SELECT 문을 간단하게 만듬
    - 하이브의 기본 설정은 1,000개 이상의 동적 파티션을 생성하는 것을 막음
- 투기적 실행
  - 투기적 실행(speculative execution)은 중복된 태스크를 동시에 수행시키는 하둡의 기능. 같은 데이터를 중복하여 복사하기 때문에 더 많은 리소스를 씀. 대부분의 데이터는 버려짐
  - 이 기능의 목적은 느리게 동작하는 태스크트래커를 제거함으로써 개별 태스크의 결과가 더 도출되고 결과적으로 전체 잡 수행을 향상시키는 데 있음
    - 하둡은 동일한 태스크를 여러 노드에서 동시에 실행하여 혹시라도 특정 노드가 노후 장비 같은 문제 때문에 수행이 느려지더라도 다른 노드에서 먼저 끝나면 해당 결과만 사용하고 나머지 태스크는 중단함. 결과적으로 전체 수행 시간은 줄어들지만 시스템의 자원을 추가로는 더 사용한다는 단점이 있음 
- 가상 컬럼
  - 하이브는 두 개의 가상 컬럼을 제공함. 하나는 스플릿의 입력 파일 이름이고, 나머지는 파일에 대한 블록 오프셋임
  - 하이브가 기대하지 않았던 값이나 null을 결과값으로 생성할때 쿼리를 추적하기 위해 사용함 
  
# 기타 파일 포맷과 압축
- 데이터를 특정 포맷에 강제로 맞추지 않는 것은 하이브만의 고유 기능. 하이브는 하둡의 입력 포맷(InputFormat) API를 이용해 텍스트 파일, 시퀀스파일, 사용자 정의 파일과 같은 다양한 소스로부터 데이터를 읽음. 출력 포맷(OutputFormat) API를 이용하면 다양한 포맷으로 데이터를 쓸 수도 있음 
- 압축은 많은 양의 디스크를 절약함. 텍스트 기반의 파일은 40% 이상의 압축률을 보여줌. 압축은 처리량과 성능을 향상시킴. 압축을 하고 푸는 일이 CPU에 많은 부하를 주는 것이기 때문에 납득이 가지 않겠지만 메모리에 더 적은 바이트를 보내면서 얻은 I/O 절약은 네트워크 성능에 이득을 줄 수 있음
- 압축 코덱 선택
  - 압축을 사용하면 파일 저장에 필요한 디스크 공간 사용을 최소화하고 디스크와 네트워크 부하를 줄여주는 장점이 있음. 파일을 압축하고 푸는 작업은 CPU에 많은 부하를 줌.여분의 CPU 용량이 있거나 디스크 공간이 매우 중요한 I/O 바운드 잡에서 압축을 사용하는 것이 좋음
  - 모든 하둡 최신 버전은 GZip, BZip2 압축 방식과 이런 포맷의 성능을 가속화하는 리눅스 라이브러리를 내장하고 있음. Snappy 압축도 포함되어있음. LZO 압축도 자주 사용함 
  - BZip2는 가장 높은 압축률을 보이지만 CPU를 가장 많은 씀. 속도 대비 압축률에 있어서 GZip이 그 다음. 만약 적은 디스크 공간 사용과 I/O 부하가 중요하면 둘 다 매력적인 선택이 될 수 있음 
  - LZO와 Snappy는 압축률이 낮아 비교적 큰 파일을 만들지만 압축 속도가 빠르며 특히 압축을 풀 때는 훨씬 빠름. 만약 빠르게 압축을 푸는 것이 디스크 공간과 I/O 오버헤드보다 중요하다면 이런 압축 방식을 선택하는 것이 좋음
  - 맵리듀스는 매우 큰 입력 파일을 스플릿(split)으로 분할함. 하둡은 각각의 스플릿을 개별 맵 프로세스에게 보냄
  - 어떻게 파일을 로우(레코드)로 나누냐는 것. 텍스트 파일은 \n(줄 바꿈 문자)을 기본 로우 구분자로 사용함. 텍스트 파일 포맷을 사용하지 않으면 하이브에게 사용할 InputFormat과 OutputFormat의 이름을 알려줘야 함.
    - InputFormat은 어떻게 스플릿을 읽고 레코드로 나누는지 알고 있고, OutputFormat은 어떻게 스플릿을 파일 또는 콘솔에 출력하는지에 대해 알고 있음 
  - 레코드를 어떻게 필드(또는 컬럼)로 나누냐에 대한 것. 하이브는 텍스트 파일의 레코드를 필드로 나눌 때 ^A 문자를 기본 구분자로 사용함. 하이브는 들어오는 레코드를 쪼개고(deserializer) 레코드를 해당 포맷으로 다시 쓰는 방법을 아는(serializer) SerDE라는 모듈을 이용함 
- 중간 과정 압축
  - 중간 과정 압축(intermediate compression)은 맵에서 리듀스로 보내는 데이터양을 줄임. 중간 과정 압축에서는 압축률보다 CPU 사용량이 낮은 압축을 선택하는 것이 중요함 
- 최종 출력 압축
  - 하이브는 테이블로 결과를 출력할 때 내용을 압축할 수 있음 
- 시퀀스파일
  - 하둡은 분할 가능한 파일을 여러 조각으로 나눠 여러 맵퍼로 보내 병렬로 수행될 수 있게 함. 대부분의 압축 파일은 나눌 수 없음. 왜냐하면 무조건 처음부터 읽어야 하기 때문
  - 하둡이 지원하는 시퀀스파일 포맷은 파일을 블록으로 나눌 수 있고 선택적으로 블록을 압축할 수 있음. CREATE TABLE 절에 STORED AS SEQUENCEFILE 절을 추가하면 됨
  - 세 가지 압축 옵션. NONE, RECORD, BLOCK이고 RECORD가 기본 
- 파티션 아카이빙하기
  - 하둡은 하둡 아카이브를 의미하는 HAR이라는 저장 포맷을 가지고 있음. HAR 파일은 TAR 파일과 비슷함. 하둡 파일시스템 내에 단일 파일로 존재함 

# 개발
- Log4J 속성 변경
  - hive-log4j.properties 파일은 CLI나 로컬에서 실행되는 하이브 구성요소의 로깅을 제어함
  - hive-exec-log4j.properties 파일은 맵리듀스 태스크 안에서 로깅을 제어함 

# 함수
- 함수 탐색 및 설명
  - SHOW FUNCTIONS 명령어는 현재 하이브 세션에 로드된 내장 함수와 사용자 정의 함수의 목록을 보여주는 명령어
  - DESCRIBE FUNCTION 명령어를 사용하면 간략한 설명을 볼 수 있음
  - EXTENDED 예약어로 접근할 수 있는 확장 문서를 포함하기도 함 
- 표준 함수
  - UDF는 좁은 의미로 하나의 로우 또는 로우로부터 하나 이상의 컬럼을 받아서 하나의 값을 반환하는 함수를 지칭함 
  - > ucase() : 문자열의 문자를 대문자로 전환
- 집계 함수
  - 사용자 정의 집계 함수(UDAF : user-defined aggregate functions) : 사용자 정의와 내장된 모든 집계 함수
  - 집계 함수는 0개 이상의 로우로부터 하나 이상의 컬럼을 받아서 하나의 결과값을 반환함
- 테이블 생성 함수(table generating function)
  - 사용자 정의 테이블 생성 함수(user-defined table generating functions. UDTF) : 사용자 정의 함수와 내장된 모든 테이블 생성 함수
  - 테이블 생성 함수는 0 또는 그 이상의 입력을 받아서 여러 컬럼 또는 로우를 결과값으로 생성. array함수는 인자 목록을 받아서 ARRAY 데이터형으로 반환함
  - explode() 함수는 배열을 입력으로 받아서 배열의 각 요소를 개별 로우로 반환하는 UDTF 함수
    - ```sql
      SELECT exlpode(array(1,2,3)) AS element FROM scr;
      1
      2
      3
      ```
  - 테이블의 다른 컬럼과 같이 선택될 수 없는 제약이 있음, 허용하기 위해 LATERAL VIEW 기능을 제공함
  - ```sql
    SELECT name, Sub
    FROM employees  
    LATERAL VIEW explode(subordinates) subView AS sub;
    ```
- UDF vs GenericUDF
  - GenericUDF는 UDF에 비해 좀 더 복잡한 추상 클래스이지만 null을 더 잘 처리하고 표준 UDF에서 제공하지 못하는 몇몇 종류의 연산을 프로그래밍적으로 처리할 수 있음
  - GenericUDF의 대표적인 예로 인자에 따라 복잡한 로직을 갖는 CASE .... WHEN 문이 있음
- 영구적으로 사용을 위한 함수 등록
  - 작성한 코드를 JAR 파일로 묶고 ADD JAR 및 CREATE TEMPORARY FUNCTION 명령을 수행하여 하이브에서 사용 가능하도록 함 
- 사용자 정의 집계 함수
  - GROUP_CONCAT을 따라 하는 COLLECT UDAF 함수 생성
    - MySQL은 어떤 그룹의 모든 요소를 사용자가 정의한 구분자를 이용해 단일 문자열로 결합하는 GROUP_CONCAT이라는 유용한 함수를 가지고 있음
    - ```sql
      SELECT name, GROUP_CONCAT(friendname SEPARATOR, ',')
      FROM people
      GROUP BY name;
      ```
    - collect() 함수는 모든 집계값을 한 배열로 저장한 단일 로우를 반환함
    - ```sql
      CREATE TABLE collecttest (str STRING, countVal INT)
      ROW FORMAT DELIMITED FIELDS TERMINATED BY '09' LINES TERMINATED BY '10';
  
      LOAD DATA LOCAL INPATH '${env:HOME}/afile.txt' INTO TABLE collecttest;
  
      SELECT collect(str) FROM collecttest;
      [twelve, twelve, eleven, eleven]
      ```
    - concat_ws() 함수는 첫 번째 인자로 구분자를 입력받음. 나머지 인자는 문자열 데이터형 또는 문자열의 배열을 사용할 수 있음. 인자로 받은 문자열을 구분자로 연결한 단일 문자열을 반환함 
    - ```sql
      SELECT concat_ws( ',' , collect(str)) FROM collecttest;
      twleve,twleve,eleven,eleven
      ```
- 사용자 정의 테이블 생성 함수
  - UDF는 배열 또는 구조체를 반환형으로 사용할 수 있지만 다중 컬럼이나 다중 로우를 반환할 수 없음. UDTF는 다중 컬럼과 다중 로우를 반환하기 위한 프로그래밍 인터페이스를 제공함
  - 다중 로우 생성 UDTF
    - explode 메소드는 배열을 입력받아 배열의 각 요소를 단일 로우로 출력함
    - 시작값과 종료값을 사용자로부터 입력받아 N개 만큼의 로우를 출력함 
  - 다중 컬럼을 이용한 단일 로우 생성 UDTF
    - parse_url_tuple 함수, URL과 사용자가 보고자 하는 한 개 이상의 URL 파트를 나타내는 상수를 입력 인자로 받음
    - ```sql
      SELECT parse_url_tuple(weblogs.url, 'HOST', 'PATH')
      AS (host, path) FROM weblogs;
      google.com /index.html
      hotmail.com /a/links.html
      ```
    - UDTF는 입력으로 받은 URL을 한 번만 파싱하여 다중 컬럼을 반환하는 이점이 있음. UDF를 사용할 때 작성해야 할 코드의 양도 늘어날 뿐만 아니라 URL을 여러 번 파싱해야 하기 때문에 연산 수행 시간도 길어짐 
    - ```sql
      SELECT PARSE_HOST(a.url) as host, PARSE_PORT(url) FROM weblogs;
      ```
  - 복합 데이터형 효과를 내는 UDTF
    - 복합 데이터형은 인코딩 문자열로 직렬화될 수 있고 필요한 경우 UDTF는 복합 데이터형을 역직렬화할 것
    - ```sql
      SELECT * FROM booksl
      55555|Programming Hive|Edward,Dean,Jason
      
      SELECT cast(split(book_info,"\|")[0] AS INTEGER) AS isbn FROM books
      WHERE split(book_info,"\|")[1] = "Programming Hive";
      55555
  
      FROM (
      parse_book(book_info) AS (isbn, title, authors) FROM Book ) a
      SELECT a.isbn
      WHERE a.title = "Programming Hive"
      AND array_contains (authors, 'Edward');
      ```
- UDF에서 분산 캐시 접근
  - UDF는 분산 캐시, 로컬 파일시스템 또는 분산 파일시스템에 존재하는 파일에 접근할 수 있음. 접근할 때 발생하는 오버헤드는 상당하기 때문에 조심스럽게 사용해야함
  - 보통 웹 로그 분석에 하이브가 자주 사용됨. 많이 사용하는 연산은 IP 주소를 기반으로 웹 트래픽이 발생한 지역을 찾는 것 
  - geoip() 함수는 문자열 또는 long형 포맷을 가진 IP 주소와 COUNTRY_NAME 또는 DMA_CODE 문자열 상수와 분산 캐시에 추가되어 있는 데이터 파일의 이름을 인자로 받음 
- 함수 어노테이션
  - 결정성 : 대부분의 쿼리는 본래 결정성을 가지고 있음(동일한 입력에 대해서 동일한 출력을 만든다는 의미)
- 매크로
  - 매크로는 HiveQL에서 다른 함수와 명령어를 이용하여 함수를 정의함.
  - 매크로는 외부 코드나 스크립트를 필요로 하지 않기 때문에 특수한 상황에서 자바를 이용한 UDF 함수 생성 또는 하이브 스트리밍의 사용을 대처해서 사용하기에 편리함

# 스트리밍 
- 하이브는 하둡의 입력 포맷과 출력 포맷, 맵퍼, 리듀서와 같은 공통 추상화 구성요소뿐만 아니라 내부에서 제공하는 SerializerDeserializer(SerDe)와 사용자 정의 함수 그리고 스토리지 핸들러(StorageHandler)와 같은 구성요소를 구현된 그대로 사용하거나 확장해서 사용함. 이러한 구성요소는 모두 자바로 구현되어 있지만, 하이브는 자바 코드 대신 SQL 추상화 계층을 제공하기 때문에 사용자는 복잡한 자바 컴포넌트를 사용해 구현하지 않아도 됨 
- 스트리밍 잡을 진행하는 동안 하둡 스트리밍 API는 I/O 파이프를 외부 프로세스에 열어줌. 데이터는 해당 프로세스로 넘어가고 해당 프로세스는 표준 입력으로부터 데이터를 읽고 표준 출력으로 스트리밍 API잡에게 결과를 돌려줌 
- MAP()은 맵 단계에서 스트리밍을 강제적으로 실행하지 않는다는 점. REDUCE() 또한 리듀스 단계에서 동일하게 동작함. 쿼리를 읽는 사용자가 오해하지 않게 기능적으로 동일하면서 더 범용적인 TRANSFORM() 문 사용을 권장함
- 항등 변환(Identity Transformation)
  - 가장 기본적인 스트리밍 잡은 항등 조작. /bin/cat 명령어는 전달받은 데이터를 그대로 출력해주므로 이러한 목적에 딱 맞음
- 투사 변환 : 스트리밍에서 cut 명령어를 이용해 특정 필드를 추출하거나 투사(project)할 수 있음 
- 조작 변환 : /bin/sed/ 프로그램은 스트리밍 편집기. 입력 스트림으로부터 입력을 받아 사용자의 명세에 따라 수정한 후, 출력 스트림으로 결과를 돌려줌
- 분산 캐시 사용
  - 스트르밍 예제는 유닉스나 유닉스 파생 운영체제가 공통으로 가지고 있는 cat과 sed 같은 애플리케이션을 사용함. 일부 태스크트래커에만 설치된 파일을 쿼리에서 사용하는 경우 사용자는 클러스터 상에서 데이터나 프로그램 파일을 전달하는 목적으로 분산 캐시(distributed cache)를 이용할 수 있음. 전달한 파일은 잡이 완료될 때 삭제됨
  - 분산 캐시는 많은 수의 작은 구성요소를 대규모 클러스터에 설치하거나 삭제할 때 유용함. 각 잡에 대해 독립적으로 캐시 파일을 관리함 

# 하이브 파일과 레코드 포맷 사용자화
- 파일 포맷은 레코드가 파일 안에서 인코딩되는 방식
- 레코드 포맷은 바이트 열이 레코드 안에서 인코딩 되는 방식 
- CREATE TABLE 문 파헤치기
  - 테이블을 생성할 때 STORED AS SEQUENCEFILE, ROW FORMAT DELIMITED, SERDE, INPUTFORMAT, OUTPUTFORMAT와 같은 다양한 문법을 사용함 
  - 하이브는 테이블에서 데이터를 읽을 때는 InputFormat을 사용하고 테이블에 데이터를 쓸 때는 OutputFormat을 사용함
- 파일 포맷
  - 텍스트 파일 포맷은 피그나 grep,sed,awk와 같은 유닉스 텍스트 도구 등과 데이터를 공유하기 편리함, 파일의 내용을 직접 보고 수정하기 쉬움
  - 텍스트 포맷은 바이너리 포맷과 비교하면 저장 공간을 효과적으로 사용하지는 못함. 압축을 사용할 수도 있지만 바이너리 포맷을 사용함으로써 텍스트 포맷보다 더 향상된 디스크 I/O와 효과적인 디스크 공간 사용, 두 가지 이득을 얻을 수 있음
  - 시퀀스 파일
    - 파일 포맷의 첫 번째 대안으로 시퀀스파일(sequence file)포맷을 사용할 수 있음. 테이블 생성 시 STORED AS SEQUENCEFILE 문을 사용하여 저장함
    - 시퀀스파일은 바이너리 키-값으로 구서된 플랫 파일(flat file - 계층적 구조를 갖지 않고 단순히 같은 형식의 레코드의 모임으로 이루어졌음). 하이브는 쿼리를 맵리듀스 잡으로 변환할 때 레코드로 사용하기 위한 적당한 키-값 쌍을 정함
    - 시퀀스파일은 블럭과 레코드 수준에서 압축이 가능하므로 디스크 공간 활용과 I/O를 최적화할 수 있고 병렬 처리를 위한 블록 단위 파일 분할도 여전히 가능함
  - RCFile
    - 대부분 하둡과 하이브 저장 공간은 로우 기반이며 이는 대부분 효과적. 파일의 블록 단위 압축은 반복되는 데이터를 다루는 데 효율적이고 로우 기반의 데이터를 잘 다룰 수 있는 여러 텍스트 처리 도구나 디버깅 도구(more,head,awk)와 잘 맞음
    - 테이블이 수백 개의 컬럼을 가지고 있고 대부분 쿼리에서 그중 몇 개만 사용한다면 데이터를 가지고 오기 위해 전체 로우를 스캔하는 방식은 낭비. 대신에 데이터가 컬럼을 기준으로 저장되어 있다면 필요한 데이터 컬럼만 읽을 수 있기 때문에 성능이 향상될 것 
    - 하이브의 강력한 장점 중 하나는 서로 다른 두 데이터 포맷을 간단히 변환하는 능력. 저장 정보는 테이블의 메타데이터에 저장함. 한 테이블을 SELECT한 값을 다른 테이블로 INSERT하면 하이브는 메타데이터를 사용해 테이블 간의 데이터 변환을 자동으로 수행함 
  - 사용자 정의 입력 포맷 예제 : DualInputFormat
    - 기본값으로, 표준 하이브 테이블은 TextInputFormat을 사용함. TextInputFormat은 0 또는 그 이상의 스플릿을 입력받아 계산함 
  - 레코드 포맷 : SerDE
    - SerDE는 직렬화(serializer)/역직렬화(deserializer)의 약어. SerDe는 하이브 파일에 저장된 레코드에서 구조가 없는 바이트를 하이브가 사용할 수 있는 레코드로 변환하는 처리를 캡슐화함. SerDe는 자바로 구현함. 하이브는 여러 내장 SerDe를 가지고 있고 다른 많은 써드파티 SerDe도 사용할 수 있음 
- XML UDF
  - 구조화되어 있지 않은 XML의 특성은 하이브가 XML 데이터베이스 플랫폼으로서 강력한 위력을 발휘하게 됨 
  - 하둡이 XML 데이터베이스로 이상적인 이유 중 하나는 XML의 복잡성과 대량의 XML 데이터를 파싱할 경우 많은 리소스를 소비한다는 점. 
  - 하둡에서는 XML 문서의 병렬 처리가 가능하므로 XML 관련 데이터 솔루션을 가속화할 수 있는 완벽한 도구. HiveQL은 본래 XML의 중첩 요소와 값에 접근할 수 있고 나아가 중첩 필드, 값, 속성을 조인 연산할 수 있음 
  - XPath(XML Path 언어)는 XML 문서의 부분을 다루기 위해 W3C에서 만든 세계 표준. XML 쿼리를 표현하는 언어로써 XPath를 사용하면 XML 문서로부터 데이터를 추출하거나 추출된 데이터를 하이브 서브시스템으로 입력하기 쉬움
  - XPath는 XML 문서를 트리와 노드 형태로 형상화함 
  - ```sql
    SELECT xpath(\'<a><b id = "foo">b1</b><b id ="bar">b2</b></a>\',\'//@id\')
    FROM src LIMIT 1;
    [foo","bar]

    SELECT xpath (\'<a><b class="bb">b1</b><b>b2</b><b>b3</b><C class ="bb">c1<c><c>c2</c></a>\', \'a/*[@class="bb"]/text()\')
    FROM src LIMIT 1;
    [b1","c1\]
    
    SELECT xpath_double (\'<a><b>2</b><c>4</c></a>\', \'a/b + a/c\')
    FROM src LIMIT 1;
    6.0
    ```
- 에이브로 하이브 SerDe
  - 에이브로는 직렬화 시스템으로 주된 특징은 진화 가능한 스키마 주도의 바이너리 데이터 포맷
  - 하이브와 하이브 메타스토어는 플러그인 설계로 되어있으므로 에이브로의 스키마 추론 지원을 따를 수 있음 

# 하이브 쓰리프트 서비스
- 하이브는 하이브 서버(HiveServer) 또는 하이브 쓰리프트(HiveThrift)라 불리는 구성요소를 가지고 있음, 클라이언트는 하나의 포트로 하이브에 접근할 수 있음 
- 쓰리프트는 확장성(scalable)과 교차 언어적(cross language. 서로 다른 언어 간에 통신이 가능하다는 의미) 특징을 갖는 소프트웨어 프레임워크
- CLI는 모든 하이브 구성요소 및 설정이 로컬에 복사본으로 존재해야만 동작함. 하둡 클라이언트와 하둡 설정도 있어야 함. 하이브 CLI는 HDFS 클라이언트, 맵리듀스 클라이언트, JDBC 클라이언트(메타스토어에 접근할 때)로 동작함 
- 쓰리프트 서버 구동하기
  - 하이브 서비스는 쓰리프트를 이용함. 쓰리프트는 인터페이스 언어를 제공함
  - 쓰리프트 컴파일러는 인터페이스를 해석하여 다양한 언어로 된 네트워크 RPC 클라이언트 코드를 생성함
  - 하이브는 자바로 작성되었고, 자바 바이트코드(bytecode)는 범용 플랫폼이므로 쓰리프트 서버를 위한 자바 클라이언트를 하이브 배포에 포함
- 그루비로 연결하기
  - 그루비는(Groovy) 자바 가상 머신에서 동작하는 동적이고 민첩한(agile) 언어 
  - 그루비는 자바와 연동되고 그때그때 작성하는 코드를 위한 REPL(Read-Eval-Print Loop. 파이썬, 루비,PHP와 같이 컴파일 과정 없이 코드를 수행시킬 수 있는 환경)을 지원하기 때문
- 하둡 서비스 운영
  - TCP 부하 분산 장치(TCP load balancer)를 이용하거나 백엔드 서버의 풀(pool)로 접속하는 프록시를 만드는 방법
- 하이브 쓰리프트 메타스토어
  - 하이브 세션은 메타스토어로 이용하는 JDBC 데이터베이스에 직접 연결됨 

# 스토리지 핸들러와 NoSQL
- 스토리지 핸들러(Storage Handler)는 표준 하이브 테이블로 입력 포맷, 출력 포맷, SerDE, 외부 개체를 다루는 코드로 이루어져 있음
- 사용자는 테이블이 하둡에 텍스트 파일 형태로 저장되어 있든 HBase와 카산드라(Cassandra), 아마존 다이나모디비(Amazon DynamoDB)등의 NoSQL 데이터베이스에 저장된 컬럼 패밀리(Column Family) 형태든 상관없이 매끄럽게 쿼리를 실행할 수 있음 
- 스토리지 핸들러는 ETL을 위한 스트림라인 시스템(Streamlined System)을 제공함. 하이브 쿼리는 시퀀스파일 형태의 데이터 테이블에서 데이터를 가져와 NoSQL 데이터베이스에 출력할 수 있음 
- 스토리지 핸들러 배경 지식
  - 하둡은 서로 다른 소스와 포맷의 데이터를 잡에 입력할 수 있도록 해주는 입력 포맷 추상화를 가지고 있음 
  - TextInputFormat은 이러한 입력 포맷 구현체 중 하나. 이 구현체는 주어진 경로를 여러 태스크로 나누는 방법에 대한 정보와 각 분할로부터 데이터를 읽는 방법을 담은 RecordReader를 하둡에 제공함 
  - 하둡은 또한 잡의 결과를 가져와 개체(entity)에게 전달하는 출력 포맷 추상화를 가지고 잇음. TextOutputFormat은 이러한 출력 포맷 구현체 중 하나. 이 구현체는 출력을 HDFS나 로컬 경로에 파일로 저장함 
- 하이브 스토리지 핸들러
  - 하이브 스토리지 핸들러(HiveStorageHandler)는 HBase와 카산드라 그리고 그 외의 다양한 NoSQL 저장소를 연결할 때 사용하는 주요 인터페이스. 이 인터페이스는 입력 포맷과 출력 포맷 그리고 SerDe를 필수 구현 사항으로 정의함
  - NoSQL 데이터베이스 상에서 하이브 쿼리르 실행하는 것은 NoSQL 시스템 오버헤드로 인해 HDFS 상에서의 하이브 맵리듀스 잡보다 성능이 좋지 않음 
  - 전반적인 시스템 아키텍처에서 NoSQL 데이터베이스와 하둡을 결합하는 기술은 NoSQL 클러스터의 실시간 작업과 하둡의 배치 지향 작업(batch-oriented work)을 처리하기 위해서
  - NoSQL 시스템을 마스터 데이터 저장소 그리고 그 데이터를 하둡의 배치 작업을 이용해 쿼리하고 싶다면 벌크로 데이터를 내보내는 것이 NoSQL 데이터를 HDFS 파일로 변환하는 효과적인 방법
  - 내보내기를 통해 HDFS 파일이 생성되면 배치 하둡 잡이 최대한 효과적으로 실행됨 
- HBase
  - ```sql
    -- HiveQL을 이용해 하이브와 HBase 테이블을 생성하는 예제
    CREATE TABLE hbase_stocks(key INT, name STRING, price FLOAT)
    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
    WITH SERDEPROPERITIES ("hbase.columns.mapping" = ":key,stock:val")
    TBLPROPERTIES ("hbase.table.name" = "stocks");
    
    -- 이미 존재하는 HBase 테이블과 하이브 테이블을 연동할 때에는 CREATE EXTERNAL TABLE HiveQL 문장을 사용함
    CREATE EXTERNAL TABLE hbase_stocks(key INT, name STRING, price FLOAT)
    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
    WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf1:val")
    TBLPROPERTIES ("hbase.table.name" = "stocks");
  
    -- 주어진 하이브 쿼리에 대해 HBase 테이블 전체를 살펴보는 대신 필터 푸시다운(filter pushdown)을 이용해 하이브로 반환할 로우 데이터를 제한할 수 있음 
    CREATE TABLE hbase_pushdown(key INT, value string)
    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
    WITH SERDEPROPERITIES ("hbase.columns.mapping" = ":key,cf:string");
  
    SELECT * FROM hbase_pushdown WHERE key = 90;   
    ```

# 보안
- 맵과 리듀스 태스크는 모두 같은 시스템 사용자(보통 hadoop 또는 mapred)로 태스크트래커 노드에서 수행됨 , 하둡 구성요소는 일반적으로 높은 숫자의 포트에서 요청을 대기하고 있음 

# 잠금
- HiveQL은 SQL과 유사하긴 하지만 컬럼이나 로우에 대해 혹은 갱신이나 삽입 같은 쿼리에 대해 전통적인 잠금(locking) 지원이 부족함 
- 하둡이 제한된 의미의 이어 쓰기(append)를 지원하기는 하지만 하둡 파일은 전통적으로 write-once(한 번 쓰면 변경이 불가능한 것을 의미함) 특성을 가지고 있음
- 하둡 파일의 write-once 특성과 맵리듀스의 스트리밍 읽기 방식으로 인해 세밀한 잠금(fine-grained locking)에 대한 접근은 불필요함 
- 하둡과 하이브는 다중 사용자가 시스템이기 때문에 잠금과 코디네이션(coordination)이 특정 상황에서 필요할 수도 있음 
- 하이브, CLI, 쓰리프트 서버(Thrift server), 웹 인터페이스 인스턴스 각각이 다른 인스턴스에 대해서 완전히 독립적이라는 관점에서 보면 하이브는 팻 클라이언트(fat client)라고 생각할 수 있음 
- 주키퍼를 이용한 하이브 잠금 제공
  - 하이브는 아파티 주키퍼를 이용하여 잠금을 제공함. 주키퍼는 매우 견고하게 분산 코디네이션을 구현한 오픈소스 
  - 동시적(concurrency) 기능이 truc로 설정되어 있는 경우 하이브는 자동으로 두 가지 종류의 잠금 기능을 제공함 
  - 테이블을 읽을 때는 공유 잠금(shared lock)을 사용함. 여러 공유 잠금을 동시에 사용하는 것이 가능함 
  - 테이블을 수정하는 명령을 수행할 때 배타적 잠금(exclusive lock)을 사용함. 배타적 잠금은 테이블을 변경하려는 다른 연산을 정지시킬 뿐만 아니라 다른 프로세스에서 실행되는 쿼리를 막을 수 있음 
- 명시적 배타적 잠금
  - > LOCK TABLE people EXCLUSIVE;
  - > UNLOCK TABLE people;

# 우지와 하이브 통합
- 아파치 우지(Apache Oozie)는 하둡을 위한 작업 스케줄러
- 하이브는 내부적으로 워크플로우 시스템을 가지고 있음. 하이브는 쿼리를 맵리듀스 단계 또는 태스크 이동 단계 등 여러 단계로 변환함. 만약 특정 단계가 실패한다면 하이브는 프로세스를 정리하고 에러를 보고함 
- 하이브의 워크플로우 관리 시스템은 단일 잡이나 특정 잡 다음으로 실행하는 잡에 탁월함
- 우지 워크플로우 잡은 액션의 방향성 비순환 그래프(Directed Acyclical Graph.DAG)다. 우지 코디네이터잡(Oozie Coordinator job)은 시간(주기)과 데이터의 사용 가능 여부에 의해 반복적으로 우지 워크플로우 잡을 실행함. 우지의 중요한 기능 중 하나는 잡을 실행한 클라이언트로부터 워크플로우 상태를 분리한다는 것
- 하이브 잡은 잡을 실행할 콘솔에 붙어 있어서 콘솔이 죽으면 절반만 완료되는데 이런 경우 분리된 잡 실행은 유용함
- 우지 액션
  - MapReduce - 사용자는 맵퍼 클래스, 리듀서 클래스, 맵리듀스 설정 변수를 제공함 
  - Shell - 주어진 인자로 셸 명령을 액션으로 실행함
  - Java Action - main 메소드를 가진 자바 클래스를 선택적으로 제공한 인자를 사용해 실행함
  - Pig - 피그 스크립트를 실행함
  - Hive - 하이브 HQL 쿼리를 실행함 
  - DistCp - 다른 HDFS 클러스터로부터 혹은 HDFS 클러스터로 데이터를 복사하는 distcp 명령을 실행함 
  - 하이브 액션은 팻 클라이언트 하이브를 사용함
- 두 개의 쿼리를 실행하는 워크플로우
  - 워크플로우는 필수 JAR 파일인 job.properties 파일과 workflow.xml 파일을 가지는 특정 계층 구조의 디렉터리를 설정하여 생성됨 
- 우지 웹 콘솔
  - 우지 웹 콘솔(Oozie web console)은 잡의 문제점을 해결하기 위한 유용한 도구. 우지는 맵 태스크 내에서 각 액션을 실행시키고 모든 입력 및 출력을 캡처함 
  - 우지는 성공한 잡에 대한 정보를 제공해줄 뿐만 아니라 하둡 잡트래커 웹 콘솔에 존재하는 잡 상태 페이지에 대한 링크를 제공함 

# 하이브와 아마존 웹 서비스
- EMR(Elastic MapReduce)을 이용하면 필요할 때 노드 클러스터를 구성할 수 있음. 이 클러스터에는 피그와 같은 여러 도구뿐만 아니라 미리 설정되어 있는 하둡과 하이브가 이미 설치되어 있음. 이 클러스터에서 하이브 쿼리를 수행하거나 작업한 클러스터를 종료시킬 수 있으며, 사용한 시간만큼 요금을 지불하면 됨 
- 인스턴스
  - 아마존 클러스터는 하나 이상의 인스턴스(instance)로 구성됨. 인스턴스는 메모리,CPU,디스크,운영체제,I/O 성능 등에 따라 다양하게 제공됨
  - EMR에서는 처음에 작은 수의 인스턴스로 시작하고 강글리아(Ganglia)와 같은 도구를 사용하여 성능을 모니터링 하다가 성능 대비 최소 비용이 발생하는 최적의 클러스터 크기를 실험을 통해서 쉽게 찾을 수 있음 
- EMR 하이브 클러스터 관리하기
  - 하이브 클러스터를 관리할 수 있는 방법
    - 웹 기반의 EMR AWS 관리 콘솔
    - EMR 명령행 인터페이스
      - 간단한 루비 기반의 EMR CLI를 사용하여 클러스터를 관리 
    - EMR API
      - 다양한 언어를 지원하는 SDK를 사용하여 EMR API를 호출함으로써 EMR 클러스터를 관리할 수 있음 
    - 하이브 클러스터를 관리하기 위해서 보통 한 가지 이상의 방법을 사용함 
- EMR에서 인스턴스 그룹
  - 아마존 클러스터는 하나 또는 그 이상의 노드로 구성됨. 각각의 노드는 세가지 인스턴스 그룹 중 하나에 속함
  - 마스터 인스턴스 그룹
    - 마스터 노드라고 불리며 정확히 한 노드만을 가짐
    - 마스터 노드는 하둡의 마스터 노드가 하는 일과 동일한 작업을 수행함 
    - 네임노드(namenode)와 잡트래커(jobtracker)를 구동시킬 뿐만 아니라 이들 기반의 하이브를 가지고 있음. 게다가 EMR 하이브에서 메타스토어(metastore) 역할을 하는 MySQL도 가지고 있음(기본 메타스토어인 더비는 사용되지 않음)
    - 마스터 노드에는 다른 인스턴스 그룹의 인스턴스를 구동하고 관리하기 위한 인스턴스 제어기(controller)가 있음 
  - 코어 인스턴스 그룹
    - 코어 인스턴스 그룹에 속하는 인스턴스는 데이터노드(datanode)와 태스크트래커(tasktracker)를 구동하는 하둡의 슬레이브(slave)노드와 같은 기능을 가지고 있음. 이 노드는 HDFS를 구성하여 데이터를 저장하고 맵리듀스 잡을 처리하는 데 사용됨
    - 일단 클러스터가 시작됐다면 이 인스턴스 그룹의 노드 개수는 늘긴 해도 줄진 못함. 클러스터가 종료되면 HDFS를 구성하는 데이터노드가 가지고 있는 데이터는 사라진다는 점
  - 태스크 인스턴스 그룹
    - 이 그룹에 대한 설정은 옵션. 이 그룹에 속하는 노드 역시 하둡 슬레이브 노드와 같은 기능을 함. 여기서는 태스크트래커만 구동됨.
    - 맵리듀스 태스크를 위해서만 사용되지, HDFS 블록을 저장하기 위해서는 사용되지 않음. 클러스터가 구동되면 태스크 인스턴스 그룹에 있는 노드의 개수는 늘였다 줄였다 할 수 있음 
  - 태스크 인스턴스 그룹은 클러스터 맵리듀스 처리 능력에 대한 요구사항이 많을 때는 태스크 인스턴스 수를 늘였다가 그 이후에 다시 원래대로 줄이는 상황에 편리하게 사용됨. 또한 클러스터에서 제거됐을 때 데이터 손실 위험이 없어 낮은 가격으로 스팟(spot) 인스턴스를 사용할때 유용함. 노드 하나에서 클러스터를 운영하고 있다면 그 노드는 코어 노드인 동시에 마스터 노드 
- EMR 클러스터 설정하기
  - hive-site.xml, .hiverc, hadoop-env.sh 파일에서 설정 
- S3에 리소스, 설정, 부트스트랩 스크립트 올리기
  - 새로운 잡플로우를 구동하기 전에 필요한 모든 부트스트랩 스크립트, 설정 스크립트(hive-site.xml, .hiverc), 리소스(분산 캐시에 들어갈 필요한 파일, UDF, 스트리밍 JAR 파일)들을 S3에 올려놔야함 
- S3에 로그 남기기
  - 아마존 EMR은 log-uri 필드에서 지정한 S3 위치에 로그 파일을 저장함. 클러스터의 부트스트랩 과정뿐만 아니라 여러 클러스터 노드에 있는 데몬 프로세스로부터 나온 로그를 포함함
  - 잡 수행 중 에러가 발생할 때 구동 중인 워크플로우를 종료하도록 설정되어 있다면 클러스터에 있는 모든 로그는 클러스터 종류 후 잃게 됨 
  - log-uri 필드가 설정되어 있다면 이 로그는 클러스터가 종료된 이후에라도 S3의 지정된 위치에 있을 것 
- 스팟 인스턴스
  - 스팟 인스턴스는 사용되지 않은 아마존 유휴 자원을 경쟁 입찰을 통해 온디맨드(on-demand) 인스턴스에 비해 더 싼 가격으로 사용자에게 제공함 
- EMR vs EC2, 그리고 아파치 하이브
  - 유연성 측면에서 EMR에 대한 대안으로 아마존 EC2 노드를 시작하고 하둡과 하이브를 사용자 정의 아마존 머신 이미지(AMI)에 설치하는 방법
  - 하이브나 하둡의 버전과 설정에 대한 다양한 선택을 제공함. 예를 들어 새로 배포된 도구에 대해서는 EMR에서 서비스되기 전에 미리 실험해 볼 수 있음 

# HCatalog
- 메타데이터를 저장하는 기능은 사용자가 데이터의 스키마뿐만 아니라 데이터를 저장한 위치와 데이터 포맷도 기억할 필요가 없음 
- 데이터 생산자(data producer), 데이터 소비자(data consumer), 데이터 관리자(data administrator)는 서로 방해받지 않고 원하느 작업을 할 수 있음. 데이터 생산자는 데이터 소비자의 데이터를 읽는 애플리케이션의 작동에 영향을 끼치지 않고 새로운 컬럼을 추가할 수 있음. 관리자는 생산자 또는 소비자의 애플리케이션을 변경하지 않고도 저장된 데이터의 포맷을 변경하기 위해 데이터를 재배치할 수 있음
- 분석 쿼리를 위해 하이브를 사용하던 사용자는 ETL 처리를 하거나 데이터 모델을 구축할 때 피그를 사용해야 할 것 
- 메타데이터 저장소를 공유하면 사용자가 도구 간에 손쉽게 데이터를 공유할 수 있음. 일반적으로 피그나 맵리듀스로 데이터를 올리고 나면 정규화한 다음 하이브로 분석함 
- 모든 도구가 단일 메타스토어를 공유하면 각 도구의 사용자는 곧장 다른 도구가 만든 데이터에 접근할 수 있어 로딩과 전송 과정이 필요 없음 
- HCatalog는 하둡이 모든 도구에서 사용할 수 있는 하이브 메타스토어를 만들고, 맵리듀스와 피그의 커넥터를 제공함. 하둡 도구를 사용하여 하이브 웨어하우스의 데이터를 읽고 쓸 수 있음. 하이브를 사용하지 않는 사용자를 위한 하이브 DDL로 메타스토어를 다루는 명령행 도구를 제공하고 알림 서비스도 제공함 
- 맵리듀스
  - 데이터 읽기
    - 맵리듀스는 입력 데이터를 읽을 때 InputFormat 자바 클래스를 이용함. 이 클래스는 HDFS에서 데이터를 바로 읽음. HBase나 카산드라 혹은 기타 데이터 소스로부터 데이터를 읽는 InputFormat 구현체도 있음 
    - 데이터를 섹션(section)으로 나누는 방법을 정의해 맵리듀스의 맵 태스크에 병렬 처리를 제공함. 두 번째는 RecordReader를 제공함. 이 클래스는 맵리듀스가 입력 소스로부터 레코드를 읽어 맵 태스크가 사용할 수 있는 형태인 키와 값 형태로 변환시키는 데 사용됨 
    - HCatalog는 맵리듀스 사용자가 하이브 데이터 웨어하우스에 저장되어 있는 데이터를 읽을 수 있게 HCatInputFormat을 제공함. 사용자는 필요한 테이블의 파티션과 컬럼을 읽을 수 있음. 편한 리스트 형태로 레코드를 제공함. 사용자는 데이터를 파싱할 필요가 없음 
  - 데이터 쓰기
    - 한 번에 한 개 이상의 파티션을 쓸 수 있음. 이를 동적 파티셔닝(dynamic partitioning). 레코드가 런타임에 동적으로 파티셔닝 되기 때문 

# 사례 연구
- m6d.com(Media6Degrees)
  - R은 데이터 분석을 위해 필요한 전체 데이터를 시스템 메모리에 모두 올려야 한다는 단점 
  - R에서는 메모리 사이즈보다 데이터 사이즈가 커지면 시스템은 자동적으로 스왑핑을 하고 이것은 처리 속도를 심가하게 떨어트림
  - 하이브와 같은 새로운 도구를 평가할 때는 장점뿐만 아니라 사용할 때 투자해야 하는 기회 비용에 대해서도 고려해야 함. 높은 확장성에 대해 심도 있는 고려만큼이나 새로운 기술 도입에 따른 평가 및 적용에 투자되는 시간도 고려해야 함 
- 아웃브레인 - 콘텐츠 검색 플랫폼의 선구자
  - 하나의 세션 : 일련의 연속적인 접근 혹은 행동이 최대 30분 미만의 시간 간격으로 지속적으로 나타나는 것
  - 하이브는 반복적이지 못함
    - 어떤 페이지뷰가 세션의 첫 페이지(origin page)인지 식별함
    - 모든 페이지뷰마다 세션의 첫 시작 페이지를 확인하여 버킷에 담음
    - 각 첫 시작 페이지마다 모든 페이지뷰를 조합함
    - 각 첫 페이지 접근에 라벨을 붙이고 각 세션의 관계를 계산함
  - 하나의 행에 하나의 완전한 세션이 기록된 테이블을 얻게 되고 이를 사용하여 다양한 세션 분석 작업을 수행할 수 있음 
  - 세션 수, 각 세션 마다 평균 페이지 접근 수, 각 세션별로 가중치를 둔 평균 페이지 접근에 대한 최대값 및 최소값 등을 포함하여 다양한 분석을 수행할 수 있음 
- NASA 제트 추진 연구소
  - 속도 향상을 위하여 노드를 추가하고 맵 작업 트래커 수를 늘리고(다양한 노드 수로 실험), DFS 블록 사이즈를 변경해보고(32MB, 64MB, 128MB, 256MB), LZO 압축을 사용해보고, 다른 많은 설정값을 변경(io.sort.factor, io.sort.mb)해보았으나 별 소득이 없음
  - 얼마나 많은 작업 트래커를 실행시키는 것과는 상관없이 각 노드의 높은 I/O 지연이 문제 
- 포토버킷
  - 하이브에는 기존의 데이터 웨어하우스와 비교하여 몇 개의 큰 장점이 있음. 하둡과 하이브를 적용한 이유
    - 구조화된 데이터와 비구조화된 데이터를 함께 다룰 수 있음 
    - 플럼(Flume), 스크라이브(Scribe), MountableHDFS를 사용해 HDFS에 실시간으로 데이터 스트리밍할 수 있음
    - UDF를 통해 기능 확장이 가능함
    - 문서화가 잘되어 있으며 OLTP와 다른 OLAP를 위한 SQL과 유사한 인터페이스를 제공함 

# 오라클 마이그레이션 
- 하이브의 탄생 목적 자체가 하둡에 적재된 빅데이터를 분석하기 위한 SQL 인터페이스를 제공하기 위함이고 대부분의 배치 작업으로 처리됨. 하이브를 통해 SQL을 입력하면 하이브는 입력한 SQL을 맵리듀스 잡으로 변환하여 하둡에서 데이터를 처리하게 됨 
- 하이브는 갱신, 트랜잭션, 색인 등의 전통적인 데이터베이스의 핵심 기능이 고려되지 않았음. 그 이우는 하이브가 HDFS 상의 데이터를 맵리듀스로 처리하기 때문. 이 환경에서는 테이블 전체를 스캔하는 것이 기본적인 방식
- 하둡은 배치처리를 위한 시스템이기 때문에 하이브로 쿼리를 실행하면 응답 시간이 매우 길음. 전통적인 데이터베이스에서 수초 안에 끝나는 쿼리도 하이브에서는 더 오랜 시간이 걸림. 맵리듀스를 실행하기 위한 부하(10초 전후)가 있기 때문 
- 대용량 데이터셋에서의 성능이 전통적인 데이터베이스보다 더 좋기 때문. 전통적인 데이터베이스에서는 처리하지 못했던 수십, 수백 기가바이트 또는 테라바이트 데이터를 SQL로 분석할 수 있다는 것이 하이브가 가지는 가장 큰 매력 
- 파티셔닝
  - 오라클 파티셔닝 방식 중에 리스트 파티션만 하이브에서 지원함
  - 그 외 범위 파티션(Range Partition), 해시 파티션(Hash Partition), 복합 파티션(Composite Partition)은 지원하지 않음. 하이브는 파티션 컬럼=값으로 HDFS에 파티션 디렉터리를 생성하기 때문 
- 복잡한 서브 쿼리
  - 하이브는 기본적으로 FROM 절에 오는 서브쿼리만 지원함. 나머지 복잡한 서브쿼리는 지원하지 않음. 적절하게 동등한 쿼리로 변환해야 함 
- TRUNCATE TABLE 문
  - 오라클의 TRUNCATE TABLE 문은 개별 행 삭제를 로깅하지 않고 모든 행을 테이블에서 제거하기 위해 사용함. TRUNCATE TABLE은 기능상으로 WHERE 절이 없는 DELETE 문과 동일하지만 더 빠르고 시스템 및 트랜잭션 로그 리소스를 덜 사용함
  - 하이브에서는 TRUNCATE 문을 지원하지 않음. 차선책으로 하이브에서는 INSERT INTO만 지원하기 때문에 TABLE 모든 행을 삭제하는 TRUNCATE 문은 사용할 수 없음
