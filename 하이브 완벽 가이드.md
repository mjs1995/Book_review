# 서평
- 이 책은 데이터 모델링부터 쿼리, 색인, 튜닝, 함수, 스토리지 핸들러, HCatalog 등의 고급 기능까지 총망라하여 제공하고 있다.
- 이 책의 목적은 개발자,데이터베이스 관리자, 아키텍트는 물로인고 비즈니스 분석가처럼 기술 수준이 낮은 사용자에게 이르는 모든 이에게 HiveQL을 예제 중심으로 설명하는 것, 개발자나 하이브 쿼리의 성능 튜닝 및 사용자 정의 함수, 커스텀 데이터 포맷 정의를 사용할 하둡 관리자에게 필요한 기술적 내용을 자세히 제공하는 것 

# 소개
- 맵리듀스 프로그래밍은 큰 연산 작업을 다수의 값싼 서버로 구성된 클러스터에 고르게 나누어 처리하는 프로그래밍 모델로, 비용효율성과 수평확장성을 제공함, 이 연산 모델 아래에는 하둡 분산 파일시스템(HDFS, Hadoop Distributed Filesystem)이라는 분산 파일 시스템이 존재함 
- 하이브는 하둡 클러스터에 있는 데이터를 검색하기 위해 하이브 쿼리 언어(HiveQL) 혹은 HQL이라 부르는 SQL 호환 언어를 제공함 
- 하이브에서 레코드 단위 갱신(record-level update), 삽입, 삭제를 할 수 없긴 하지만 쿼리로 새 테이블을 만들 수 있고 쿼리 결과를 파일로 남길 수도 있음.
- 하둡은 배치처리 기반 시스템(batch-oriented system)이기 때문에 하이브 쿼리 응답 시간이 매우 길음, 맵리듀스의 기동 부하(start-up overhead)가 있기 때문
- 하이브는 트랜잭션을 제공하지 않음
- 큰 규모의 데이터를 다룰 수 있는 OLTP가 필요하면 NoSQL 데이터베이스를 사용하는 것이 더 좋음. NoSQL에는 하둡과 연동된 NoSQL인 HBase, 카산드라, 만약 아마존의 Elastic MapReduce 또는 Elastic Compute Cloud을 이용한다면 다이나모디비와 같은 것도 있음 
- 하이브는 아주 큰 데이터셋을 다루는 곳에서 데이터를 분석하여 정보를 찾아내고 보고서를 만드는 데이터 웨어하우스 애플리케이션에 적합함
- 하둡과 맵리듀스 개요
  - 맵리듀스
    - 맵리듀스는 큰 데이터를 다루는 잡(job)을 여러 태스크(task)로 나누어 다수의 서버로 구성된 클러스터에서 병렬 처리하는 연산 모델. 태스크의 결과를 하나로 합쳐 최종 결과물을 만들 수 있음 
    - 맵리듀스라는 용어는 맵과 리듀스라는 두 개의 기본 데이터 변환연산에서 나왔음. 맵연산은 컬렉션(collection)의 모든 원소를 한 형태에서 다른 형태로 변환함. 이때 입력 키-값(key-value) 쌍은 0개에서 다수의(zero-to-many) 출력 키-값 쌍으로 변환됨. 입력키와 결과키, 입력값과 결과값은 완전히 달라질 수 있음 
    - 맵리듀스에서 같은 키를 가진 모든 키-값 쌍은 컬렉션 형태로 동일한 리듀서 함수에 전달됨. 리듀서 함수의 목적은 값 컬렉션을 합계, 평균과 같은 값으로 변경하거나 다른 컬렉션으로 변경하는 것. 최종 키-값 쌍은 리듀서가 내보냄 
    - 맵리듀스의 입력과 출력에 사용되는 자료구조는 키-값 쌍. 맵퍼가 시작되면 문서의 줄마다 반복 호출됨. 호출할 때 전달되는 키는 문서 시작 부분의 문자 오프셋임
    - 맵퍼는 한 줄의 단어를 키-값 쌍으로 출력함. 정렬과 셔플 단계를 자동으로 처리함. 하둡은 키 순서대로 키-값 쌍을 정렬함. 
    - 셔플 : 같은 키를 가진 키를 모두 모아서 같은 리듀서에게 보냄.
    - 리듀서의 입력 역시 키-값 쌍. 키는 맵퍼에서 나온 단어 중 하나이고 값은 그 단어 발생 횟수의 컬렉션
    - 키 데이터형은 문자열이고 값 데이터형은 정수형 컬렉션
    - 모든 리듀서는 각 단어의 값 컬렉션별로 값을 모두 더하고 단어-총 발생 수 키-값 쌍을 출력해야 함 
- 하둡 생태계에서 하이브
  - SQL 사용자를 위한 친숙한 프로그래밍 모델을 제공할 뿐만 아니라 자바에서 해야 하는 반복적이고 까다로운 코딩을 하지 않도록 해줌 
  - 하이브와 상호동작하는 몇 가지 인터페이스가 있음. CLI, 그래픽 사용자 인터페이스가 편한 사용자는 상용인 Karmasphere, 오픈소스인 클라우데라(Cloudera)의 Hue, Qubole의 새로운 Hive-as-service 등 
  - CLI와 간단한 하이브 웹 인터페이스(HWI), JDBC, ODBC, 쓰리프트(Thrift)서버 
  - 모든 명령어와 쿼리는 입력을 해석하고 필요한 연산을 최적화한 후 보통은 맵리듀스 잡으로 되어있는 단계를 실행하는 드라이버(Driver)로 보내짐 
  - 하이브는 맵리듀스 잡을 시작하기 위해 잡트래커(JobTracker)와 통신함
  - 하이브는 테이블 스키마와 시스템 메타데이터를 보관하기 위한 메타스토어(MetaStore)를 저장하기 위해 별도의 관계형 데이터베이스 (보통 MySQL)을 이용함
  - 하이브는 실시간 응답을 요하는 쿼리나 레코드 수준의 삽입, 갱신, 삭제를 필요로 하지 않는 데이터 웨어하우스 애플리케이션에 적합함
  - 피그 
    - 야후!는 피그를 개발함.
    - 피그는 쿼리 언어가 아닌 데이터 흐름 언어(data flow language), 피그에서는 다른 관계로부터 관계를 정의하기 위해 일련의 선언문을 작성함. 각각의 새로운 관계별로 새로운 데이터 변환을 실행함 
  - HBase
    - HBase는 분산 및 확장 가능한 데이터 저장소. 로우 단위 갱신, 빠른 응답 시간과 로우 단위 트랜잭션을 제공함(복수의 로우 트랜잭션은 제공하지 않음)
    - 칼럼 지향 저장소를 지원하는 것 
  - 캐스케이딩, 크런치, 그 외 도구
    - 모두 JVM 라이브러리
    - 개발자에게 이러한 도구는 튜링 완전체(Turing complete) 프로그래밍 언어의 강력함을 제공함 \
    - 고수준 하둡을 위한 대체 라이브러리
      - 캐스케이딩(Cascading) - 데이터 처리 추상화를 위한 자바 API. 캐스케이딩을 위한 도메인 특화 언어(DSLs)가 있음. 스칼라,그루비,JRuby, Jython
      - 캐스캐로그(Cascalog) - 캐스케이딩용 클로저(Clojure) DSL. 데이터 처리 및 쿼리 추상화를 위한 Datalog로부터 영감을 받은 추가 기능을 제공함
      - 크런치(Crunch) - 데이터 플로우 파이프라인을 위한 자바,스칼라 API
    - 맵리듀스를 사용하지 않는 분산 데이터 처리 도구
      - 스파크(Spark) - 분산된 데이터를 스칼라 API로 처리하는 분산 연산 프레임워크, HDFS와 동작함. 많은 맵리듀스 처리에서 눈에 띄는 성능 향상을 제공함. 하이브를 스파크에 포팅하는 프로젝트도 있으며 샤크라고 불림 
      - 스톰(Storm) - 실시간 이벤트 스트림 처리 시스템
      - 카프카(Kafka) - 분산 메시지 시스템
    - 기타 데이터 처리 언어와 도구
      - R - 통계 분석, 데이터 도식활르 위한 오픈소스 언어로 통계학자, 경제학자 등이 주로 사용함. 분산 시스템이 아니기 때문에 처리할 수 있는 데이터 크기는 제한되어 있음 
      - 매트랩 - 데이터 분석, 수치 연산을 위한 상용 시스템, 엔지니어와 과확자가 주로 사용함
      - 옥타브(Octave) - 매트랩의 오픈소스 버전
      - 메스메티카(Mathmatica) - 상용 데이터 분석, 기호 처리, 수치 계산 시스템. 과학자와 엔지니어가 주로 사용함
      - 사이파이(SciPy), 넘피(NumPy) - 과학 프로그램을 위한 파이썬 확장 소프트웨어 패키지. 데이터 과학자 사이에 널리 사용됨 

# 시작하기
- VMware용 하둡 가상 머신 
  - 클라우데라(Cloudera) - 클라우데라가 자체적으로 배포하는 하둡 버전을 사용하고 있음
  - 맵알(MapR) - HDFS를 MapR 파일시스템으로 대체하여 자체적으로 배포하는 하둡 버전을 사용하고 있음 
  - 홀튼웍스(Hortonworks) - 가장 최신의 안정된 아파치 배포 하둡을 사용함. 현재는 배포 중지한 상태
  - 씽크 빅 아날리틱스(Think Big Analytics) - 가장 최신의 안정된 아파치 배포 하둡을 사용함 
- 로컬 모드, 의사 분산 모드, 분산 모드 
  - 런타임 모드, 기본 모드는 로컬 파일시스템을 사용하는 로컬 모드. 하이브 쿼리를 포함하는 하둡 잡(Job)이 실행될 때 맵 태스크와 리듀스 태스크가 동일한 프로세스에서 동작함 
  - 실제 클러스터는 분산 모드로 설정함. 잡은 이 클러스터 상의 잡트래커(JobTracker)에 의해서 관리되고 각각의 태스크는 각기 다른 프로세스에서 동작함 
  - 하나의 머신에서 의사 분산 (pseudodistributed) 모드로 동작하게 설정할 수 있음 
  - 의사 분산 모드는 분산 모드와 같게 동작함. 파일 시스템은 분산 파일시스템을 기본으로 참조하고 잡은 잡트래커 서비스로 관리함. 다른 점은 하나의 컴퓨터라는 점 
  - HDFS 블록의 복사 개수를 하나로 제한함. 노드가 하나인 클러스털처럼 동작함 
- 하이브 구성 
  - 쓰리프트 서비스는 다른 프로세스에서 원격으로 접근할 수 있도록 해줌. JDBC와 ODBC로 접근할 수 있는 방법을 제공하는데, 이는 쓰리프트 위에서 동작함
  - 하이브를 설치할 대 테이블 스키마와 같은 메타데이터를 저장하기 위해서 메타스토어(metastore)서비스가 필요함 , 메타스토어는 관계형 데이터베이스의 테이블로 구현함 
  - 하이브는 기본 데이터베이스로 프로세스에 내장되어 있고 제한된 기능을 제공하는 더비(Derby) SQL 서버를 사용함 
  - 원격으로 하이브에 접근할 수 있는 웹 인터페이스인 HWI(Hive Web Interface) 를 제공함
- 하둡 환경 설정하기
  - 분산과 의사 분산 모드 설정
    - 분산 모드에서는 여러 서비스가 클러스터 내에서 동작함. 잡트래커는 잡을 관리하고 네임노드는 HDFS 마스터.
    - 작업 노드는 태스크트래커(TaskTracker)와 데이터노드(DataNode)로 구성됨 
    - 태스크트래커는 잡태스크를 구동하고 관리하며 데이터노드는 분산 파일시스템에 저장된 파일을 구성하는 블록을 관리함 
  - JDBC를 사용하는 메타스토어
    - 메타스토어는 create table x... 또는 alter table y... 등과 같은 명령을 수행할 때 명시되는 테이블 스키마와 파티션 정보와 같은 메타데이터를 저장하고 있음 
    - 메타스토어가 단일 고장점(SPOF : Single Point Of Failure)이기 때문에 안정된 서비스를 보장하기 위해서 메타데이터를 복제하고 백업하기 위한 방법을 강구해야 함 
- 하이브 명령
  - 하이브 서비스
    - CLI(명령행 인터페이스) - 테이블을 정의하거나 쿼리 등을 수행하기 위해 사용됨. 다른 서비스가 명시되어 있지 않으면 기본 서비스로 동작함 
    - hiveserver(하이브 서버) - 별도의 프로세스에서 쓰리프트 연결을 위해 대기하고 있는 데몬 
    - hwi(하이브 웹 인터페이스) - 클러스터로 로그인하거나 CLI를 사용하지 않고 쿼리나 다른 명령을 수행하기 위한 간단한 웹 인터페이스
    - jar - 하이브 환경에서 애플리케이션을 수행하기 위한 것으로 hadoop jar 명령의 확장이라고 이해하면 됨
    - metastore - 다중 클라이언트를 지원하기 위해서 하이브 외부에 메타스토어를 구동하는 서비스 
    - rcfilecat - RCFile의 내용을 출력하기 위해서 사용되는 도구 
  
  # 데이터형과 파일 포맷
  - 원시 데이터형
    - 하이브는 여러 크기의 정수형과 부동소수점 및 하나의 불린형(Boolean)과 임의의 길이를 가지는 문자열을 제공함 
    - 원시 데이터형
      - TINYINT - 1바이트 크기의 정수 데이터형 (20)
      - SMALLINT - 2바이트 크기의 정수 데이터형 (20)
      - INT - 4바이트 크기의 정수 데이터형 (20)
      - BIGINT - 8바이트 크기의 정수 데이터형 (20)
      - BOOLEAN - TRUE 또는 FALSE - TRUE
      - FLOAT - 단정도 부동소수점 - 3.14159
      - DOUBLE - 배정도 부동소수점 - 3.14159
      - STRING - 문자의 시퀀스, 문자열 셋도 설정 가능작은 따옴표 혹은 큰 따옴표 생성 가능  
      - TIMESTAMP - 정수형, 부동소수점형, 문자열형
      - BINARY - 바이트 배열 형태 지원
    - 하이브는 다른 SQL에서 일반적으로 지원하는 최대 허용 길이를 갖는 문자 배열(character array)을 지원하지 않음. 관계형 데이터베이스에서 성능 최적화 기능으로 색인이나 스캔 등이 용의한 고정된 길이의 레코드를 지원함 
    - 하이브가 속한 느슨한 세계(looser world)에서는 자체 데이터 저장소를 가지지 않고 다양한 파일 포맷을 지원해야 하기 때문에 필드를 구분할 수 있는 구분 기호에 의존함 
    - 하둡 및 하이브는 디스크의 읽기, 쓰기 성능 최적화를 강조하기 때문에 컬럼값의 길이를 고정하는 것은 상대적으로 중요하지 않음 
- 컬렉션 데이터형
  - STRUCT - C 언어의 구조와 객체와 유사함 - struct('John','Doe')
  - MAP - ['key']처럼 필드를 배열 표기법으로 접근할 수 있는 키-값 형태의 컬렉션 데이터형, map('first','John','last','Doe')
  - ARRAY - 0으로 시작하는 정수로 색인할 수 있는 동일한 데이터형의 순차 시퀀스 - array('John','Doe')
  - 컬렉션 데이터형은 정규화를 깰 수 있기 때문에 대부분의 관계형 데이터베이스에서는 컬렉션 데이터형을 지원하지 않음
  - 정규화를 깸으로써 발생하는 더 큰 문제는 데이터 중복, 저장 공간 낭비, 잠재적 데이터의 불일치가 있음. 중복된 데이터는 데이터에 어떤 변경이 발생할 때 동기화되는 데이터를 증가시킬 수 있음 
  - 테라바이트에서 페타바이트 급의 데이터를 검색할 때 최소한의 디스크 탐색은 필수. 컬렉션 데이터형을 이용하면 최소한의 디스크 탐색으로 레코드를 빠르게 검색할 수 있음. 그러나 외래 키 관계의 데이터를 탐색하는 것은 상당한 성능 오버 헤드와 함께 여러 디스크를 걸치는 탐색을 요구함 
- 데이터값의 텍스트 파일 인코딩
  - 쉼표로 필드를 구분하는 텍스트 파일(CSVs:Comma-separated values) 또는 탭으로 필드를 구분하는 텍스트 파일(TSVs:tab-separated values)
  - 하이브의 레코드와 필드 기본 구분 기호
    - \n : 텍스트 파일에서 각 줄은 하나의 레코드가 됨. 그러므로 줄 바꿈 문자는 레코드를 분리함
    - ^A : 모든 컬럼을 분리함. CREATE TABLE문에서 명시적으로 지정할 때는 8진수 코드 '\001'을 사용함
    - ^B : ARRAY, STRUCT 또는 MAP 키-값 쌍의 요소를 분리함
    - ^C : MAP의 키-값 쌍에서 키를 관련된 값과 분리함 
- Schema on Read
  - schema on write 
    - 전통 데이터베이스에서 외부 데이터 로딩, 쿼리 결과물 혹은 UPDATE 문 수행을 통하여 데이터를 쓸 때 데이터베이스는 데이터 저장소 모두를 제어함
    - 데이터베이스는 문지기(gatekeeper)가 되는 것
    - 데이터베이스가 데이터를 쓸 때 스키마를 강제로 맞추려 하기 때문
  - schema on read
    - 하이브는 저장소에 대해 쓰기 제어를 가지고 있지 않음. 생성, 수정, 그리고 심지어 하이브가 쿼리할 때 데이터를 손상시키는 여러 방법이 있음. 하이브는 단지 쿼리 결과를 읽어들일 때에만 스키마를 적용함 

# HiveQL : 데이터 정의
- 하이브는 로우 레벨의 삽입과 변경 그리고 삭제를 지원하지 않음. 트랜잭션 또한 지원하지 않음. 하둡이 지원하는 범위 안에서 보다 나은 성능을 위해 확장할 수 있는 기능을 제공하며 사용자가 정의한 확장과 외부 프로그램을 하이브와 연동할 수 있음 
- 데이터 정의 언어는 데이터베이스, 테이블, 뷰, 함수, 색인을 생성하고 변경하고 삭제하는 데 사용함 
- 하이브에서의 데이터 베이스
  - 하이브에서 데이터베이스 개념은 단지 테이블의 카탈로그 또는 네임스페이스. 여러 팀과 사용자가 어우러져 큰 규모로 작업할 때 테이블명의 충돌을 막는 매우 유용한 방법
  - 데이터베이스 테이블을 논리 그룹으로 구성하는 데 사용
  - IF NOT EXISTS 문으로 필요할 때만 데이터베이스를 즉시 생성 
  - USE 명령어는 파일시스템에서 작업(working) 디렉터리를 변경하는 것과 비슷하게 작업 데이터베이스를 설정하는 데 사용함 
- 테이블 생성
  - 하이브는 자동으로 두 개의 테이블 속성을 넣음. last_modified_by 속성은 테이블을 수정한 마지막 사용자의 이름을 저장하고 last_modified_time 속성은 수정한 유닉스 표준 시간을 담음 
  - 매니지드 테이블
    - 생성한 테이블은 하이브가 데이터의 생명 주기를 제어하기 때문에 매니지드 테이블(Managed Table) 또는 내부 테이블로 부름 
    - 매니지드 테이블 대신 외부 테이블(External Table)을 정의하여 하이브가 해당 데이터를 소유하지 않도록 함 
  - 외부 테이블
    - EXTERNAL 예약어는 해당 테이블이 외부에 있으면 LOCATION ... 절엣허 지정한 위치에 존재한다는 것을 하이브에게 알려줌 
    - 외부에 존재하기 때문에 하이브는 해당 데이터를 소유하지 않음. 테이블을 삭제할 때 테이블의 메타데이터는 지워지지만 해당 데이터를 지우지는 않음 
- 파티셔닝된 매니지드 테이블
  - 많은 형태를 가질 수 있지만 수평적으로 부하를 분산하기 위해 자주 사용하는 데이터를 사용자와 물리적으로 가까운 위치에 두는 등의 목적으로 사용함 
  - 데이터를 파티셔닝하는 가장 중요한 이유는 빠른 쿼리를 위해서
  - 특정값을 걸러내는 WHERE 조건절을 포함할 때 이러한 조건을 파티션 필터라 부름 
  - > SHOW ParTITIONS {table} : 파티셔닝된 정보를 확인
  - > DESCRIBE EXTENDED {table} : 파티션 키를 보여줌 
  - 파티셔닝된 외부 테이블
    - 로그 파일 분석, 대부분의 기관은 시간, 심각도(severity, ex - ERROR, WARNING, INFO), 서버 이름, 프로세스 아이디, 그리고 임의의 텍스트 메신저로 이루어진 표준 포맷의 로그 메시지를 사용함 
    - 한 달 이상된 오래된 데이터를 S3로 이동
      - > hadoop distcp /data/log_messages/2011/12/02 s3n://ourbucket/logs/2011/12/02
      - 해당 파티셔닝 내 데이터를 S3로 복사함. 
      - > ALTER TABLE log_messages PARTITION(year=2011, month=12, day=2) SEL LOCATION 's3n://ourbucket/logs/2011/01/02';
      - 테이블의 파티셔닝이 S3 위치를 가리키도록 변경함 
      - > hadoop fs -rmr 명령어를 이용하여 HDFS 파티셔닝을 삭제함 
      - hdaddop fs -rmr 명령어를 이용하여 HDFS 파티셔닝을 삭제함
    - 새로운 데이터는 다른 디렉터리 내에 존재하는 오래된 데이터로부터 명확히 구분된 특정 디렉터리에 쓰여짐. 오래된 데이터를 아카이브 위치로 옮기거나 명확히 삭제하더라도 별도의 디렉터리에 데이터가 존재하기 때문에 새로운 데이터에 영향을 미칠 위험은 줄어듬 
  - 테이블 저장 포맷 사용자화
    - 하이브는 레코드들이 파일 내에서 인코딩되는 방법과 컬럼들이 레코드 내에서 인코딩되는 방법을 구분함 
    - 레코드 인코딩은 TEXTFILE 내부에 조재하는 자바 클래스(org.apache.hadoop.mapred.TextInputForamt)와 같은 입력 포맷 객체(Input Format Object)에 의해 다루어짐 
    - 레코드 분석은 serializer/deserializer 혹은 줄여서 SerDE(쎄데,쎄르데)에 의해서 행해짐 
- 테이블 삭제
  - > DROP TABLE IF EXISTS {table};
  - IF EXISTS 예약어는 옵션. 예약어가 없고 테이블이 조재하지 않을 때 하이브는 에러를 돌려줌 

# HiveQL : 데이터 조작
- 데이터 조작 언어(Data Manipulation Language) : 테이블에 데이터를 넣거나 테이블에서 원하는 데이터를 꺼내어 파일 시스템에 옮김 
- 매니지드 테이블에 데이터 로딩하기
  - 하이브는 로우 레벨에서 삽입,갱신,삭제를 지원하지 않음. 
  - 벌크(bulk) : 데이터를 테이블에 넣는 유일한 방법은 벌크로 로딩하는 것뿐, 아니면 테이블 디렉터리에 직접 파일을 복사하는 방법 
  - ```sql
    LOAD DATA LOCAL INPATH '{$env:HOME}/california-employees'
    OVERWRITE INTO TABLE employees
    PARTITON (country = 'US', state = 'CA');
    ```
  - 해당 파티션을 위한 디렉터리가 존재하지 않을 때 디렉터리를 생성하고 여기에 데이터를 복사함
  - LOCAL 예약어를 사용하면 파일 위치는 로컬 파일시스템에 존재하는 것으로 간주하며 데이터는 테이블 디렉터리로 복사됨 
    - LOAD DATA LOCAL...은 로컬 파일시스템의 파일을 분산 파일시스템으로 복사하며 LOAD DATA ...는 해당 데이터를 최종 위치로 이동시킴
    - 보통 뿐산 파일시스템에 데이터 파일의 사본을 갖는 것을 원하지 않는다는 가정에 기반함 

# HiveQL : 쿼리
- SELECT ... FROM 절
  - SQL에서 SELECT 프로젝션(projection) 연산자. FROM 절은 레코드를 선택하기 위해 필요한 테이블. 뷰 또는 중첩 쿼리(nested query)를 식별함 
  - 컬렉션 데이터형의 컬럼을 선택하면 하이브는 출력을 위해 JSON(Java Script Object Notation)문법을 사용함. 
  - ARRAY 데이터형은 쉼표로 구분된 목록이 [...]로 둘려싸여 있음 
  - MAP의 경우 쉼표로 구분된 키:값 쌍의 목록을 {...}로 둘러싸는 JSON 표현을 사용함 
  - STRUCT로 JSON 맵 형식을 사용함 
  - 하이브는 오버 플로우나 언더플로우가 발생할 때 더 넓은 범위의 데이터형이 존재하더라도 결과를 자동으로 변환하지 않는 자바 데이터형 규칙을 따름 
- 기타 내장 함수
  - > parse_url(url,partname,key) : HOST, PATH, QUERY, REF,PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY:<key>. 옵션 키는 마지막에 QUERY:<key>를 요청함 
  - > find_in_set(s, 쉼표로 구분된 String) : 쉼표로 구분된 문자열에서 s의 색인을 반환함. 찾지 못하면 NULL이 반환됨
  - > locate(substr,str,pos) : str의 post 위치로부터 substr이 있는 색인을 반환함 
  - > instr(str,substr) : str에서 substr의 색인을 반환함
  - > str_to_map(s,delim1,delim2) : delim1을 키-값 쌍의 구분자로 사용하고 delim2를 키와 값의 구분자로 사용하여 문자열 s를 파싱한 후 맵을 생성함 
  - > sentences(s,lang,locale) : 문자열 s를 단어의 배열로 이루어진 문장의 배열로 반환함 
  - > ngrams(array<array<string>>, N, K, pf) : 텍스트에서 top-K n-gram을 반환함. pf는 정밀도
  - > context_ngrams(array<array<string>>,array<string>, int K, int pf> : ngrams와 같지만 출력 배열에서 두 번째 단어 배열로 시작하는 n-gram을 찾음
  - > in_file(s, filename) : filenmae 파일에서 문자열 s가 나타나면 true를 반환함 
- WHERE 절
  - WHERE 절에서 컬럼 별칭을 사용할 수 없음. 하지만 중첩 SELECT 문은 사용할 수 있음 
  - ```sql
    SELECT e.* FROM 
    (SELECT name, salary, deductions['Federal Taxes"] as ded,
    salary * (1 - deductions['Federal Taxes"]) as salary_minus_fed_taxes
    FROM employees) e
    WHERE round(e.salary_minus_fed_taxes) > 70000;
    ```
  - LIKE와 RLIKE
    - 하이브는 LIKE 절을 자바 정규표현식을 사용할 수 있는 RLIKE절로 확장 
    - 마침표(.)는 어떠 한 문자와 일치하고 별(*)은 왼쪽에 있는 것이 0번에서 여러 번 반복되는 것을 의미함 
    - (x|y) 표현식은 x 혹은 y가 일치하는 것을 의미함 
- 조인 문
  - 하이브는 고전적인 SQL 조인 문을 제공함. 동등 조인(EQUAL-JOIN)만 제공함 
  - 조인 최적화
    - 하이브는 쿼리의 마지막 테이블이 가장 크다고 가정함. 다른 테이블을 버퍼링하려고 시도하고 각 레코드에 대해서 조인을 수행하면서 마지막 테이블을 흘려보냄. 조인 쿼리를 구성할 때는 가장 큰 테이블이 가장 마지막에 오도록 함 
    - 하이브는 쿼리 최적화(optimizer)이기에 어떤 테이블을 마지막으로 흘려보내야 하는지 지정하는 힌트(hint) 메카니즘을 제공함
    - ```sql
      SELECT /** STREAMTABLE(s) */ s.ymd, s.symbol, s.price_close, d.dividend
      FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol
      WHERE s.symbol = 'AAPL';
      ```
    - 중첩 SELECT 문
    - ```sql
      SELECT s.ymd, s.symbol, s.price_close, d.dividend FROM
      (SELECT * FROM stocks WHERE symbol = 'AAPL' AND exchange = 'NASDAQ') s
      LEFT OUTER JOIN
      (SELECT * FROM dividends WHERE symbol = 'AAPL' AND exchange = 'NASDAQ') d
      ON s.ymd = d.ymd;
      ```
    - 중첩 SELECT 문은 데이터 조인 전에 파티션 필터를 적용하는 데 필요한 푸시다운(push down)을 수행함
      - 푸시다운은 WHERE 절의 술어 중 일부를 떼어내어 미리 실행하는 것을 말함. 리지주얼(residual)은 푸시ㅏ운 후에 남은 술어를 일컫음 
      - 하이브는 조인을 수행한 후에 WHERE 절을 평가함. WHERE 절은 NULL이 되지 않는 컬럼값에 대해서만 필터를 적용하는 술어를 사용해야함. 하이브 문서와는 달리 파티션 필터는 외부 조인의 ON 절에서 동작하지 않음 
  - 왼쪽 세미 조인
    - 왼쪽 세미 조인(LEFT SEMI-JOIN)은 오른쪽 테이블에서 ON의 술어를 만족하는 레코드를 찾을 경우 왼쪽 테이블의 레코드를 반환함, 하이브는 오른쪽 세미 조인을 지원하지 않음
  - 맵 사이드 조인(Map-side Join)
    - 만약 한 테이블만 빼고 모두 작다면 작은 테이블은 메모리에 캐시하고 가장 큰 테이블은 맵퍼로 흘려보낼 수 있음. 하이브는 메모리에 캐시한 작은 테이블로부터 일치하는 모든 것을 찾아 낼 수 있기 때문에 맵에서 모든 조인을 할 수 있음. 이렇게 하면 일반 조인 시나리오에서 필요한 리듀스 단계를 제거할 수 있음 
    - 더 작은 데이터셋에 대해서도 이 최적화는 일반 조인보다 눈에 띌 만큼 빠름. 리듀스 단계를 제거할 뿐만 아니라 맵 단계 역시 줄어들기 때문
    - ```sql
      SELECT /*+ MAPJOIN(d) */ s.ymd, s.symbol, s.price_close, d.dividend 
      FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol
      WHERE s.symbol = 'AAPL'
      ```
    - 하이브는 오른쪽 외부 조인과 완전 외부 조인에 대해서 최적화를 지원하지 않음
- ORDER BY와 SORT BY
  - 하이브는 ORDER BY 대신 데이터를 각 리듀서에 정렬하는 SORT BY를 추가함. 각 리듀스의 출력이 정렬되도록 지역 정렬(local ordering)을 수행하는 것을 의미
- SORT BY와 함께 사용하는 DISTRIBUTE BY
  - DISTRIBUTE BY는 맵의 출력을 리듀서로 어떻게 나누어 보내는지를 제어함 
  - 기본적으로 맵리듀스는 맵퍼가 출력하는 키에 대해서 해시값을 계산하고 해시값을 이용하여 키-값 쌍을 가용한 리듀서로 균등하게 분산하려고 노력함 
  - 맵퍼에서 출력한 키-값 쌍의 값을 계산하여 리듀서를 선택하는 것은 파티셔너의 역할 
  - SORT BY는 리듀서 안에서 데이터 정렬을 제어하는 반면 DISTRIBUTE BY는 리듀서가 처리할 로우를 어떻게 받는지 제어한다는 점에서 GROUP BY처럼 동작함
  - 하이브는 SORT BY 절 전에 DISTRIBUTE BY 절을 사용할 것을 요구하므로 주의해야 함 
- CLUSTER BY
  - DISTRIBUTE BY ... SORT BY 또는 간단히 사용하는 CLUSTER BY 절은 출력 파일 간에 전체 정렬을 이루면서 SORT BY의 병렬 처리를 사용하도록 하는 방법 
- 데이터 표본을 만드는 쿼리
  - 매우 큰 데이터셋에 대해서 전체를 사용하는 것이 아니라 어떤 쿼리를 수행하여 나온 결과를 대표 표본으로 하여 작업하고자 하는 경우가 종종 있음
  - 하이브는 테이블을 버킷으로 구성하여 표본을 만든느 쿼리로 이를 지원함 
  - 버킷 테이블에 대한 입력 푸루닝
    - pruning - 푸루닝은 데이터 분석 작업에 불필요한 데이터를 미리 잘라내는 작업을 말함. 가지치기, 추리기 정도로 번역할 수 있으나 데이터베이스나 데이터 분석에 많이 사용하는 용어이므로 원어 그대로 사용 

# HiveQL : 뷰
- 쿼리는 뷰를 테이블처럼 저장하거나 다룸. 하지만 테이블처럼 데이터르리 저장하지는 않기 때문에 논리적인 구조물 
- 하이브는 매티리얼라이즈드 뷰(materialized view)를 지원하지 않음
- 쿼리의 복잡합을 줄여주는 뷰
  - 캡슐화(encapsulation)화는 최종 사용자로 하여금 재사용 가능한 부분을 이용하여 복잡한 쿼리를 더 이해하기 쉽게 만듬, 하이브 쿼리는 일반적으로 다단계의 중첩 구조로 되어있음
- 동적 테이블을 위한 뷰와 MAP 데이터형
  - 하이브는 ARRAY, MAP, STRUCT 데이텨형을 지원함. 전통적인 데이터베이스는 제 1 정규형(first normal form)을 깬다는 이유로 이런 데이터형을 지원하지 않음
  - 하이브는 한 줄의 텍스트를 고정된 개수의 컬럼만으로 처리하는 전통적인 데이터베이스와는 달리 MAP으로도 처리할 수 있음. 이 기능을 활용하면 뷰와 결합하여 한 물리 테이블에서 여러 논리 테이블을 정의할 수 있음 
- 기타
  - 하이브는 뷰를 먼저 수행한 후 쿼리를 수행함. 그러나 하이브는 쿼리 최적화 차원에서 쿼리와 뷰 두 절을 하나의 실제 쿼리로 결합할 수 있음 
  - 뷰와 뷰를 사용하는 쿼리 둘 다 ORDER BY 절이나 LIMIT 절을 가지고 있다면 뷰의 개념이 여전히 적용됨. 쿼리 절을 사용하기 전에 뷰 절을 먼저 수행하는 것 
  - 테이블과 마찬가지로 DESCRIBE shipments와 DESCRIBE EXTENDED shipments는 shipments 뷰에 대한 유용한 정보를 보여줌. 후자를 사용하면 Detailed Table Information의 tableType 속성은 VIRTUAL_VIEW 값을 가지고 있을 것
  - 뷰를 INSERT나 LOAD 명령의 대상(traget)으로 사용할수 없음. 뷰는 읽기 전용. 뷰의 TRLPROPERTIES 메타데이터만 변경할 수 있음 

# HiveQL : 색인
- 색인은 또한 논리적 파티션이 크기는 작고 개수가 많은 경우 파티셔닝을 대신해서 사용하기에도 좋음. 색인은 맵리듀스 잡의 입력으로 사용할 테이블 블록을 푸루닝(데이터 분석에 불필요한 데이터를 미리 잘라내는 작업을 말함)하는데 도움을 줄 수 있음
- 색인 생성
  - ```sql
    CREATE INDEX employees_index
    ON TABLE employees (country)
    AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'
    WITH DEFERED REBUILD
    IDXPROPERTIES ('creator = 'me', 'created_at' = 'some_time')
    IN TABLE employees_index_table
    PARTITIONED BY (country, name)
    COMMENT 'Employees indexed by country and name.';
    ```
  - 그래뉴얼리티(granularity) : 어떤 대상을 잘게 쪼개는 정도를 나타내는 용어
  - AS ... 절은 색인을 구현한 자바 클래스를 색인 핸들러(index handler)로 지정함
  - 비트맵 색인
    - 비트맵 색인은 컬럼에 고유값이 많지 않은 경우 사용함 
- 색인 재구축
  - 만약 WITH DEFERRED REBUILD를 지정했다면 새로운 색인은 비어 있는 상태에서 시작함, ALTER INDEX 문을 이용하여 언제든지 재구축할 수 있음
  - ```sql
    ALTER INDEX employees_index
    ON TABLE employees
    PARTITION (country = 'US')
    REBUILD;
    ```
- 색인 보기
  - > SHOW FORMATTED INDEX ON employees;
  - 테이블의 어떤 컬럼에 대해서든 정의되어 있는 모든 색인을 보여줌 
- 색인 삭제
  - 색인을 사젝하면 색인 테이블도 삭제됨 

# 스키마 설계
- 파티션 설계 시 고려사항
  - 네임노드는 파일 시스템에 대한 모든 메타데이터를 메모리에 유지하고 있어야함. 각 파일의 메타데이터를 저장하기 위해서 대략 150바이트 정도의 작은 메모리가 필요한데 이것이 결국 HDFS가 관리할 수 있는 파일의 최대 개술르 제한함 
  - MapR이나 아마존 S3와 같은 다른 파일시스템은 이런 제한이 없음 
- 고유 키와 정규화
  - 관계형 데이터베이스는 데이터베이스 메모리 구조에 맞게 데이터를 저장하기 위해 일반적으로 고유 키(unique key), 색인, 정규ㅜ화(normalization)을 사용함 
  - 하이브에는 기본 키와 자동으로 순차적인 키를 생성하는 개념이 없음. 비정구화된 데이터에 대한 조인은 가능하면 피해야 함 
  - 하나의 로우에서 데이터를 일대다(one-to-many) 형태로 저장하기 위해서는 ARRAY,MAP,STRUCT와 같은 컬렉션 데이터형을 사용함. 정규화를 절대 사용하지 말라는 것이 아니라 스타 스키마(star-schema)를 사용하여 설계하라는 뜻
  - 정규화를 피해야하 하는 주요 이유는 외래 키 관계를 찾아다니면서 생기는 디스크 탐색을 최소화하기 위해서. 데이터를 비정규화하면 디스크 상의 인접한 대규모의 데이터를 찾아보거나 쓰기 작업을 할 때에 최적의 I/O 성능을 낼 수 있음. 비정규화로 발생하는 데이터 중복과 데이터 불일초로 인한 더 큰 위험을 감수해야 함 
- 동일 데이터에 대한 다중 패스 만들기
  - 하이브에는 한 번의 소스 데이터 통과로 반복 스캔 없이 여러 번 집계를 할 수 있는 특별한 문법이 있음 
  - 다중 패스(multiple passes) 기능은 대규모 입력 데이터를 처리할 때 상당한 시간을 절약해줌 
  - ```sql
    INSERT OVERWRITE TABLE sales
    SELECT * FROM history WHERE action='purchased';
    INSERT OVERWRITE TABLE credits
    SELECT * FROM history WHERE action='returned';
    -- 문제없이 정확하지만 비효율적이라 아래처럼 변경 
    FROM history
    INSERT OVERWRITE sales SELECT * WHERE action='purchased'
    INSERT OVERWRITE credits SELECT * WHERE action='returned';
    ```
- 테이블에 컬럼 추가하기
  - SerDe는 일반적으로 왼쪽에서 오른쪽으로 파싱해가면서 특정 컬럼 구분자를 사용하여 로우를 나눔
  - SerDe를 ETL에서 쿼리를 통해서 데이터를 가져와 적합한 포맷으로 변환하기 위한 필터 정도로 이해하면 됨 
- 컬럼 기반 테이블 사용하기
  - 하이브는 전형적으로 로우 지향(row oriented) 저장소를 사용하지만 로우와 컬럼을 둘 다 지향하는 하이브리드 형태로 정보를 저장하는 컬럼 기반의 SerDe 또한 가지고 있음 

# 튜닝
- HiveQL은 선언형 언어(declarative language), 사용자가 선언적 쿼리를 내보내면 하이브는 쿼리를 어떻게 맵리듀스 잡으로 변환할지 알아냄 
  - 문제를 해결하는 절차(control flow)를 기술하지 않고 문제를 설명하는(logic of computation) 고급 언어. 어떻게 할지를 기술하는 것이 아니라 무엇을 할지를 기술함. 반대 개념의 언어는 어떻게 할지를 기술하는 명령형 언어(imperative language). 선언형 언어의 예로는 SQL, 정규식, 논리 프로그래밍 언어, 함수 프로그래밍 언어 등이 있음
- EXPLAIN 사용하기
  - EXPLAIN 기능을 사용하여 하이브가 쿼리를 어떻게 맵리듀스 잡으로 변환하는지를 살펴보는 것 
  - 추상 문법 트리(abstract syntax tree)가 출력됨. 하이브가 어떻게 쿼리를 토큰(token)과 리터럴(literals)로 분해하는지 보여줌
  - 파서(parser)와 토크나이저(tokenizer)에 익숙하지 않은 사용자는 내용을 이해하기 어려울 것(일단 TOK_ 접두사는 무시해라)
  - 하이브 잡은 하나 이상의 스테이지(stage)로 구성됨. 스테이지 사이에는 의존관계가 있음. 
  - 스테이지는 맵리듀스 잡, 샘플링(sampling) 스테이지, 병합 스테이지, 결과 제한(limit) 스테이지를 비롯해 그 외 하이브가 필요한 일을 하는 스테이지로 구성될 수 있음. 하이브는 이러한 스테이지를 한 번에 하나씩 실행함 
- EXPLAIN EXTENDED
  - EXPLAIN과의 차별점은 Reduce Operator Tree 부분 
- LIMIT 튜닝
  - LIMIT 절은 여전히 데이터 전체에 대해 쿼리를 실행하고 일부 결과만 반환함, 이는 낭비이기 때문에 가능한 피해야 함 
- 최적화된 조인
  - 가장 큰 테이블을 조인 절 마지막에 두거나 /*streamtable(table_name) */ 지시문(directive)을 사용하는 것은 꼭 기억하기 
  - 나머지 테이블은 모두 크지만 한 테이블만 메모리에 올릴 수 있을 정도로 충분히 작다면 하이브는 맵 사이드 조인을 수행할 수 있음. 리듀스 태스크가 필요 없을 뿐만 아니라 일부 맵 태스크도 없앨 수 있음 
- 로컬 모드
  - 많은 하둡 잡이 대규모 데이터셋을 처리하기 위해 하둡의 규모 가변성(scalability)을 활용함 
- 병렬 수행
  - 하이브는 쿼리를 하나 이상의 스테이지(stage)로 변환함. 스테이지는 맵리듀스 스테이지, 샘플링, 병합, 결과 개수 제한 등 하이브가 필요로 하는 모든 것이 될 수 있음 
  - 하이브는 기본적으로 한 번에 한 스테이지만 실행함. 하지만 종속 관계가 없는 스테이지가 포함된 잡은 더 일찍 수행을 마칠 수 있게 병렬로 실행할 수 있음 
- strict모드
  - 하이브를 strict 모드로 설정하면 사용자가 의도하지 않거나 원하지 않는 효과를 발생시키는 쿼리 입력을 막아줌 
  - 파티션이 있는 테이블은 WHERE 절에 범위를 제한하는 파티션 필터를 포함하지 않으면 쿼리를 실행할 수 없음. 전체 파티션을 검색하는 쿼리를 막음 
  - LIMIT 절 없이 ORDER BY 절을 사용하는 것. ORDER BY는 정렬을 수행하기 위해 모든 결과를 단일 리듀서에 보내기 때문. LIMIT 절을 명시하도록 강제하는 것은 리듀서가 예상보다 긴 시간 동안 수행하는 것을 막아줌 
  - 카타시안 프로덕트(cartesian product)
- 맵퍼와 리듀서 수의 최적화
  - 하이브는 쿼리를 한 개 이상의 맵리듀스 잡으로 나누어 처리하여 병렬로 수행함. 맵리듀스 각각은 다수의 맵퍼와 리듀서 태스크로 되어 있는데, 그중 적어도 몇몇은 병렬로 수행될 수 있음
  - 적절한 수의 맵퍼와 리듀서 수를 결정하는 일은 입력의 크기, 데이터에 수행하는 연산의 종류 등과 같은 다양한 변수에 의존적
  - 너무 많은 맵퍼와 리듀서 태스크는 잡을 초기화하고 스케줄링하고 실행하기 위해 많은 부하를 주는 반면, 너무 적은 태스크는 클러스터가 가진 병렬성을 제대로 활용하지 못하기 때문 
- 자바 가상 머신 재사용
  - 자바 가상 머신(JVM)의 재사용은 하이브 성능과 매우 밀접함
  - 특히 작은 파일을 처리해야 하는 경우와 대부분의 태스크가 수행 시간이 짧은 상황을 처리해야만 할 때 유용함
  - 하둡의 기본 설정은 일반적으로 포크된(forked) 자바 가상 머신에서 맵 또는 리듀서 태스크를 실행함 
- 색인
  - 색인은 GROUP BY 쿼리의 계산 속도를 높이는 데 사용됨
  - 비트맵 색인은 (도시명 같이 한정된 수의 값이 있는 컬럼) 주어진 컬럼에 고유값이 비교적 적을 때 주로 사용됨
  - 동적 파티셔닝 튜닝
    - 동적 파티션 INSERT 문은 파티션된 테이블에 많은 새 파티션을 생성하여 SELECT 문을 간단하게 만듬
    - 하이브의 기본 설정은 1,000개 이상의 동적 파티션을 생성하는 것을 막음
- 투기적 실행
  - 투기적 실행(speculative execution)은 중복된 태스크를 동시에 수행시키는 하둡의 기능. 같은 데이터를 중복하여 복사하기 때문에 더 많은 리소스를 씀. 대부분의 데이터는 버려짐
  - 이 기능의 목적은 느리게 동작하는 태스크트래커를 제거함으로써 개별 태스크의 결과가 더 도출되고 결과적으로 전체 잡 수행을 향상시키는 데 있음
    - 하둡은 동일한 태스크를 여러 노드에서 동시에 실행하여 혹시라도 특정 노드가 노후 장비 같은 문제 때문에 수행이 느려지더라도 다른 노드에서 먼저 끝나면 해당 결과만 사용하고 나머지 태스크는 중단함. 결과적으로 전체 수행 시간은 줄어들지만 시스템의 자원을 추가로는 더 사용한다는 단점이 있음 
- 가상 컬럼
  - 하이브는 두 개의 가상 컬럼을 제공함. 하나는 스플릿의 입력 파일 이름이고, 나머지는 파일에 대한 블록 오프셋임
  - 하이브가 기대하지 않았던 값이나 null을 결과값으로 생성할때 쿼리를 추적하기 위해 사용함 
  
# 기타 파일 포맷과 압축
- 데이터를 특정 포맷에 강제로 맞추지 않는 것은 하이브만의 고유 기능. 하이브는 하둡의 입력 포맷(InputFormat) API를 이용해 텍스트 파일, 시퀀스파일, 사용자 정의 파일과 같은 다양한 소스로부터 데이터를 읽음. 출력 포맷(OutputFormat) API를 이용하면 다양한 포맷으로 데이터를 쓸 수도 있음 
- 압축은 많은 양의 디스크를 절약함. 텍스트 기반의 파일은 40% 이상의 압축률을 보여줌. 압축은 처리량과 성능을 향상시킴. 압축을 하고 푸는 일이 CPU에 많은 부하를 주는 것이기 때문에 납득이 가지 않겠지만 메모리에 더 적은 바이트를 보내면서 얻은 I/O 절약은 네트워크 성능에 이득을 줄 수 있음
- 압축 코덱 선택
  - 압축을 사용하면 파일 저장에 필요한 디스크 공간 사용을 최소화하고 디스크와 네트워크 부하를 줄여주는 장점이 있음. 파일을 압축하고 푸는 작업은 CPU에 많은 부하를 줌.여분의 CPU 용량이 있거나 디스크 공간이 매우 중요한 I/O 바운드 잡에서 압축을 사용하는 것이 좋음
  - 모든 하둡 최신 버전은 GZip, BZip2 압축 방식과 이런 포맷의 성능을 가속화하는 리눅스 라이브러리를 내장하고 있음. Snappy 압축도 포함되어있음. LZO 압축도 자주 사용함 
  - BZip2는 가장 높은 압축률을 보이지만 CPU를 가장 많은 씀. 속도 대비 압축률에 있어서 GZip이 그 다음. 만약 적은 디스크 공간 사용과 I/O 부하가 중요하면 둘 다 매력적인 선택이 될 수 있음 
  - LZO와 Snappy는 압축률이 낮아 비교적 큰 파일을 만들지만 압축 속도가 빠르며 특히 압축을 풀 때는 훨씬 빠름. 만약 빠르게 압축을 푸는 것이 디스크 공간과 I/O 오버헤드보다 중요하다면 이런 압축 방식을 선택하는 것이 좋음
  - 맵리듀스는 매우 큰 입력 파일을 스플릿(split)으로 분할함. 하둡은 각각의 스플릿을 개별 맵 프로세스에게 보냄
  - 어떻게 파일을 로우(레코드)로 나누냐는 것. 텍스트 파일은 \n(줄 바꿈 문자)을 기본 로우 구분자로 사용함. 텍스트 파일 포맷을 사용하지 않으면 하이브에게 사용할 InputFormat과 OutputFormat의 이름을 알려줘야 함.
    - InputFormat은 어떻게 스플릿을 읽고 레코드로 나누는지 알고 있고, OutputFormat은 어떻게 스플릿을 파일 또는 콘솔에 출력하는지에 대해 알고 있음 
  - 레코드를 어떻게 필드(또는 컬럼)로 나누냐에 대한 것. 하이브는 텍스트 파일의 레코드를 필드로 나눌 때 ^A 문자를 기본 구분자로 사용함. 하이브는 들어오는 레코드를 쪼개고(deserializer) 레코드를 해당 포맷으로 다시 쓰는 방법을 아는(serializer) SerDE라는 모듈을 이용함 
- 중간 과정 압축
  - 중간 과정 압축(intermediate compression)은 맵에서 리듀스로 보내는 데이터양을 줄임. 중간 과정 압축에서는 압축률보다 CPU 사용량이 낮은 압축을 선택하는 것이 중요함 
- 최종 출력 압축
  - 하이브는 테이블로 결과를 출력할 때 내용을 압축할 수 있음 
- 시퀀스파일
  - 하둡은 분할 가능한 파일을 여러 조각으로 나눠 여러 맵퍼로 보내 병렬로 수행될 수 있게 함. 대부분의 압축 파일은 나눌 수 없음. 왜냐하면 무조건 처음부터 읽어야 하기 때문
  - 하둡이 지원하는 시퀀스파일 포맷은 파일을 블록으로 나눌 수 있고 선택적으로 블록을 압축할 수 있음. CREATE TABLE 절에 STORED AS SEQUENCEFILE 절을 추가하면 됨
  - 세 가지 압축 옵션. NONE, RECORD, BLOCK이고 RECORD가 기본 
- 파티션 아카이빙하기
  - 하둡은 하둡 아카이브를 의미하는 HAR이라는 저장 포맷을 가지고 있음. HAR 파일은 TAR 파일과 비슷함. 하둡 파일시스템 내에 단일 파일로 존재함 

# 개발
- Log4J 속성 변경
  - hive-log4j.properties 파일은 CLI나 로컬에서 실행되는 하이브 구성요소의 로깅을 제어함
  - hive-exec-log4j.properties 파일은 맵리듀스 태스크 안에서 로깅을 제어함 

# 함수
- 함수 탐색 및 설명
  - SHOW FUNCTIONS 명령어는 현재 하이브 세션에 로드된 내장 함수와 사용자 정의 함수의 목록을 보여주는 명령어
  - DESCRIBE FUNCTION 명령어를 사용하면 간략한 설명을 볼 수 있음
  - EXTENDED 예약어로 접근할 수 있는 확장 문서를 포함하기도 함 
- 표준 함수
  - UDF는 좁은 의미로 하나의 로우 또는 로우로부터 하나 이상의 컬럼을 받아서 하나의 값을 반환하는 함수를 지칭함 
  - > ucase() : 문자열의 문자를 대문자로 전환
