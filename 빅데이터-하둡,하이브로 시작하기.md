# 서평
- 빅데이터에 수집,정제,적재,분석,시각화의 여러단계에 관해서 관련 프레임워크나 오픈소스를 잘 설명하고 있으며 하둡과 Yarn에 대해 많은 정보를 얻어서 유익하였습니다.
- 빅데이터 에코시스템에 관해서 관련 기술과 특징들에 대해 전반적인 구조를 알수 있으며 하둡과 하이브에대한 다양한 예제 및 아키텍처들을 설명하고 있어서 빅데이터 시스템을 이해하는데 도움이 많이 되었습니다. 또한 빅데이터 시스템을 이용하는 회사의 아키텍처와 클러스터 운영시 관련 에러를 예시를 들어 설명을 하고 있어서 간접경험을 하는데 큰 도움이 되었습니다.
- 하둡과 하이브에 대해 자세히 설명을 해주고 정리가 잘 되어있어서 이해하는데 좋았다. 또한 관련 사례 및 예제를 통해서 조금 더 친밀하게 경험을 할 수 있었다.
- 데이터 엔지니어 업무를 하고있거나 빅데이터 아키텍처에 관심이 많은 분이라면 기본 개념을 탄탄하게 잡을 수 있도록 도움이 될 것입니다.

# 빅데이터 - 하둡, 하이브로 시작하기[https://wikidocs.net/book/2203]
![image](https://user-images.githubusercontent.com/47103479/154843391-6c4d6166-42b4-44ed-b640-a779a6c24b23.png)

# 빅데이터
## 데이터 수집 형태
- 정형
  - 데이터베이스, CSV, 엑셀과 같이 칼럼 단위의 명확한 구분자와 형태가 존재하는 데이터
- 반정형
  - XML, HTML, JSON 형태와 같이 여러 가지 형태가 있을 수 있지만, 메타데이터나 스키마가 존재하는 데이터
- 비정형
  - 동영상, SNS 메시지, 사진, 오디오, 음성 데이터처럼 형태가 존재하지 않는 데이터

## 수집 시간
- 배치
  - 시, 일, 주, 월 단위로 일정한 주기로 수집, 처리되는 데이터
- 실시간
  - 실시간 검색어, 실시간 차트 처럼 사용자의 입력과 동시에 처리되는 데이터

## 내부/외부 데이터
- 내부 데이터
  - 시스템 로그, DB 데이터
- 외부 데이터
  - 동영상, 오디오 정보
  - 웹 크롤링 데이터
  - SNS 데이터

## 빅데이터 에코시스템
- 빅데이터는 수집, 정제, 적재, 분석, 시각화의 여러 단계를 거칩니다. 이 단계를 거치는 동안 여러가지 기술을 이용하여 처리되고, 이 기술들을 통틀어 빅데이터 에코 시스템(Bigdata Eco System)

## 수집 기술 및 에코시스템
- Flume
  - 플룸은 많은 양의 로그 데이터를 효율적으로 수집, 취합, 이동하기 위한 분산형 소프트웨어
  - 플룸은 클라우데라에서 개발한 서버 로그 수집 도구 입니다. 각 서버에 에이전트가 설치 되고, 에이전트로부터 데이터를 전달 받는 콜렉터로 구성됩니다.
- Kafka
  - 오픈 소스 메시지 브로커 프로젝트
  - 카프카는 링크드인에서 개발한 분산 메시징 시스템입니다. 대용량 실시간 로그 처리에 특화 되어 있습니다. 발행(publish) - 구독(subscribe) 모델로 구성됩니다.
  -  메시징, 메트릭 수집, 로그 수집, 스트림 처리 등 다양한 용도로 사용
  - 특징
    - 빠르다: Fast
      - 수 천개의 데이터 소스로 부터 초당 수백 메가바이트의 데이터를 입력 받아도 안정적으로 처리 가능
    - 확장가능: Scalable
      - 메시지를 파티션으로 분리햐여 분산 저장, 처리할 수 있어 클러스터로 구성하여 확장 가능
    - 안정적이다: Durable
      - 클러스터에 파티션 복제하여 장애 내구성을 가짐
- Sqoop
  - 관계형 데이터 베이스와 아파치 하둡간의 대용량 데이터들을 효율적으로 변환 하여 주는 명령 줄 인터페이스 애플리케이션
  - RDBMS와 HDFS간 대용량 데이터 전송을 위한 솔루션입니다. HDFS, RDBMS, DW, NoSQL 등 다양한 저장소에 대용량 데이터를 신속하게 전송할 수 있는 방법을 제공합니다. 상용RDBMS도 지원하고, MySQL, PostgreSQL 오픈소스 RDBMS도 지원합니다.
- Nifi
  - 소프트웨어 시스템 간 데이터 흐름을 자동화하도록 설계된 소프트웨어 프로젝트
  - 미국 국가안보국(NSA)에서 개발한 시스템 간 데이터 전달을 효율적으로 처리, 관리, 모니터링하기 위한 최적의 시스템입니다.
- Flink
  - 오픈 소스 스트림 처리 프레임 워크
- Splunk
  - 기계가 생성한 빅 데이터를, 웹 스타일 인터페이스를 통해 검색, 모니터링, 분석하는 소프트웨어
- Logstash
  - 실시간 파이프라인 기능을 가진 오픈소스 데이터 수집 엔진
- Fluentd
  - 크로스 플랫폼 오픈 소스 데이터 수집 소프트웨어 프로젝트
  - 트레저 데이터에서 개발한 로그 수집시스템입니다. 주로 루비와 C로 작성되었습니다. 여러 형태의 로그를 전달받아서 원하는 저장소에 쌓을수 있습니다. 비정형, 반정형 데이터를 필터링, 버퍼링하여 사용자가 지정하는 데이터베이스나 클라우드 저장소에 효율적으로 저장할 수 있습니다.
기본적인 구조는 Flume NG와 비슷합니다. Flume의 Source, Channel, Sink가 Input, Buffer, Output으로 대체되었습니다. 장점은 각 파트 별로 플러그인을 만들기 쉽습니다.

## 작업관리기술
- AIrflow
  - 에어플로우는 에어비앤비에서 개발한 데이터 흐름의 시각화, 스케쥴링, 모니터링이 가능한 워크플로우 플랫폼입니다. 하이브, 프레스토, DBMS 엔진과 결합하여 사용 할 수 있습니다.

- Azkaban
  - 아즈카반은 링크드인에서 개발한 워크플로우 스케쥴러, 시각화된 절차, 인증 및 권한 관리, 작업 모니터링 및 알람 등 다양한 기능을 가지는 워크플로우 관리 도구 입니다.

- Oozie
  - 우지는 하둡 작업을 관리하는 워크플로우 및 코디네이터 시스템입니다. 자바 웹 애플리케이션 서버로 UI 제공 하고, 맵리듀스, hive, pig 작업 같은 특화된 액션으로 구성된 XML 포맷의 워크플로우로 작업을 제어합니다.

## 데이터 직렬화
- Avro
  - 에이브로(Avro)는 아파치의 하둡 프로젝트에서 개발된 원격 프로시저 호출(RPC) 및 데이터 직렬화 프레임워크입니다. 자료형과 프로토콜 정의를 위해 JSON을 사용하며 콤팩트 바이너리 포맷으로 데이터를 직렬화합니다.
- Thrift
  - 스리프트는 페이스북에서 개발한 서로 다른 언어로 개발된 모듈의 통합을 지원하는 RPC 프레임워크 입니다. 데이터 타입과 서비스 인터페이스를 선언하면, RPC 형태의 클라이언트와 서버 코드를 자동으로 생성해 줍니다. 자바, C++, C#, Perl, PHP, 파이썬, 델파이, Erlang, Go, Node.js 등과 같이 다양한 언어를 지원합니다
- Protocol Buffers
  - 프로토콜 버퍼(Protocol Buffers)는 구글에서 개발한 RPC 프레임워크 입니다. 구조화된 데이터를 직렬화하는 방식을 제공합니다. C++,C#, Go, Java, Python, Object C, Javascript, Ruby 등 다양한 언어를 지원하며 특히 직렬화 속도가 빠르고 직렬화된 파일의 크기도 작아서 Apache Avro 파일 포맷과 함께 많이 사용됩니다.

## 저장
- HDFS
  - 하둡 분산 파일 시스템(HDFS, Hadoop distributed file system)은 하둡 프레임워크를 위해 자바 언어로 작성된 분산 확장 파일 시스템입니다. HDFS는 범용 컴퓨터를 클러스터로 구성하여 대용량의 파일을 블록단위로 분할하여 여러서버에 복제하여 저장합니다
- S3
  - S3는 아마존에서 제공하는 인터넷용 저장소입니다. 아마존에서 자체적으로 제공하는 여러가지 서비스에 잘 적용되는 저장소 입니다.
- NoSQL
- HBase
  - HBase는 HDFS 기반의 칼럼 기반 NoSQL 데이터베이스입니다. 구글의 빅테이블(BigTable) 논문을 기반으로 개발됐습니다. 실시간 랜덤 조회 및 업데이트가 가능하며, 각 프로세스는 개인의 데이터를 비동기적으로 업데이트할 수 있습니다.
  - HBase의 기본 동작단위는 칼럼입니다. H마스터가 H리전을 관리하는 구조를 가지고 있습니다. 주키퍼가 H마스터를 관리하여 SPOF를 회피합니다.

## 데이터 처리
- MapReduce
  - 맵리듀스는 HDFS상에서 동작하는 가장 기본적인 분석 기술입니다. 간단한 단위작업을 반복할 때 효율적인 맵리듀스 모델을 이용하여 데이터를 분석합니다.

- Spark
  - 스파크(Spark)는 인메모리 기반의 범용 데이터 처리 플랫폼입니다. 배치 처리, 머신러닝, SQL 질의 처리, 스트리밍 데이터 처리, 그래프 라이브러리 처리와 같은 다양한 작업을 수용할 수 있도록 설계되어 있습니다. 2009년 버클리 대학의 AMPLab에서 시작됐으며, 현재 가장 빠르게 성장하고 있는 오픈소스 프로젝트 중의 하나입니다.

- Impala
  - 임팔라(Impala)는 클라우데라에서 개발한 하둡 기반의 분산 쿼리 엔진입니다. 맵리듀스를 사용하지 않고, C++로 개발한 인메모리 엔진을 사용해 빠른 성능을 보여줍니다. 임팔라는 데이터 조회를 위한 인터페이스로 HiveQL을 사용하며, 수초 내에 SQL 질의 결과를 확인할 수 있습니다. 2015년말 아파치 재단의 인큐베이션 프로젝트로 채택됐습니다.

- Presto
  - 프레스토(Presto)는 페이스북이 개발한 대화형 질의를 처리하기 위한 분산 쿼리 엔진입니다. 메모리 기반으로 데이터를 처리하며, 다양한 데이터 저장소에 저장된 데이터를 SQL로 처리할 수 있습니다. 특정 질의 경우 하이브 대비 10배 정도 빠른 성능을 보여주며, 현재 오픈소스로 개발이 진행되고 있습니다

- Hive
  - 하이브(Hive)는 하둡 기반의 데이터웨어하우징용 솔루션입니다. 페이스북에서 개발했으며, 오픈소스로 공개되며 주목받은 기술입니다. SQL과 매우 유사한 HiveQL이라는 쿼리 언어를 제공합니다. 그래서 자바를 모르는 데이터 분석가들도 쉽게 하둡 데이터를 분석할 수 있게 도와줍니다. HiveQL은 내부적으로 맵리듀스 잡으로 변환되어 실행됩니다.

- Hcatalog
  - HCatalog는 Pig, MapReduce, Spark에서 Hive 메타스토어 테이블에 액세스할 수 있는 도구입니다. 테이블을 생성하거나 기타 작업을 수행할 수 있는 REST 인터페이스 및 명령줄 클라이언트를 제공합니다.

- Pig
  - 피그(Pig)는 야후에서 개발됐으나 현재는 아파치 프로젝트에 속한 프로젝트로서, 복잡한 맵리듀스 프로그래밍을 대체할 피그 라틴(Pig Latin)이라는 자체 언어를 제공합니다. 맵리듀스 API를 매우 단순화한 형태이고 SQL과 유사한 형태로 설계됐습니다. SQL과 유사하기만 할 뿐, 기존 SQL 지식을 활용하기가 어려운 편입니다.

## 클러스터 관리
- YARN
  - 얀(YARN)은 데이터 처리 작업을 실행하기 위한 클러스터 자원(CPU, 메모리, 디스크등)과 스케쥴링을 위한 프레임워크입니다. 기존 하둡의 데이터 처리 프레임워크인 맵리듀스의 단점을 극복하기 위해서 시작된 프로젝트이며, 하둡2.0부터 이용이 가능합니다. 맵리듀스, 하이브, 임팔라, 타조, 스파크 등 다양한 애플리케이션들은 얀에서 리소스를 할당받아서, 작업을 실행하게 됩니다.

- Mesos
  - 메소스(Mesos)는 클라우드 인프라스트럭처 및 컴퓨팅 엔진의 다양한 자원(CPU, 메모리, 디스크)을 통합적으로 관리할 수 있도록 만든 자원 관리 프로젝트입니다. 메소스는 2009년 버클리 대학에서 Nexus 라는 이름으로 시작된 프로젝트이며, 2011년 메소스라는 이름으로 변경됐으며, 현재는 아파치 탑레벨 프로젝트로 진행중이며, 페이스북, 에어비엔비, 트위터, 이베이 등 다양한 글로벌 기업들이 메소스로 클러스터 자원을 관리하고 있습니다. 메소스는 클러스터링 환경에서 동적으로 자원을 할당하고 격리해주는 매커니즘을 제공하며, 이를 통해 분산 환경에서 작업 실행을 최적화시킬 수 있습니다. 1만대 이상의 노드에도 대응이 가능하며, 웹 기반의 UI, 자바, C++, 파이썬 API를 제공합니다. 하둡, 스파크(Spark), 스톰(Storm), 엘라스틱 서치(Elastic Search), 카산드라(Cassandra), 젠킨스(Jenkins) 등 다양한 애플리케이션을 메소스에서 실행할 수 있습니다.

## 분산 서버 관리
- 클러스터에서 여러가지 기술이 이용될 때 하나의 서버에서 모든 작업이 진행되면 이 서버가 단일실패지점(SPOF)가 됩니다. 이로 인한 리스크를 줄이기 위해 분산 서버 관리 기술을 이용
- Zookeeper
  - 분산 환경에서 서버 간의 상호 조정이 필요한 다양한 서비스를 제공하는 시스템으로, 크게 다음과 같은 네 가지 역할을 수행합니다.
  - 첫째, 하나의 서버에만 서비스가 집중되지 않게 서비스를 알맞게 분산해 동시에 처리하게 해줍니다. 둘째, 하나의 서버에서 처리한 결과를 다른 서버와도 동기화해서 데이터의 안정성을 보장합니다. 셋째, 운영(active) 서버에 문제가 발생해서 서비스를 제공할 수 없을 경우, 다른 대기 중인 서버를 운영 서버로 바꿔서 서비스가 중지 없이 제공되게 합니다. 넷째, 분산 환경을 구성하는 서버의 환경설정을 통합적으로 관리합니다.

## 시각화
- Zeppelin
  - Zeppelin은 한국의 NFLab이라는 회사에서 개발하여 Apache top level 프로젝트로 최근 승인 받은 오픈소스 솔루션으로, Notebook 이라고 하는 웹 기반 Workspace에 Spark, Tajo, Hive, ElasticSearch 등 다양한 솔루션의 API, Query 등을 실행하고 결과를 웹에 나타내는 솔루션입니다.
- Hue
  - 하둡 휴(Hue, Hadoop User Experience)는 하둡과 하둡 에코시스템의 지원을 위한 웹 인터페이스를 제공하는 오픈 소스 입니다. Hive 쿼리를 실행하는 인터페이스를 제공하고, 시각화를 위한 도구를 제공합니다. 잡의 스케줄링을 위한 인터페이스와 잡, HDFS, 등 하둡을 모니터링하기 위한 인터페이스도 제공합니다.

## 보안
- Ranger
  - 레인저는 하둡 클러스터의 각 모듈에 대한 보안 정책을 관리할 수 있습니다. HDFS의 ACL, Hive 데이터베이스의 접근권한 등의 보안 정책과 각 모듈에 대한 접근 기록(Audit)을 보관합니다.

## 데이터 거버넌스
- 기업의 여기저기 산재한 데이터를 같은 저장소에 관리, 비정형 데이터를 규칙에 맞게 표준화하는 전사 차원의 빅데이터 관리 체계
- Atlas
  - 아틀라스는 데이터 거버넌스로 조직이 보안/컴플라이언스 요구사항을 준수할 수 있도록 지원합니다. 데이터 자원에 대한 태깅, 다운스트림 데이터셋에 대한 태그 전파, 메타 데이터 접근에 대한 보안등 다양한 기능을 가지고 있습니다. 메타데이터 변경 알림 기능을 제공하고, Hive, HBase, Kafka의 데이터가 변경되는 것을 알리는 기능을 제공합니다.
- Amundsen
  - 아문센은 데이터 디스커버리 플랫폼입니다. 기업에 존재하는 데이터를 검색하고, 추천하는 기능을 가지고 있습니다. 검색, 추천, 미리보기/컬럼통계/소유자/주사용자 들이 잘 표현된 테이블 상세 페이지를 지원합니다.

# 카카오 광고시스템 
- 데이터 수집 기술
  - 카프카, 로그스태쉬
  - 스파크 스트리밍, 플링크
- 데이터 처리
  - 하이브, 임팔라
- 저장
  - HDFS, 카산드라, HBase, 엘라스틱서치, 레디스
- 시각화
   - 태블루, 제플린, 카프카, 키린
- 운영
  - 에어플로우, 그라파나, 키바나, 프로메테우스
- ![image](https://user-images.githubusercontent.com/47103479/154843455-f62234d2-1511-4006-b740-733c12b80e9d.png) 

# 하둡
- 하둡은 하나의 성능 좋은 컴퓨터를 이용하여 데이터를 처리하는 대신, 적당한 성능의 범용 컴퓨터 여러 대를 클러스터화하고, 큰 크기의 데이터를 클러스터에서 병렬로 동시에 처리하여 처리 속도를 높이는 것을 목적으로 하는 분산처리를 위한 오픈소스 프레임워크

- 구성요소
  - Hadoop Common
    - 하둡의 다른 모듈을 지원하기 위한 공통 컴포넌트 모듈
  - Hadoop HDFS
    - 분산저장을 처리하기 위한 모듈
여러개의 서버를 하나의 서버처럼 묶어서 데이터를 저장
  - Hadoop YARN
    - 병렬처리를 위한 클러스터 자원관리 및 스케줄링 담당
  - Hadoop Mapreduce
    - 분산되어 저장된 데이터를 병렬 처리할 수 있게 해주는 분산 처리 모듈
  - Hadoop Ozone
    - 하둡을 위한 오브젝트 저장소
- 장점 
  - 오픈소스로 라이선스에 대한 비용 부담이 적음
  - 시스템을 중단하지 않고, 장비의 추가가 용이(Scale Out)
  - 일부 장비에 장애가 발생하더라도 전체 시스템 사용성에 영향이 적음(Fault tolerance)
   - 저렴한 구축 비용과 비용대비 빠른 데이터 처리
  - 오프라인 배치 프로세싱에 최적화
- 단점
  - HDFS에 저장된 데이터를 변경 불가
  - 실시간 데이터 분석 같이 신속하게  처리해야 하는 작업에는 부적합
  - 너무 많은 버전과 부실한 서포트
설정의 어려움

## HDFS
- 특징
  - 블록 단위 저장
  - 블록 복제를 이용한 장애 복구
  - 읽기 중심
  - 데이터 지역성
- 네임노드 : 메타데이터 관리와 데이터노드의 관리
- 메타데이터 관리
  - 메타데이터는 파일이름, 파일크기, 파일생성시간, 파일접근권한, 파일 소유자 및 그룹 소유자, 파일이 위치한 블록의 정보 등으로 구성됩니다. 각 데이터노드에서 전달하는 메타데이터를 받아서 전체 노드의 메타데이터 정보와 파일 정보를 묶어서 관리
- 데이터 노드 관리
  - 데이타노드는 파일을 저장하는 역할을 합니다. 파일은 블록단위로 저장
  - 활성 상태는 데이터노드가 Live 상태인지 Dead 상태인지를 나타냅니다
  - 운영 상태는 데이터노드의 업그레이드, 패치 같은 작업을 하기 위해 서비스를 잠시 멈추어야 할 경우 블록을 안전하게 보관하기 위해 설정
- HDFS Federation
  - 디렉토리(네임스페이스) 단위로 네임노드를 등록하여 사용하는 것
  - ex) user, hadoop, tmp 세개의 디렉토리가 존재할 때, /user, /hadoop, /tmp 디렉토리 단위로 총 3개의 네임노드를 실행하여 파일을 관리하게 하는 것


## 맵리듀스
- 간단한 단위작업을 반복하여 처리할 때 사용하는 프로그래밍 모델
- 간단한 단위작업을 처리하는 맵(Map) 작업과 맵 작업의 결과물을 모아서 집계하는 리듀스(Reduce) 단계로 구성
- 매퍼 : 파일을 읽어서 바로 쓰는 작업의 경우 리듀서가 필요 없어서
- 리듀서 : 집계를 진행함 
- 처리 단계
  - 입력
    - 데이터를 입력하는 단계
    - 텍스트, csv, gzip 형태의 데이터를 읽어서 맵으로 전달
  - 맵(Map)
    - 입력을 분할하여 키별로 데이터를 처리
  - 컴바이너(Combiner)
    - 네트워크를 타고 넘어가는 데이터를 줄이기 위하여 맵의 결과를 정리
    - 로컬 리듀서라고도 함
    - 컴바이너는 작업의 설정에 따라 없을 수도 있음
  - 파티셔너(Partitoner)
    - 맵의 출력 결과 키 값을 해쉬 처리하여 어떤 리듀서로 넘길지를 결정
  - 셔플(Shuffle)
    - 각 리듀서로 데이터 이동
  - 정렬(Sort)
    - 리듀서로 전달된 데이터를 키 값 기준으로 정렬
  - 리듀서(Reduce)
    - 리듀서로 데이터를 처리하고 결과를 저장
  - 출력
    - 리듀서의 결과를 정의된 형태로 저장

## YARN
- YARN(Yet Another Resource Negotiator)은 하둡2에서 도입한 클러스터 리소스 관리 및 애플리케이션 라이프 사이클 관리를 위한 아키텍처
- 클러스터 자원 관리는 리소스 매니저(ResourceManager)와 노드매니저(NodeManager)를 이용하여 처리
  - 노드매니저는 클러스터의 각 노드마다 실행됩니다. 현재 노드의 자원 상태를 관리하고, 리소스매니저에 현재 자원 상태를 보고
  - 리소스매니저는 노드매니저로부터 전달받은 정보를 이용하여 클러스터 전체의 자원을 관리합니다. 자원 사용 상태를 모니터링하고, 애플리케이션 마스터에서 자 자원을 요청하면 비어 있는 자원을 사용할 수 있도록 처리
  - 스케줄러(Scheduler) : 자원을 분배하는 규칙을 설정, 스케줄러에 설정된 규칙에 따라 자원을 효율적으로 분배
    - 리소스 매니저는 클러스터 자원을 관리하고, 애플리케이션 마스터의 요청을 받아서 자원을 할당합니다. 자원 할당을 위한 정책
    - 피포(FIFO) 스케줄러
      - 먼저 들어온 작업이 먼저 처리됩니다. 작업의 제출 순서대로 처리되고, 먼저 들어온 작업이 종료될 때까지 다음작업은 대기
      - 이로 인해 자원을 효율적으로 사용할 수 없기 때문에 FIFO 스케줄러는 테스트 목적으로만 사용하는 것이 좋음
    - 페어(Fair) 스케줄러
      - Fair 스케줄러는 제출 된 작업이 동등하게 리소스를 점유합니다. 작업 큐에 작업이 제출되면 클러스터는 자원을 조절하여 작업에 균등하게 자원을 할당 하여 줌
    - 커패시티(Capacity) 스케줄러
      - 하둡2의 기본 스케줄러 입니다. 트리 형태로 큐를 선언하고 각 큐 별로 이용할 수 있는 자원의 용량을 정하여 주면 그 용량에 맞게 자원을 할당 하여 줌
      - 트리 형태로 계층화된 큐를 선언하고, 큐별로 사용가능한 용량을 할당하여 자원을 관리
      - 사용자 큐 매핑(queue-mappings) : 사용자가 큐를 지정하지 않아도, 자동으로 사용자와 큐가 매핑되게 할 수 있습니다.
```shell
$ yarn application -list 
$ yarn application -status application_1234_1
$ yarn application -kill <application_id>
$ yarn applicationattempt -list application_1234_1
$ yarn container -list appattempt_1234_1_000001
$ yarn logs -applicationId <application_id>

# 루트의 상태 체크 
$ hdfs fsck /

# /user/hadoop/ 디렉토리의 상태 체크 
$ hdfs fsck /user/hadoop/

```
- YARN REST API
  - 리소스 매니저는 REST API를 제공합니다. 이를 통해 클러스터의 상태정보, 운영정보를 확인할 수 있음

- YARN Node Labels
  - 서버를 특성에 맞게 구분하여 작업을 처리하게 하는 기능을 제공

- YARN 고가용성
  - YARN은 리소스 매니저가 단일 실패 지점입니다. 리소스 매니저에 문제가 발생하면 클러스터의 자원관리, 작업 관리 기능을 사용할 수 없기 때문에 하둡 2.4 버전부터 HA 기능을 제공
  - 리소스 매니저 고가용성은 주키퍼와 액티브, 스탠바이 리소스 매니저를 이용하여 제공
- 하둡 아카이브
  - 하둡 HDFS는 작은 사이즈의 파일이 많아지면 네임노드에서 이를 관리하는데 많은 어려움을 겪게 되는 문제가 발생합니다. 따라서 블록사이즈 정도로 파일을 유지해주는 것이 좋습니다. 이를 위해서 하둡은 파일을 묶어서 관리하고, 사용할 수 있는 하둡 아카이브(Hadoop Archive) 기능을 제공
- 오존(Ozone)
  - 하둡을 위한 확장성(scalable) 있는 분산 객체 저장소(distributed object store)
  - 볼륨(사용자 계정), 버켓(디렉터리), 키(파일)

- 설정
  - core-site.xml
    - 공통으로 사용하는 주요 설정
  - hdfs-site.xml
    - HDFS 관련 설정
  - mapred-site.xml
    - 맵리듀스 작업 관련 설정
  - yarn-site.xml
    - YARN 관련 설정
  - hadoop-env.sh
    - 하둡을 이용하는데 필요한 환경변수 설정

- ![image](https://user-images.githubusercontent.com/47103479/154843479-823acfd6-e335-4590-96ae-126516dab76f.png)


# 하이브(Hive)
- 하둡 에코시스템 중에서 데이터를 모델링하고 프로세싱하는 경우 가장 많이 사용하는 데이터 웨어하우징용 솔루션
- RDB의 데이터베이스, 테이블과 같은 형태로 HDFS에 저장된 데이터의 구조를 정의하는 방법을 제공하며, 이 데이터를 대상으로 SQL과 유사한 HiveQL 쿼리를 이용하여 데이터를 조회하는 방법을 제공
- 구성 요소
  - UI
    - 사용자가 쿼리 및 기타 작업을 시스템에 제출하는 사용자 인터페이스
    - CLI, Beeline, JDBC 등
  - Driver
    - 쿼리를 입력받고 작업을 처리
    - 사용자 세션을 구현하고, JDBC/ODBC 인터페이스 API 제공
  - Compiler
    - 메타 스토어를 참고하여 쿼리 구문을 분석하고 실행계획을 생성
  - Metastore
    - 디비, 테이블, 파티션의 정보를 저장
  - Execution Engine
    - 컴파일러에 의해 생성된 실행 계획을 실행
- 하이브 서비스
  - 메타스토어 
    - HDFS 데이터 구조를 저장하는 실제 데이터베이스를 가지고 있습니다. 메타스토어는 3가지 실행모드가 있습니다.
    - 테스트로 동작할 때는 임베이디드 모드를 사용하고, 실제 운영에서는 리모트 모드를 많이 사용
  - 하이브서버2(hiveserver2)
    - 하이브서버2는 다른 언어로 개발된 클라이언트와 연동 서비스를 제공
  - 비라인(beeline)
    - 일반적인 CLI처럼 내장형 모드로 작동하거나 JDBC로 하이브서버2 프로세스에 접근할 수 있는 하이브의 명령행 인터페이스
  - HCatalog
    - Pig, MapReduce, Spark에서 하이브의 데이터 파일에 접근할 수 있도록 도와주는 추상 계층을 제공
  - WebHCat
    - HCatalog의 기능을 REST API로 제공합니다. 기본적으로 50111 포트를 이용
- 파티션
  - 파티션은 데이터를 디렉토리로 분리하여 저장
  - 하이브 같은 파일 기반 테이블은 기본적으로 테이블의 모든 roww 정보를 읽기 때문에 데이터가 많아지면 속도가 느려짐
  - 고정 파티션(dynamic) : 테이블에 데이터를 입력하는 시점에 파티션 정보를 전달하기 때문에 입력되는 파티션을 알 수 있음 
  - 동적 파티션(static) : 칼럼의 정보를 이용하여 동적으로 파티션이 생성되기 때문에 쿼리 시점에는 파티션을 알 수 없음
- 버켓팅 : 지정된 칼럼의 값을 해쉬 처리하고 지정한 수의 파일로 나누어 저장
- 스큐 : 칼럼에 특정 데이터가 주로 들어오는 경우 분리하여 저장하는 기능
  - 파티션은 주로 데이터를 크게 구분하는 용도로 사용합니다. 보통 일자별로 구분할 때 많이 사용
  - 스큐는 칼럼의 데이터를 구분할 때 사용
- 정렬
  - order by : 모든 데이터를 정렬하여 하나의 파일로 생성
  - sort by : 리듀서 별로 입력된 데이터를 정렬하여 출력
  - distribute by : 매퍼의 결과를 리듀서로 전달 할 때 같은 값을 가지는 row는 같은 리듀서로 전달
  - cluster by : sort by 와 DISTRIBUTE by 를 동시에 수행
- 서데(SerDe) : 서데(SerDe, Serializer/Deserialaizer)는 하이브가 데이터를 해석하는 방법을 제공
  - 하이브는 파일을 읽을 때 파일포맷(FileFormat)을 이용하고, 디시리얼라이저(Deserializer)를 이용하여 원천 데이터를 테이블 포맷에 맞는 로우 데이터로 변환합니다. 파일을 쓸때는 로우 데이터를 시리얼라이저(Serializer)를 이용하여 키, 밸류 형태로 변경하고 파일포맷을 이용하여 저장 위치에 씁니다. 서데는 doDeserialize(), doSerialize() 를 구현하여 각각의 경우를 처리
  - HDFS files --> InputFileFormat --> [key, value] --> Deserializer --> Row object
  - Row object --> Serializer --> [key, value] --> OutputFileFormat --> HDFS files
  - 기본 서데는 7가지(Avro, ORC, RegEx, Thrift, Parquet, CSV, JsonSerDe)
- 가상 칼럼(Virtual Column)
  - 하이브에는 입력된 원천데이터의 위치를 확인
- 쿼리 분석 : 하이브에서는 explain 명령을 이용하여 쿼리 실행 계획을 확인
- 통계 정보 : 하이브는 테이블의 로우 수, 파일 개수, 사이즈등의 통계정보를 이용하여 빠른 데이터처리를 지원
- 파일 머지 
  - 하이브의 작업중 매퍼 단독 작업의 경우 파일이 많이 생성될 수 있습니다. 작은 사이즈의 파일이 많이 생성되면 HDFS에 부담이 될 수 있기 때문에 이때는 파일을 묶어주는 것이 좋음
- 파일 압축
- 조인 타입
  - hive에서 테이블간의 조인을 처리하는 방법
  - 셔플 조인(Shuffle Join)	 : 머지 조인(Merge Join)
    - 셔플(Shuffle) 단계에서 조인을 처리
    - 두 개의 테이블을 조인할 때 각 테이블을 맵(Map) 단계에서 읽고, 파티션 키를 조인 키로 설정하여 셔플 단계에서 조인 키를 기준으로 리듀서로 데이터가 이동되고 테이블을 조인
      - 어떤형태의 데이터 크기와 구성에도 사용 가능
      - 가장 자원을 많이 사용하고 느린 조인 방식
  - 맵 조인(Map Join)	 : 브로드캐스트 조인(Broadcast Join), 맵사이드 조인(Mapside Join)
    - 두 개의 테이블을 조인할 때 하나의 테이블이 메모리에 로드 되어 처리됨
    - 하나의 테이블이 메모리에 올라갈 수 있을 정도로 작을 때 맵조인을 적용
    - 셔플 조인에 비하여 빠른 속도로 처리할 수 있음
    - 테이블이 메모리에 올라갈 수 있는 크기여야 함
  - 정렬-병합-조인(SMB Join. Sort-Merge-Bucket Join)	
    - 조인 테이블이 버켓팅 되어 있을 때 사용할 수 있음
    - 버켓팅된 키의 정보를 이용하여 빠르게 조인을 처리할 수 있음 
    - 어떤 크기의 테이블에서도 가장 빠른 속도로 조인을 처리할 수 있음

- 함수
  - UDF : UDF는 1개의 열(row)을 처리하여, 1개의 열을 반환하는 함수입니다. substr(), round() 등의 함수
  - UDAF : UDAF는 N개의 열을 이용하여, 1개의 열을 반환, RDB의 윈도우 함수, count(), sum(), max() 등의 함수
  - UDTF : UDTF는 1개의 열을 입력 받아서 N개의 열을 반환, 리스트, 맵 형태의 데이터를 테이블로 보여줄 때 사용
  - show : 함수, 테이블, 데이터베이스의 목록을 확인
  - desc : 함수, 테이블, 데이터베이스의 설정값을 확인, extended, formatted 와 함께 사용하여 상세 설정을 확인
  - map(key1,val1,)	: 주어진 키와 값으로 맵을 생성
  - array(val1,val2) :	주어진 값으로 배열을 생성
  - struct(val1,val2) : 주어진 값으로 구조체를 생성
  - named_struct(col1, val1, col2, val2, ...) :	주어진 값으로 이름있는 구조체를 생성
create_union(index, val1, val2)	주어진 값으로 Union 생성
  - cast( expr as <type>)	: 주어진 값의 형을 변경
  - size(Map or Array) :	맵과 배열의 값의 수를 반환
  - array_contains(Array, value) :	배열에 값이 있는지 확인
  - sort_array(Array)	배열을 정렬
  - collect_set(col) : 중복 제거된 배열을 반환
  - collect_list(col) :	칼럼 값의 배열을 반환
- 관리
  - DESC(DESCRIBE)
    - Database
    - 데이터베이스 이름, 주석(설정된 경우) 및 파일 시스템의 루트 위치를 보여줌
    - Table/View/Materialized View/Column
      - Display Column Statistics
    - Partition
  - SHOW
    - 메타스토어에 정의된 데이터베이스, 테이블, 뷰, 테이블 파티션 등의 목록을 확인할 때 사용
    - SHOW DATABASES 또는 SHOW SCHEMAS는 메타 스토어에 정의 된 모든 데이터베이스를 나열
- 트랜잭션
  - 처리 순서
    - 테이블이나 파티션은 베이스 파일의 집합으로 저장
    - insert, update, delete 에 대해서는 델타 파일로 저장
    - 읽는 시점에 베이스 파일과, 델터 파일을 합쳐서 수정된 내용을 반환
  - 컴팩션 
    - 델타 파일(delta_xxx)을 정리하는 작업
    - 마이너 컴팩션(minor compaction) : 델타 파일을 모아서 버켓당 하나의 델타 파일로 다시 생성
    - 메이저 컴팩션(major compaction) : 베이스 파일과 델타 파일을 새로운 베이스 파일로 생성
  - 락(Lock)
    - 락은 트랜잭션을 처리할 때 테이블, 파티션에 접근을 제어하는 용도
      - 공유 잠금(Shared(S))
        - 읽기 잠금(Read Lock),  다른 트랜잭션에서 데이터를 읽으려고 할 때 다른 공유 잠금은 허용되지만, 배타적 잠금은 허용되지 않음
      - 배타적 잠금(Exclusive(X))
        - 쓰기 잠금(Write Lock), . 데이터를 변경(INSERT, UPDATE, DELETE)하려고 할 때 다른 트랜잭션에서 데이터를 읽거나 변경하지 못하게 배타적 잠금을 설정
- 성능 최적화
  - TEZ 엔진 사용
    - 작업 처리 결과를 메모리에 저장하여 맵리듀스보다 빠른 속도로 작업을 처리
    - 맵리듀스(MR)엔진(맵리듀스는 맵단계에서 데이터를 읽어서 처리하고, 리듀스 단계에서 처리 결과를 저장)은 연산의 중간 파일을 로컬 디스크에 쓰면서 진행하여 이로 인한 잦은 IO 처리로 작업이 느려짐
    - 테즈(TEZ)는 YARN 기반의 비동기 사이클 그래프 프레임워크
  - 파일 저장 포맷: ORC 파일 사용
    - ORC 파일 포맷은 데이터를 컬럼 단위로 저장하기 때문에 검색 속도가 빠르고, 압축률이 높음
    - ORC(Optimized Row Columnar)는 칼럼 기반의 파일 저장방식으로, Hadoop, Hive, Pig, Spark등에 적용가능
  - 데이터 저장 효율화: 파티셔닝, 버켓팅 사용
    - 하이브는 디렉토리 단위로 데이터를 처리하기 때문에 검색에 사용되는 데이터를 줄이기 위한 방안으로 파티셔닝, 버켓팅 기능을 이용
    - 파티셔닝은 데이터를 폴더 단위로 구분하여 저장
    - 버켓팅은 지정한 개수의 파일에 컬럼의 해쉬값을 기준으로 데이터를 저장  

## 맵리듀스 vs TEZ (중요)
- TEZ
  - 테즈(TEZ)는 YARN 기반의 비동기 사이클 그래프 프레임워크
  - 하이브에서 맵리듀스 대신 실행엔진으로 사용
  - 맵단계 처리 결과를 메모리에 저장하고, 이를 리듀스 단계로 바로 전달
  - 리듀스 작업의 결과를 맵단계를 거치지 않고 리듀스 단계로 전달하여 IO 오버헤드를 줄여서 속도를 높임
- 맵리듀스
  - 맵리듀스는 맵단계에서 데이터를 읽어서 처리하고, 리듀스 단계에서 처리 결과를 저장
  - 하나의 작업이 여러 단계의 맵, 리듀스를 거치게 되면 중간 작업 결과를 HDFS에 쓰고, 다시 맵단계에서 파일을 읽어서 처리
  - 작업 중간 임시 데이터도 디스크에 쓰게 되어 IO 작업으로 인한 오버헤드가 많음
  - ![image](https://user-images.githubusercontent.com/47103479/155325599-73ee1796-19e3-4406-ba7f-c8b715e3a5db.png)
  - ![image](https://user-images.githubusercontent.com/47103479/155325629-ff396d47-512e-4d50-8f31-0e471f95db4b.png)

# 하둡 클러스터 운영
- 네임노드
  - 힙메모리에 HDFS에 존재하는 모든 파일의 메타정보를 저장
  - HDFS는 네임노드가 SPOF로 네임노드에 장애가 발생하면 클러스터가 중단됩니다. 이를 회파하기 위해서 주키퍼를 이용해 HDFS를 HA 구성으로 설정
- 파일 개수 줄이는 법
  - Hadoop Archive 이용
    - 하둡은 har 기능을 제공하여 여러개의 파일을 하나로 묶을 수 있음
  - 사용하지 않는 파일 정리
    - 파일의 접근시간을 이용하여 접근 시간이 오래된 파일을 정리
    - hadoop fs -ls -u /user/
      - -u 옵션 : 접근 시간(access time) 확인
  - oiv(offline fsimage viewer)를 이용하여 접근 시간을 확인
    - hdfs oiv
# 하이브 성능 최적화, ETL 성능 향상
- HiveQL 측면에서 성능 향상하기
  - 조건절 내의 UDF 제거
    - where 조건절에 UDF를 사용하면 하이브 옵티마이저는 정보를 이용할 수 없어서 파티션 가지치기(Partiton Pruning)이 발생하지 않습니다. 따라서 많은 양의 데이터를 읽어야 하기 때문에 성능에 나쁜 영향을 주게 됩니다.
- DISTINCT COUNT 연산 피하기
  - COUNT(DISTINCT column) 함수를 이용하면 하나의 리듀서에서 정보를 처리하기 때문에 작업이 느려질 수 있습니다. GROUP BY 함수를 이용한 방법으로 처리하는 것이 좋습니다.
- JOIN 사용시 고려할 점
  - 하이브 옵티마이저는 가장 마지막의 테이블이 가장 크다고 가정합니다. 가장 큰 데이터의 테이블을 마지막에 놓거나 /*+STREAMTABLE(a)*/ 옵션을 이용하면 좋습니다.
  - 아웃 조인시 조인 수행후 WHERE 조건을 처리하기 때문에 중첩 SELECT 문을 이용해 먼저 데이터를 필터링 후 조인을 진행하도록 처리하는 것이 좋습니다.
- SELECT 사용 시 고려사항
  - *를 이용해 모든 데이터를 가져오지 않고, 필요한 칼럼 데이터만 가져오는 것이 좋습니다.

- RDBMS 쿼리 측면에서 성능 향상하기
  - 불필요한 인덱스 줄이기
    - 인덱스는 DDL문의 성능에 안 좋은 영향을 주고, Nested Loops Join을 이용하게 될 수 있기 때문에 적절한 수의 인덱스를 선언하는 것이 좋습니다.
  - WHERE 조건에서 함수 피하기
    - 함수를 이용하면 정확한 카디날리티를 얻을 수 없기 때문에 옵티마이저가 안좋은 선택을 하게 할 수 있습니다.
  - OR 조건 피하기
    - OR 조건은 최적화 되지 않은 실행계획을 만들 수도 있습니다. OR 조건은 UNION 문을 대체하면 더 빠를 수도 있습니다.
